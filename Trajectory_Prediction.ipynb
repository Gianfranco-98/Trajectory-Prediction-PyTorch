{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYwxiuChyg8u"
      },
      "source": [
        "## Deep Learining project\n",
        "\n",
        "\n",
        "*   Gianfranco Di Marco - 1962292\n",
        "*   Giacomo Colizzi Coin - 1794538\n",
        "\n",
        "\n",
        "\\\n",
        "**- Trajectory Prediction -**\n",
        "\n",
        "Is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.\n",
        "\n",
        "**- nuScenes Dataset -**\n",
        "\n",
        "Available at. https://www.nuscenes.org/nuscenes. The nuScenes\n",
        "dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360Â° coverage\n",
        "\n",
        "\n",
        "> Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and Giancarlo Baldan and Oscar Beijbom: \"*nuScenes: A multimodal dataset for autonomous driving*\", arXiv preprint arXiv:1903.11027, 2019.\n",
        "\n",
        "The most important part of this dataset for our project is the Map Expansion Pack, which simplify the trajectory prediction problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3uPCs50bDU"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VbP-XZY0xvv"
      },
      "source": [
        "**Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ-Kwgkp0xog",
        "outputId": "9b1ef494-7f58-4b72-c94b-816216eadd9b"
      },
      "outputs": [],
      "source": [
        "# Necessary since Google Colab supports only Python 3.7\n",
        "# -> some libraries can be different from local and Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    ENVIRONMENT = 'colab'\n",
        "    %pip install tf-estimator-nightly==2.8.0.dev2021122109\n",
        "    %pip install folium==0.2.1\n",
        "except:\n",
        "    ENVIRONMENT = 'local'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_RVZEdYvzu"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y4wvD942xC89",
        "outputId": "313777e0-4580-461d-eedd-778f9eda7b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nuscenes-devkit in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (1.1.9)\n",
            "Requirement already satisfied: Pillow>6.2.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (9.0.1)\n",
            "Requirement already satisfied: jupyter in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.0.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.8.0)\n",
            "Requirement already satisfied: Shapely in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.8.1.post1)\n",
            "Requirement already satisfied: pyquaternion>=0.9.5 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (0.9.9)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (4.5.5.64)\n",
            "Requirement already satisfied: cachetools in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (5.0.0)\n",
            "Requirement already satisfied: fire in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (0.4.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (3.5.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.21.5)\n",
            "Requirement already satisfied: pycocotools>=2.0.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (2.0.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (4.64.0)\n",
            "Requirement already satisfied: descartes in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (1.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (4.33.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from matplotlib->nuscenes-devkit) (3.0.4)\n",
            "Requirement already satisfied: six in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from fire->nuscenes-devkit) (1.16.0)\n",
            "Requirement already satisfied: termcolor in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from fire->nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: ipykernel in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (6.9.1)\n",
            "Requirement already satisfied: qtconsole in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (5.3.0)\n",
            "Requirement already satisfied: jupyter-console in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (6.4.3)\n",
            "Requirement already satisfied: ipywidgets in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (7.6.5)\n",
            "Requirement already satisfied: notebook in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (6.4.8)\n",
            "Requirement already satisfied: nbconvert in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter->nuscenes-devkit) (6.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from scikit-learn->nuscenes-devkit) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from scikit-learn->nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tqdm->nuscenes-devkit) (0.4.4)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (1.5.1)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (6.1)\n",
            "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (7.2.2)\n",
            "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (8.2.0)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (1.5.5)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (0.1.2)\n",
            "Requirement already satisfied: traitlets<6.0,>=5.1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipykernel->jupyter->nuscenes-devkit) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipywidgets->jupyter->nuscenes-devkit) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipywidgets->jupyter->nuscenes-devkit) (3.5.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipywidgets->jupyter->nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipywidgets->jupyter->nuscenes-devkit) (5.3.0)\n",
            "Requirement already satisfied: pygments in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter-console->jupyter->nuscenes-devkit) (2.11.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter-console->jupyter->nuscenes-devkit) (3.0.20)\n",
            "Requirement already satisfied: testpath in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.5.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (3.0.3)\n",
            "Requirement already satisfied: bleach in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (4.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (4.11.1)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.5.13)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.4)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.8.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (0.1.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (1.5.0)\n",
            "Requirement already satisfied: jupyter-core in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbconvert->jupyter->nuscenes-devkit) (4.9.2)\n",
            "Requirement already satisfied: prometheus-client in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from notebook->jupyter->nuscenes-devkit) (0.13.1)\n",
            "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from notebook->jupyter->nuscenes-devkit) (0.13.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from notebook->jupyter->nuscenes-devkit) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from notebook->jupyter->nuscenes-devkit) (21.3.0)\n",
            "Requirement already satisfied: pyzmq>=17 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from notebook->jupyter->nuscenes-devkit) (22.3.0)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from qtconsole->jupyter->nuscenes-devkit) (2.0.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.2.0)\n",
            "Requirement already satisfied: backcall in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (61.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (5.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jinja2>=2.4->nbconvert->jupyter->nuscenes-devkit) (2.0.1)\n",
            "Requirement already satisfied: pywin32>=1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jupyter-core->nbconvert->jupyter->nuscenes-devkit) (302)\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (4.4.0)\n",
            "Requirement already satisfied: fastjsonschema in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (2.15.1)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->nuscenes-devkit) (0.2.5)\n",
            "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->nuscenes-devkit) (2.0.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from argon2-cffi->notebook->jupyter->nuscenes-devkit) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->nuscenes-devkit) (2.3.1)\n",
            "Requirement already satisfied: webencodings in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from bleach->nbconvert->jupyter->nuscenes-devkit) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.8.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (0.18.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->nuscenes-devkit) (1.15.0)\n",
            "Requirement already satisfied: asttokens in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (2.0.5)\n",
            "Requirement already satisfied: executing in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.8.3)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->nuscenes-devkit) (0.2.2)\n",
            "Requirement already satisfied: pycparser in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->nuscenes-devkit) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->nuscenes-devkit) (3.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pytorch-lightning in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.8.* in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (1.11.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (2022.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (4.64.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: requests in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.27.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from packaging>=17.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.20.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.6.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (61.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from tqdm>=4.41.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: six in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: kmeans-pytorch in c:\\users\\gianfranco\\anaconda3\\envs\\torch2022\\lib\\site-packages (0.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nuscenes-devkit\n",
        "%pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8tTIrgJRPOjo"
      },
      "outputs": [],
      "source": [
        "# Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, resnet34\n",
        "from torchvision.transforms import Normalize\n",
        "from torchmetrics import functional\n",
        "from sklearn.cluster import KMeans\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# Math\n",
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "from nuscenes.nuscenes import NuScenes\n",
        "from nuscenes.prediction import PredictHelper\n",
        "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
        "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
        "from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
        "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
        "from nuscenes.eval.prediction.config import PredictionConfig, load_prediction_config\n",
        "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
        "from nuscenes.eval.prediction import metrics, data_classes\n",
        "\n",
        "# File system\n",
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import zipfile\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.utils.testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# Generic\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from collections import defaultdict\n",
        "from abc import abstractmethod\n",
        "import multiprocessing as mp\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hftZeWZYE3"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5fNsDEfaMN3"
      },
      "source": [
        "**Generic Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HfRgW1VNZX-D"
      },
      "outputs": [],
      "source": [
        "# Environment-dependent parameters\n",
        "if ENVIRONMENT == 'colab':\n",
        "    ROOT = '/content/drive/MyDrive/DL/Trajectory-Prediction-PyTorch/'\n",
        "    MAX_NUM_WORKERS = 0\n",
        "    MAX_BATCH_SIZE = 8\n",
        "    PROGRESS_BAR_REFRESH_RATE = 20\n",
        "elif ENVIRONMENT == 'local':\n",
        "    ROOT = os.getcwd()\n",
        "    # TODO: solve problem with VRAM with PL\n",
        "    if os.name == 'nt':\n",
        "        MAX_NUM_WORKERS = 0\n",
        "        MAX_BATCH_SIZE = 16\n",
        "    else:\n",
        "        MAX_NUM_WORKERS = 4\n",
        "        MAX_BATCH_SIZE = 8\n",
        "    PROGRESS_BAR_REFRESH_RATE = 10\n",
        "else:\n",
        "    raise ValueError(\"Wrong 'environment' value\")\n",
        "\n",
        "# Train parameters\n",
        "BATCH_SIZE = MAX_BATCH_SIZE\n",
        "NUM_WORKERS = MAX_NUM_WORKERS\n",
        "LEARNING_RATE = 1e-4\n",
        "MOMENTUM = 0.9\n",
        "TRAIN_EPOCHES = 20 \n",
        "PLOT_PERIOD = 1     # 1 = plot at each epoch\n",
        "LOGDIR = os.path.join(ROOT, 'logdir')\n",
        "CHECKPOINT_DIR = os.path.join(ROOT, 'checkpoints')\n",
        "BEST_CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, 'best')\n",
        "CHECKPOINT_MONITOR = \"val_loss\"\n",
        "TOP_K_SAVE = 10\n",
        "\n",
        "# Test parameters\n",
        "DEBUG_MODE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rFOOPSWgiOG"
      },
      "source": [
        "**Network Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iS5EccU9giHb"
      },
      "outputs": [],
      "source": [
        "PREDICTION_MODEL = 'P2T'\n",
        "AGENT_HISTORY = 1\n",
        "SHORT_TERM_HORIZON = 3\n",
        "LONG_TERM_HORIZON = 6\n",
        "TRAJ_HORIZON = SHORT_TERM_HORIZON\n",
        "if PREDICTION_MODEL == 'CoverNet':\n",
        "    # - Architecture parameters\n",
        "    BACKBONE_WEIGHTS = 'ImageNet'\n",
        "    BACKBONE_MODEL = 'ResNet18'\n",
        "    K_SIZE = 20000\n",
        "    # - Trajectory parameters\n",
        "    TRAJ_LINK = 'https://www.nuscenes.org/public/nuscenes-prediction-challenge-trajectory-sets.zip'\n",
        "    TRAJ_DIR = os.path.join(ROOT, 'trajectory_sets')\n",
        "    EPSILON = 2\n",
        "elif PREDICTION_MODEL == 'P2T':\n",
        "    # - RL parameters\n",
        "    INITIAL_STATE = [19, 12]\n",
        "    POLICY_SAMPLES = 200\n",
        "    MDP_HORIZON = 40\n",
        "    # - Reward Model parameters\n",
        "    TRAIN_RM_EPOCHES = 100 #(?) \n",
        "    REWARD_MODEL_LR = 0.0001\n",
        "    RM_LOGDIR = os.path.join(LOGDIR, 'reward_model')\n",
        "    # - Trajectory Generator parameters\n",
        "    PRETRAIN_TG_EPOCHES = 100\n",
        "    TRAIN_TG_EPOCHES = 400\n",
        "    TRAJ_HIDDEN_SIZE = 32\n",
        "    PLAN_HIDDEN_SIZE = 32\n",
        "    ATT_HIDDEN_SIZE = 32\n",
        "    POS_EMBEDDING_SIZE = 16\n",
        "    SCENE_EMBEDDING_SIZE = 32\n",
        "    AGENT_EMBEDDING_SIZE = 16\n",
        "    SCENE_FEATURES_SIZE = 64\n",
        "    AGENT_FEATURES_SIZE = 4\n",
        "    DYN_FEATURES_SIZE: 3\n",
        "    ACTIVATION_SLOPE: 0.1\n",
        "    TRAJ_GEN_LR_PRE: 0.001\n",
        "    TRAJ_GEN_LR: 0.0001\n",
        "    TRAJ_CLUSTERS = 10\n",
        "    MAX_CLIP_NORM = 10\n",
        "    PRE_TG_LOGDIR = os.path.join(LOGDIR, 'traj_generator', 'pretrain')\n",
        "    FT_TG_LOGDIR = os.path.join(LOGDIR, 'traj_generator', 'finetune')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXoj0mI-aO--"
      },
      "source": [
        "**Dataset Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "btLY02YCaO4Y"
      },
      "outputs": [],
      "source": [
        "# Organization parameters\n",
        "HELPER_NEEDED = False\n",
        "PREPARE_DATASET = False\n",
        "PREPROCESSED = True\n",
        "\n",
        "# File system parameters\n",
        "PL_SEED = 42\n",
        "DATAROOT = os.path.join(ROOT, 'data', 'sets', 'nuscenes')\n",
        "PREPROCESSED_FOLDER = 'preprocessed'\n",
        "GT_SUFFIX = '-gt'\n",
        "FILENAME_EXT = '.pt'\n",
        "ADDITIONAL_EXT = '.npy'\n",
        "DATASET_VERSION = 'v1.0-trainval'\n",
        "AGGREGATORS = [{'name': \"RowMean\"}]\n",
        "\n",
        "# Other parameters\n",
        "MAX_PREDICTED_MODES = 25\n",
        "SAMPLES_PER_SECOND = 2\n",
        "NORMALIZATION = 'imagenet'\n",
        "GRID_EXTENT = [-25, 25, -10, 40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Vxmwu00dEd"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfvjWlfxa6Gy"
      },
      "source": [
        "**Initialization**\n",
        "\n",
        "N.B: The download links in function *urllib.request.urlretrieve()* should be replaced periodically because it expires. Steps to download correctly are (on Firefox):\n",
        "\n",
        "\n",
        "1.   Dowload Map Expansion pack (or Trainval metadata) from the website\n",
        "2.   Stop the download\n",
        "3.   Right-click on the file -> copy download link\n",
        "4.   Paste the copied link into the first argument of the urlretrieve function. The second argument is the final name of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHQCO92UCoCD",
        "outputId": "8419e9f9-7ee9-4f94-dda6-0d75c7120761"
      },
      "outputs": [],
      "source": [
        "# Drive initialization\n",
        "if ENVIRONMENT == 'colab':\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uJ6oVSHz0iCf"
      },
      "outputs": [],
      "source": [
        "if PREPARE_DATASET:\n",
        "\n",
        "    # Creating dataset dir\n",
        "    os.makedirs(DATAROOT, exist_ok=True)\n",
        "    os.chdir(DATAROOT)\n",
        "\n",
        "    # Downloading Map Expansion Pack\n",
        "    os.mkdir('maps')\n",
        "    os.chdir('maps')\n",
        "    print(\"Downloading and extracting Map Expansion pack ...\")\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/data.nuscenes.org/public/v1.0/nuScenes-map-expansion-v1.3.zip?AWSAccessKeyId=AKIA6RIK4RRMFUKM7AM2&Signature=AvzxB6d7CxtpCUYIUChItvDSA3Q%3D&Expires=1651141974', 'nuScenes-map-expansion-v1.3.zip')\n",
        "    with zipfile.ZipFile('nuScenes-map-expansion-v1.3.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(os.getcwd())\n",
        "    os.remove('nuScenes-map-expansion-v1.3.zip')\n",
        "\n",
        "    # Downloading Trainval Metadata\n",
        "    os.chdir('..')\n",
        "    print(\"Downloading and extracting TrainVal metadata ...\")\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/data.nuscenes.org/public/v1.0/v1.0-trainval_meta.tgz?AWSAccessKeyId=AKIA6RIK4RRMFUKM7AM2&Signature=ZDr9UgOoV3UpYCI5RCY%2BNKiZVZ4%3D&Expires=1651142002', 'v1.0-trainval_meta.tgz')\n",
        "    tar_ref = tarfile.open('v1.0-trainval_meta.tgz', 'r:gz')\n",
        "    tar_ref.extractall(os.getcwd())\n",
        "    tar_ref.close()\n",
        "    os.remove('v1.0-trainval_meta.tgz')\n",
        "    os.chdir(DATAROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75UDbRMTbA9V"
      },
      "source": [
        "**Dataset definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mHqZde4mbDkC"
      },
      "outputs": [],
      "source": [
        "class TrajPredDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" Trajectory Prediction Dataset\n",
        "\n",
        "    Base Class for Trajectory Prediction Datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, name, data_type, preprocessed, split,\n",
        "                 dataroot, preprocessed_folder, filename_ext, additional_ext,\n",
        "                 gt_suffix, traj_horizon, max_traj_horizon, num_workers):\n",
        "        \"\"\" Dataset Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset: the instantiated dataset\n",
        "        name: name of the dataset\n",
        "        data_type: data type of the dataset elements\n",
        "        preprocessed: True if data has already been preprocessed\n",
        "        split: the dataset split ('train', 'train_val', 'val')\n",
        "        dataroot: the root directory of the dataset\n",
        "        preprocessed_folder: the folder containing preprocessed data\n",
        "        filename_ext: the extension of the generated filenames\n",
        "        additional_ext: the extenstion of the generated additional files\n",
        "        gt_suffix: the suffix added after each GT filename (before ext)\n",
        "        traj_horizon: horizon (in seconds) for the future trajectory\n",
        "        max_traj_horizon: maximum trajectory horizon possible (in seconds)\n",
        "        num_workers: num of processes that collect data\n",
        "        \"\"\"\n",
        "        super(TrajPredDataset, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.name = name\n",
        "        self.data_type = data_type\n",
        "        self.preprocessed = preprocessed\n",
        "        self.split = split\n",
        "        self.dataroot = dataroot\n",
        "        self.preprocessed_folder = preprocessed_folder\n",
        "        self.filename_ext = filename_ext\n",
        "        self.additional_ext = additional_ext\n",
        "        self.gt_suffix = gt_suffix\n",
        "        self.traj_horizon = traj_horizon\n",
        "        self.max_traj_horizon = max_traj_horizon\n",
        "        self.num_workers = num_workers\n",
        "        self.helper = None\n",
        "        self.tokens = None\n",
        "        self.static_layer_rasterizer = None\n",
        "        self.agent_rasterizer = None\n",
        "        self.input_representation = None\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Return the size of the dataset \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Return an element of the dataset \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_data(self):\n",
        "        \"\"\" Data generation\n",
        "\n",
        "        If self.preprocessed, directly collect data.\n",
        "        Otherwise, generate data without preprocess it.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_raster(self, token):\n",
        "        \"\"\" Convert a token split into a raster\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token: token containing instance token and sample token\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        raster: the raster image\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class TrajPredictionDataModule(pl.LightningDataModule):\n",
        "    \"\"\" PyTorch Lightning Data Module for the Trajectory Prediction Problem \"\"\"\n",
        "    def __init__(self, train_dataset, val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "        \"\"\" Data Module initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataset: instance of the train dataset class\n",
        "        nuscenes_val: instance of the validation dataset class \n",
        "        batch_size: number of samples to extract from the dataset at each step\n",
        "        num_workers: number of cores implied in data collection\n",
        "        \"\"\"\n",
        "        super(TrajPredictionDataModule, self).__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "    \n",
        "    @abstractmethod\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\" Setup the data module \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_dataloader(self):\n",
        "        \"\"\" Dataloader for the training part \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def val_dataloader(self):\n",
        "        \"\"\" Dataloader for the validation part \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def test_dataloader(self):\n",
        "        \"\"\" Dataloader for the testing part \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class nuScenesDataset(TrajPredDataset):\n",
        "    \"\"\" nuScenes Dataset for Trajectory Prediction challenge \"\"\"\n",
        "    def __init__(self, helper, data_type='raster', preprocessed=False,\n",
        "                 split='train', include_static=False, include_add_data=False,\n",
        "                 dataroot=DATAROOT, preprocessed_folder=PREPROCESSED_FOLDER,\n",
        "                 filename_ext=FILENAME_EXT, additional_ext=ADDITIONAL_EXT,\n",
        "                 gt_suffix=GT_SUFFIX, traj_horizon=TRAJ_HORIZON, \n",
        "                 max_traj_horizon=LONG_TERM_HORIZON, samples_per_second=SAMPLES_PER_SECOND,\n",
        "                 agent_history=AGENT_HISTORY, normalization=NORMALIZATION, \n",
        "                 grid_extent=GRID_EXTENT, num_workers=NUM_WORKERS):\n",
        "        \"\"\" nuScenes Dataset Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        helper: the helper of the instantiated nuScenes dataset (None if not needed)\n",
        "        data_type: data type of the dataset elements\n",
        "        preprocessed: True if data has already been preprocessed\n",
        "        split: the dataset split ('train', 'train_val', 'val')\n",
        "        include_static: if to return also static rasters in __getitem__\n",
        "        include_add_data: if to return also additional data in __getitem__\n",
        "        dataroot: the root directory of the dataset\n",
        "        preprocessed_folder: the folder containing preprocessed data\n",
        "        filename_ext: the extension of the generated filenames\n",
        "        additional_ext: the extenstion of the generated additional files\n",
        "        gt_suffix: the suffix added after each GT filename (before ext)\n",
        "        traj_horizon: horizon (in seconds) for the future trajectory\n",
        "        max_traj_horizon: maximum trajectory horizon possible (in seconds)\n",
        "        samples_per_second: sampling frequency (in Hertz)\n",
        "        agent_history: the seconds of considered agent history\n",
        "        normalization: which kind of normalization to apply to input\n",
        "        grid_extent: extension of the grid for eventual map discretization\n",
        "        num_workers: num of processes that collect data\n",
        "        \"\"\"\n",
        "        # General initialization\n",
        "        super(nuScenesDataset, self).__init__(\n",
        "            None, 'nuScenes', data_type, preprocessed, split, dataroot, preprocessed_folder, \n",
        "            filename_ext, additional_ext, gt_suffix, traj_horizon, max_traj_horizon, num_workers)\n",
        "        self.include_static = include_static\n",
        "        self.include_add_data = include_add_data\n",
        "        self.agent_history = agent_history\n",
        "        self.grid_extent = grid_extent\n",
        "        self.helper = helper\n",
        "        self.tokens = get_prediction_challenge_split(\n",
        "            split, dataroot=dataroot)\n",
        "        self.samples_per_second = samples_per_second\n",
        "        if data_type == 'raster':\n",
        "            if helper is not None:\n",
        "                self.static_layer_rasterizer = StaticLayerRasterizer(self.helper)\n",
        "                self.agent_rasterizer = AgentBoxesWithFadedHistory(\n",
        "                    self.helper, seconds_of_history=1)\n",
        "                self.input_representation = InputRepresentation(\n",
        "                    self.static_layer_rasterizer, self.agent_rasterizer, Rasterizer())\n",
        "            else:\n",
        "                self.static_layer_rasterizer = None\n",
        "                self.agent_rasterizer = None\n",
        "                self.input_representation = None\n",
        "        else:   # NOTE: possible also other type of input data\n",
        "            pass\n",
        "        if not self.preprocessed:\n",
        "            print(\"Preprocessing data ...\")\n",
        "            self.generate_data()\n",
        "\n",
        "        # Normalization function\n",
        "        if normalization == 'imagenet':\n",
        "            self.normalization = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        else:\n",
        "            raise ValueError(\"Available only 'imagenet' normalization\")\n",
        "            \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\" Return the size of the dataset \"\"\"\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
        "        \"\"\" Return an element of the dataset \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: index of the element\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        agent_state_vector: vector [velocity, acceleration, yaw rate] of the target agent\n",
        "        raster_img_static: raster map of the scene, with only static element (empty if not self.include_static)\n",
        "        raster_img_dynamic: raster map of the scene, complete with dynamic elements\n",
        "        gt_trajectory: ground truth of the agent (agent future)\n",
        "        idx: index of the element\n",
        "        \"\"\"\n",
        "        # Select subfolder\n",
        "        if idx < 0:\n",
        "            idx = len(self) + idx\n",
        "        subfolder = f'batch_{idx//128}'\n",
        "\n",
        "        # Load files\n",
        "        complete_tensor = torch.load(\n",
        "            os.path.join(self.dataroot, self.preprocessed_folder, self.split,\n",
        "                         subfolder, self.tokens[idx] + self.filename_ext))\n",
        "        gt_trajectory = torch.load(\n",
        "            os.path.join(self.dataroot, self.preprocessed_folder, self.split, subfolder,\n",
        "                         self.tokens[idx] + self.gt_suffix + self.filename_ext))\n",
        "\n",
        "        # Separate state and rasters\n",
        "        agent_state_vector, raster_img = self.tensor_io_conversion(\n",
        "            \"read\", None, None, complete_tensor)\n",
        "\n",
        "        # Include static data\n",
        "        if not self.include_static:\n",
        "            raster_img_dynamic = raster_img\n",
        "            raster_img_static = torch.empty((0,))\n",
        "        else:\n",
        "            # TODO: adapt also to non-square images\n",
        "            raster_img_static, raster_img_dynamic = \\\n",
        "                raster_img.split(raster_img.shape[-1], dim=1)\n",
        "        \n",
        "        # Normalization\n",
        "        raster_img_static = self.normalization(raster_img_static)\n",
        "        raster_img_dynamic = self.normalization(raster_img_dynamic)\n",
        "\n",
        "        return agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\" Data generation\n",
        "\n",
        "        If self.preprocessed, directly collect data.\n",
        "        Otherwise, generate data without preprocess it.\n",
        "        \"\"\"\n",
        "        # Generate directories if don't exist\n",
        "        preprocessed_dir = os.path.join(self.dataroot, self.preprocessed_folder)\n",
        "        split_dir = os.path.join(preprocessed_dir, self.split)\n",
        "        if self.preprocessed_folder not in os.listdir(self.dataroot):\n",
        "            os.mkdir(preprocessed_dir)\n",
        "        if self.split not in os.listdir(preprocessed_dir):\n",
        "            os.mkdir(split_dir)\n",
        "\n",
        "        # Variable useful to restore interrupted preprocessing\n",
        "        preprocessed_batches = os.listdir(split_dir)\n",
        "        already_preproc = \\\n",
        "            len([f for f in preprocessed_batches\n",
        "                 if os.path.isfile(os.path.join(split_dir, f))])\n",
        "\n",
        "        # Create subfolders\n",
        "        if len(preprocessed_batches) == 0:\n",
        "            n_subfolders = len(self.tokens) // 128 + int(len(self.tokens) % 128 != 0)\n",
        "            for i in range(n_subfolders):\n",
        "                subfolder = 'batch_' + str(i)\n",
        "                os.mkdir(os.path.join(split_dir, subfolder))\n",
        "\n",
        "        # Generate data\n",
        "        if self.data_type == 'raster':\n",
        "            for i, t in enumerate(tqdm(self.tokens)):\n",
        "                subfolder = f'batch_{i//128}'\n",
        "                if i >= int(already_preproc/2):\n",
        "                    self.generate_raster_data(t, split_dir, subfolder)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def generate_raster_data(self, token, batches_dir, subfolder):\n",
        "        \"\"\" Generate raster and agent data from a dataset token\n",
        "\n",
        "        The generated input data consists in a tensor like this:\n",
        "            [raster map | agent state vector]\n",
        "        The generated ground truth data is the future agent trajectory tensor\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token: token containing instance token and sample token\n",
        "        batches_dir: the directory in which the batches will be generated\n",
        "        subfolder: the data is divided into subfolders in order to avoid Drive timeouts;\n",
        "            this parameter tells which is the actual subfolder towhere place data\n",
        "        \"\"\"\n",
        "        # Generate dynamic raster image, state and GT\n",
        "        instance_token, sample_token = token.split(\"_\")\n",
        "        raster_img = self.input_representation.make_input_representation(\n",
        "            instance_token, sample_token)\n",
        "        raster_tensor = torch.Tensor(raster_img).permute(2, 0, 1) / 255.\n",
        "        agent_state_vector = torch.Tensor(\n",
        "            [[self.helper.get_velocity_for_agent(instance_token, sample_token),\n",
        "              self.helper.get_acceleration_for_agent(instance_token, sample_token),\n",
        "              self.helper.get_heading_change_rate_for_agent(instance_token, sample_token)]])\n",
        "        gt_trajectory = torch.Tensor(\n",
        "            self.helper.get_future_for_agent(instance_token, sample_token,\n",
        "                                             seconds=self.max_traj_horizon, in_agent_frame=True))\n",
        "\n",
        "        # Generate additional data\n",
        "        if self.include_add_data:\n",
        "            future_indefinite = torch.Tensor(\n",
        "                self.helper.get_future_for_agent(instance_token, sample_token,\n",
        "                                                 seconds=300, in_agent_frame=True))\n",
        "            xy_past = torch.Tensor(\n",
        "                self.helper.get_past_for_agent(instance_token, sample_token,\n",
        "                                               seconds=self.agent_history, in_agent_frame=True))\n",
        "            complete_past = torch.Tensor(\n",
        "                self.helper.get_past_for_agent(instance_token, sample_token,\n",
        "                                               seconds=self.agent_history, in_agent_frame=True,\n",
        "                                               just_xy=False))\n",
        "            annotations = self.helper.get_annotations_for_sample(sample_token)\n",
        "            sample_annotation = self.helper.get_sample_annotation(instance_token, sample_token)\n",
        "            additional_dict = {\n",
        "                'future_indefinite': future_indefinite,\n",
        "                'xy_past': xy_past,\n",
        "                'complete_past': complete_past,\n",
        "                'annotations': annotations,\n",
        "                'sample_annotation': sample_annotation\n",
        "            }\n",
        "            np.save(os.path.join(batches_dir, subfolder, token + 'add' + self.additional_ext), additional_dict)\n",
        "\n",
        "        # Handle incomplete GT and nan values\n",
        "        while gt_trajectory.shape[0] < self.samples_per_second * self.max_traj_horizon:\n",
        "            gt_trajectory = torch.concat((gt_trajectory, gt_trajectory[-1].unsqueeze(0)))\n",
        "        gt_trajectory = gt_trajectory[:(self.samples_per_second * self.traj_horizon)]\n",
        "        nan_mask = agent_state_vector != agent_state_vector\n",
        "        if nan_mask.any():\n",
        "            agent_state_vector[nan_mask] = 0\n",
        "\n",
        "        # Generate static raster image\n",
        "        if self.include_static:     \n",
        "            raster_img_static = \\\n",
        "                self.static_layer_rasterizer.make_representation(instance_token, sample_token)\n",
        "            raster_tensor_static = torch.Tensor(raster_img_static).permute(2, 0, 1) / 255.\n",
        "            raster_tensor = torch.cat([raster_tensor_static, raster_tensor], dim=1)\n",
        "\n",
        "        # Concatenate and save to disk\n",
        "        raster_agent_tensor, _ = \\\n",
        "            self.tensor_io_conversion('write', raster_tensor, agent_state_vector)\n",
        "        torch.save(raster_agent_tensor, os.path.join(\n",
        "            batches_dir, subfolder, token + self.filename_ext))\n",
        "        torch.save(gt_trajectory, os.path.join(\n",
        "            batches_dir, subfolder, token + self.gt_suffix + self.filename_ext))\n",
        " \n",
        "    @staticmethod\n",
        "    def tensor_io_conversion(mode, big_t=None, small_t=None, complete_t=None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Utility IO function to concatenate tensors of different shape\n",
        "\n",
        "        Normally used to concatenate (or separate) raster map and agent state vector in order to speed up IO\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mode: 'write' (concatenate) or 'read' (separate)\n",
        "        big_t: the bigger tensor (None if we are going to separate tensors)\n",
        "        small_t: the smaller tensor (None if we are going to separate tensors)\n",
        "        complete_t: the concatenated tensor (None if we are going to concatenate tensors)\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        out1: small tensor (mode == 'read') or complete tensor (mode == 'write')\n",
        "        out2: big tensor (mode == 'read') or empty tensor (mode == 'write') \n",
        "        \"\"\"\n",
        "        out1, out2 = None, None\n",
        "        if mode == 'write':    # concatenate\n",
        "            if big_t is None or small_t is None:\n",
        "                raise ValueError(\"Wrong argument: 'big_t' and 'small_t' cannot be None\")\n",
        "            small_t = small_t.permute(1, 0).unsqueeze(2)\n",
        "            small_t = small_t.expand(-1, -1, big_t.shape[-1])\n",
        "            out1 = torch.cat((big_t, small_t), dim=1)\n",
        "            out2 = torch.empty(small_t.shape)\n",
        "        elif mode == 'read':    # separate\n",
        "            if complete_t is None:\n",
        "                raise ValueError(\"Wrong argument: 'complete_t' cannot be None\")\n",
        "            out1 = complete_t[..., -1, -1].unsqueeze(0)\n",
        "            out2 = complete_t[..., :-1, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Wrong argument 'mode'; available 'read' or 'write'\")\n",
        "        return out1, out2\n",
        "\n",
        "class nuScenesDataModule(TrajPredictionDataModule):\n",
        "    \"\"\" PyTorch Lightning Data Module for the nuScenes dataset \"\"\"\n",
        "    def __init__(self, nuscenes_train, nuscenes_val, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "        \"\"\" Data Module initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        nuscenes_train: instance of the nuScenesDataset class (split='train')\n",
        "        nuscenes_val: instance of the nuScenesDataset class (split='val')\n",
        "        batch_size: number of samples to extract from the dataset at each step\n",
        "        num_workers: number of cores implied in data collection\n",
        "        \"\"\"\n",
        "        super(nuScenesDataModule, self).__init__(\n",
        "            nuscenes_train, nuscenes_val, batch_size, num_workers)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\" Setup the data module \"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.nusc_train = self.train_dataset\n",
        "            self.nusc_val = self.val_dataset\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.nusc_test = self.val_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\" Dataloader for the training part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_train, self.batch_size, shuffle=True,\n",
        "                                           num_workers=self.num_workers, drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\" Dataloader for the validation part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_val, self.batch_size, shuffle=False, \n",
        "                                           num_workers=self.num_workers, drop_last=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\" Dataloader for the testing part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_test, self.batch_size, shuffle=False,\n",
        "                                           num_workers=self.num_workers, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isw61mh9-IZD"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blyIPM3r-TgN"
      },
      "source": [
        "**Covernet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_OGzCNn-TZA"
      },
      "outputs": [],
      "source": [
        "class CoverNet(pl.LightningModule):\n",
        "    \"\"\" CoverNet model for Trajectory Prediction \"\"\"\n",
        "    def __init__(self, K_size, epsilon, traj_link, traj_dir, device, \n",
        "                 lr=LEARNING_RATE, momentum=MOMENTUM,\n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON):\n",
        "        \"\"\" CoverNet initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        K_size: number of modes (trajectories) (needed ?)\n",
        "        epsilon: value (in meters) relative to the space coverage\n",
        "        traj_link: link from which to download the trajectories\n",
        "        device: target device of the model (e.g. 'cuda:0')\n",
        "        lr: learning rate of the optimizer\n",
        "        momentum: momentum of the optimizer\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.K_size = K_size\n",
        "        self.convModel = resnet50(pretrained=True)\n",
        "        self.activation = {}\n",
        "        def get_activation(name):\n",
        "            def hook(model, input, output):\n",
        "                self.activation[name] = output\n",
        "            return hook\n",
        "        self.convModel.layer4.register_forward_hook(get_activation('layer4'))\n",
        "        self.trajectories = prepare_trajectories(epsilon, traj_link, traj_dir)\n",
        "        self.fc1 = nn.Linear(2051, 4096)\n",
        "        self.fc2 = nn.Linear(4096, self.trajectories.size()[0])\n",
        "        self.traj_samples = traj_samples\n",
        "        self.tgt_device = device\n",
        "        self.momentum = momentum\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        \"\"\" Network inference \"\"\"\n",
        "        img, state = x\n",
        "        self.convModel(img)\n",
        "        resnet_output = torch.flatten(self.convModel.avgpool(self.activation['layer4']),start_dim=1)\n",
        "        x = torch.cat([resnet_output, state], 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\" Training step of the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: batch of data\n",
        "        batch_idx: index of the actual batch (from 0 to len(dataset))\n",
        "        \"\"\"\n",
        "        # Collect data\n",
        "        x_state, _, x_img, gt, _ = batch\n",
        "        x_state = torch.flatten(x_state, 0, 1)\n",
        "        reduced_traj = self.trajectories[:, :self.traj_samples]\n",
        "        # Prepare positive samples\n",
        "        with torch.no_grad():\n",
        "            y = get_positives(reduced_traj, gt.to('cpu'))\n",
        "            y = y.to(self.tgt_device)\n",
        "        # Inference\n",
        "        y_hat = self((x_img, x_state))\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        # Log\n",
        "        self.log('train_loss', loss.item(), on_step=True)\n",
        "            \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\" Validation step of the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: batch of data\n",
        "        batch_idx: index of the actual batch (from 0 to len(dataset))\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Collect data\n",
        "            x_state, _, x_img, gt, _ = batch\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            reduced_traj = self.trajectories[:, :self.traj_samples]\n",
        "            # Prepare positive samples\n",
        "            y = get_positives(reduced_traj, gt.to('cpu'))\n",
        "            y = y.to(self.tgt_device)\n",
        "            # Inference\n",
        "            y_hat = self((x_img, x_state))\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "        # Log\n",
        "        self.log('val_loss', loss.item(), on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\" Set the optimizer for the model \"\"\"\n",
        "        # TODO: find best optimizer and parameters\n",
        "        #return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "\n",
        "# TODO: check if generated trajectory are expressed in the same frame of the agent\n",
        "def get_positives(trajectories, ground_truth) -> torch.Tensor:\n",
        "    \"\"\" Get positive samples wrt the actual GT\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trajectories: the pre-generated set of trajectories\n",
        "    ground_truth: the future trajectory for the agent\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    positive_traj: as defined in the original CoverNet paper, \n",
        "        'positive samples determined by the element in the trajectory set\n",
        "        closest to the actual ground truth in minimum average \n",
        "        of point-wise Euclidean distances'\n",
        "    \"\"\"\n",
        "    euclidean_dist = torch.stack([torch.pow(torch.sub(trajectories, gt), 2) \n",
        "                                  for gt in ground_truth]).sum(dim=3).sqrt() \n",
        "    mean_euclidean_dist = euclidean_dist.mean(dim=2)\n",
        "    positive_traj = mean_euclidean_dist.argmin(dim=1)\n",
        "    return positive_traj\n",
        "\n",
        "def prepare_trajectories(epsilon, download_link, directory) -> torch.Tensor:\n",
        "    \"\"\" Function to download and extract trajectory sets for CoverNet \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    epsilon: value (in meters) relative to the space coverage\n",
        "    download_link: link from which to download trajectory sets\n",
        "    directory: directory where to download trajectory sets\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    trajectories: tensor of the trajectory set for the specified epsilon\n",
        "    \"\"\"\n",
        "    # 1. Download and extract trajectories\n",
        "    filename_zip = 'nuscenes-prediction-challenge-trajectory-sets.zip'\n",
        "    filename = filename_zip[:-4]\n",
        "    filename_dir = os.path.join(directory, filename)\n",
        "    filename_zipdir = os.path.join(directory, filename_zip)\n",
        "    if (not os.path.isdir(filename_dir) \n",
        "        or any(e not in os.listdir(filename_dir)\n",
        "               for e in ['epsilon_2.pkl', 'epsilon_4.pkl', 'epsilon_8.pkl'])):\n",
        "        print(\"Downloading trajectories ...\")\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        urllib.request.urlretrieve(download_link, filename_zipdir)\n",
        "        with zipfile.ZipFile(filename_zipdir, 'r') as archive:\n",
        "            archive.extractall(directory)\n",
        "        os.remove(filename_zipdir)\n",
        "\n",
        "    # 2. Generate trajectories\n",
        "    traj_set_path = os.path.join(filename_dir, 'epsilon_' + str(epsilon) + '.pkl')\n",
        "    trajectories = pickle.load(open(traj_set_path, 'rb'))\n",
        "    return torch.Tensor(trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**P2T**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class P2T(nn.Module):\n",
        "    \"\"\" P2T Model for trajectory prediction \"\"\"\n",
        "    def __init__(self, mdp, mdp_horizon=MDP_HORIZON, initial_state=INITIAL_STATE, \n",
        "                 policy_samples=POLICY_SAMPLES, traj_clusters=TRAJ_CLUSTERS,\n",
        "                 from_checkpoint=False, pretrain_traj_gen=True, \n",
        "                 train_rm_epoches=TRAIN_RM_EPOCHES, pretrain_tg_epoches=PRETRAIN_TG_EPOCHES, \n",
        "                 train_tg_epoches=TRAIN_TG_EPOCHES, reward_model_lr=REWARD_MODEL_LR, \n",
        "                 traj_gen_lr_pre=TRAJ_GEN_LR_PRE, traj_gen_lr=TRAJ_GEN_LR, rm_logdir=RM_LOGDIR, \n",
        "                 pre_tg_logdir=PRE_TG_LOGDIR, ft_tg_logdir=FT_TG_LOGDIR, max_norm=MAX_CLIP_NORM):\n",
        "        \"\"\" P2T Model initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        mdp: Markov Decision Process class instance\n",
        "        mdp_horizon: horizon (in seconds) of the MDP\n",
        "        initial_state: initial state of the agent\n",
        "        policy_samples: how many samples to extract from the policy\n",
        "        traj_clusters: num of trajectory clusters\n",
        "        from_checkpoint: True if the model is instantiated from checkpoints\n",
        "        pretrain_traj_gen: True if the Trajectory Generator should be pretrained\n",
        "        train_rm_epoches: # epoches to train the Reward Model\n",
        "        pretrain_tg_epoches: # epoches to pre-train the Trajectory Generator\n",
        "        train_tg_epoches: # epoches to train (fine-tune) the Trajectory Generator\n",
        "        reward_model_lr: learning rate of the Reward Model\n",
        "        traj_gen_lr_pre: learning rate of the Trajectory Generator (pre-train)\n",
        "        traj_gen_lr: learning rate of the Trajectory Generator (fine-tune)\n",
        "        max_norm: maximum norm to clip gradients\n",
        "        \"\"\"\n",
        "        self.mdp = mdp\n",
        "        self.mdp_horizon = mdp_horizon\n",
        "        self.initial_state = initial_state\n",
        "        self.policy_samples = policy_samples\n",
        "        self.traj_clusters = traj_clusters\n",
        "        self.rm_logger = SummaryWriter(rm_logdir)\n",
        "        self.pre_tg_logger = SummaryWriter(pre_tg_logdir)\n",
        "        self.ft_tg_logger = SummaryWriter(ft_tg_logdir)\n",
        "        self.pretrain_traj_gen = pretrain_traj_gen\n",
        "        self.pretrain_tg_epoches = pretrain_tg_epoches\n",
        "        self.train_rm_epoches = train_rm_epoches\n",
        "        self.train_tg_epoches = train_tg_epoches\n",
        "        self.reward_model_lr = reward_model_lr\n",
        "        self.traj_gen_lr_pre = traj_gen_lr_pre\n",
        "        self.traj_gen_lr = traj_gen_lr\n",
        "        self.max_norm = max_norm\n",
        "        if not from_checkpoint:\n",
        "            self.reward_model = RewardModel()\n",
        "            self.traj_generator = Trajectory_Generator(pretrain_traj_gen)\n",
        "        else:\n",
        "            # TODO: handle checkpoints loading\n",
        "            pass\n",
        "\n",
        "    def train(self, trainval_dm):\n",
        "        \"\"\" P2T function for training all components \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        print(\"Starting P2T training\")\n",
        "        start = time.time()\n",
        "        self.train_reward_model(trainval_dm)\n",
        "        self.train_traj_generator(trainval_dm)\n",
        "        print(\"P2T Model trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    def train_reward_model(self, trainval_dm):\n",
        "        \"\"\" Reward Model training \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        print(\"Starting P2T Reward Model training\")\n",
        "        start = time.time()\n",
        "        print(\"Reward Model trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def train_traj_generator(self, trainval_dm):\n",
        "        \"\"\" Trajectory Generator training\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        # Training initialization\n",
        "        print(\"Starting P2T Trajectory Generator training\")\n",
        "        start = time.time()\n",
        "        self.reward_model.eval()\n",
        "        loss = TrajGenLoss('train')\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.traj_generator.parameters(), lr=self.traj_gen_lr)\n",
        "        train_dataloader = trainval_dm.train_dataloader()\n",
        "        val_dataloader = trainval_dm.val_dataloader()\n",
        "\n",
        "        # Trajectory Generator pre-training\n",
        "        if self.pretrain_traj_gen:\n",
        "            self.pretrain_traj_generator(self, trainval_dm)\n",
        "\n",
        "        # Training Loop\n",
        "        for e in range(self.train_tg_epoches):\n",
        "            print(\"-------- [TG] Epoch %d --------\" % e)\n",
        "\n",
        "            # Train\n",
        "            self.traj_generator.train()\n",
        "            for b, data in enumerate(train_dataloader):\n",
        "                \n",
        "                # Data preparation\n",
        "                (x_img_static, gt, past, motion_feats, _, agents, _) = \\\n",
        "                    self.traj_data_preparation(data, trainval_dm)               \n",
        "\n",
        "                # Reward Model inference\n",
        "                rew_path, rew_goal, img_feats = self.reward_model(motion_feats, x_img_static)\n",
        "\n",
        "                # MaxEntropy Reinforcement Learning\n",
        "                grid_extent = trainval_dm.train_dataset.grid_extent\n",
        "                svf, policy = max_entropy_rl(self.mdp, self.initial_state, \n",
        "                                             rew_path.detach(), rew_goal.detach())\n",
        "                plan, scene_feats, agent_feats = \\\n",
        "                    sample_policy(self.mdp, self.initial_state, policy, \n",
        "                                  self.policy_samples, grid_extent, img_feats, agents)\n",
        "\n",
        "                # Trajectory Prediction\n",
        "                sc_feat_size = self.traj_generator.scene_feat_size\n",
        "                ag_feat_size = self.traj_generator.agent_feat_size\n",
        "                plan = plan.reshape(-1, self.mdp.horizon, 2).permute(1, 0, 2).to(device)\n",
        "                scene_feats = scene_feats.reshape(\n",
        "                    -1, self.mdp.horizon, sc_feat_size).permute(1, 0, 2).to(device)\n",
        "                agent_feats = agent_feats.reshape(\n",
        "                    -1, self.mdp.horizon, ag_feat_size).permute(1, 0, 2).to(device)\n",
        "                past = past.unsqueeze(2).repeat(1, 1, self.policy_samples, 1)\n",
        "                past = past.reshape(past.shape[0], -1, past.shape[3])\n",
        "                traj_pred = self.traj_generator(past, plan, scene_feats, agent_feats)\n",
        "\n",
        "                # K-Means trajectory clustering\n",
        "                # NOTE: problems with multiprocessing in Windows. Verify on Ubuntu\n",
        "                # TODO: try to avoid loops in final clustering\n",
        "                traj_pred = traj_pred.reshape(\n",
        "                    -1, self.num_samples, traj_pred.shape[1], traj_pred.shape[2])\n",
        "                traj_flat = traj_pred.flatten(-2).detach().cpu().numpy()\n",
        "                clust_ids = [self.kmeans_cluster(t, self.traj_clusters) for t in traj_flat]                \n",
        "                traj_clust = torch.empty(traj_pred.shape[0], self.traj_clusters, \n",
        "                                         traj_pred.shape[2], traj_pred.shape[3])\n",
        "                for n in range(traj_pred.shape[0]):\n",
        "                    centroids = torch.empty(\n",
        "                            self.traj_clusters, traj_pred.shape[2], traj_pred.shape[3])\n",
        "                    for cl in list(range(self.traj_clusters)):\n",
        "                        centroids[cl] = traj_pred[cl, np.where(clust_ids == cl)[0]].mean(dim=0)\n",
        "                    traj_clust[n] = centroids\n",
        "                traj_clust = traj_clust.to(device)\n",
        "                \n",
        "                # Learning\n",
        "                batch_loss = loss(traj_clust, gt)\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                a = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.traj_generator.parameters(), self.max_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Logging\n",
        "                loss_val = batch_loss.item()\n",
        "                iterations = e*len(train_dataloader) + b\n",
        "                self.ft_tg_logger.add_scalar('train loss', loss_val, iterations)\n",
        "                print(\"[TG %d] %d - train loss = %f\" % (e, b, loss_val)) \n",
        "\n",
        "            # Val\n",
        "            val_losses = []\n",
        "            self.traj_generator.eval()\n",
        "            for b, data in enumerate(val_dataloader):\n",
        "\n",
        "                # Data preparation\n",
        "                (x_img_static, gt, past, motion_feats, _, agents, _) = \\\n",
        "                    self.traj_data_preparation(data, trainval_dm)               \n",
        "\n",
        "                # Reward Model inference\n",
        "                rew_path, rew_goal, img_feats = self.reward_model(motion_feats, x_img_static)\n",
        "\n",
        "                # MaxEntropy Reinforcement Learning\n",
        "                grid_extent = trainval_dm.train_dataset.grid_extent\n",
        "                svf, policy = max_entropy_rl(self.mdp, self.initial_state, \n",
        "                                             rew_path.detach(), rew_goal.detach())\n",
        "                plan, scene_feats, agent_feats = \\\n",
        "                    sample_policy(self.mdp, self.initial_state, policy, \n",
        "                                  self.policy_samples, grid_extent, img_feats, agents)\n",
        "\n",
        "                # Trajectory Prediction\n",
        "                sc_feat_size = self.traj_generator.scene_feat_size\n",
        "                ag_feat_size = self.traj_generator.agent_feat_size\n",
        "                plan = plan.reshape(-1, self.mdp.horizon, 2).permute(1, 0, 2).to(device)\n",
        "                scene_feats = scene_feats.reshape(\n",
        "                    -1, self.mdp.horizon, sc_feat_size).permute(1, 0, 2).to(device)\n",
        "                agent_feats = agent_feats.reshape(\n",
        "                    -1, self.mdp.horizon, ag_feat_size).permute(1, 0, 2).to(device)\n",
        "                past = past.unsqueeze(2).repeat(1, 1, self.policy_samples, 1)\n",
        "                past = past.reshape(past.shape[0], -1, past.shape[3])\n",
        "                traj_pred = self.traj_generator(past, plan, scene_feats, agent_feats)\n",
        "\n",
        "                # K-Means trajectory clustering\n",
        "                # NOTE: problems with multiprocessing in Windows. Verify on Ubuntu\n",
        "                # TODO: try to avoid loops in final clustering\n",
        "                traj_pred = traj_pred.reshape(\n",
        "                    -1, self.num_samples, traj_pred.shape[1], traj_pred.shape[2])\n",
        "                traj_flat = traj_pred.flatten(-2).detach().cpu().numpy()\n",
        "                clust_ids = [self.kmeans_cluster(t, self.traj_clusters) for t in traj_flat]                \n",
        "                traj_clust = torch.empty(traj_pred.shape[0], self.traj_clusters, \n",
        "                                         traj_pred.shape[2], traj_pred.shape[3])\n",
        "                for n in range(traj_pred.shape[0]):\n",
        "                    centroids = torch.empty(\n",
        "                            self.traj_clusters, traj_pred.shape[2], traj_pred.shape[3])\n",
        "                    for cl in list(range(self.traj_clusters)):\n",
        "                        centroids[cl] = traj_pred[cl, np.where(clust_ids == cl)[0]].mean(dim=0)\n",
        "                    traj_clust[n] = centroids\n",
        "                traj_clust = traj_clust.to(device)\n",
        "\n",
        "                # Logging\n",
        "                loss_val = loss(traj_clust, gt).item()\n",
        "                val_losses.append(loss_val)\n",
        "                print(\"[TG %d] %d - val loss = %f\" % (e, b, loss_val))  \n",
        "\n",
        "            # Validation logging\n",
        "            self.ft_tg_logger.add_scalar('val loss', np.array(val_losses).mean(), e)                     \n",
        "        \n",
        "        self.ft_tg_logger.close()\n",
        "        print(\"Trajectory Generator trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    def pretrain_traj_generator(self, trainval_dm):\n",
        "        \"\"\" Trajectory Generator pre-training\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        # Training initialization\n",
        "        print(\"Starting P2T Trajectory Generator pre-training\")\n",
        "        start = time.time()\n",
        "        loss = TrajGenLoss('pretrain')\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.traj_generator.parameters(), lr=self.traj_gen_lr_pre)\n",
        "        train_dataloader = trainval_dm.train_dataloader()\n",
        "        val_dataloader = trainval_dm.val_dataloader()\n",
        "\n",
        "        # Training loop\n",
        "        for e in range(self.pretrain_tg_epoches):\n",
        "            print(\"-------- [TG (Pre)] Epoch %d --------\" % e)\n",
        "\n",
        "            # Train\n",
        "            self.traj_generator.train()\n",
        "            for b, data in enumerate(train_dataloader):\n",
        "\n",
        "                # Data preparation\n",
        "                (x_img_static, gt, past, motion_feats, plan_e, agents, grid_idcs) = \\\n",
        "                    self.traj_data_preparation(data, trainval_dm)\n",
        "                grid_idcs.to(device)\n",
        "                plan_e.to(device)\n",
        "\n",
        "                # Reward Model inference\n",
        "                _, _, img_feats = self.reward_model(x_img_static, motion_feats)\n",
        "\n",
        "                # Trajectory Prediction\n",
        "                scene_feats, agent_feats = \\\n",
        "                    extract_plan_features(grid_idcs, img_feats, agents)\n",
        "                traj_pred = self.traj_generator(past, plan_e, scene_feats, agent_feats)\n",
        "\n",
        "                # Learning\n",
        "                batch_loss = loss(traj_pred, gt)\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                a = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.traj_generator.parameters(), self.max_norm)\n",
        "                optimizer.step()    \n",
        "\n",
        "                # Logging\n",
        "                loss_val = batch_loss.item()\n",
        "                iterations = e*len(train_dataloader) + b\n",
        "                self.pre_tg_logger.add_scalar('train loss', loss_val, iterations)\n",
        "                print(\"[TG (pre) %d] %d - train loss = %f\" % (e, b, loss_val)) \n",
        "\n",
        "            # Val\n",
        "            val_losses = []\n",
        "            self.traj_generator.eval()\n",
        "            for b, data in enumerate(val_dataloader):\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Data preparation\n",
        "                    (x_img_static, gt, past, motion_feats, plan_e, agents, grid_idcs) = \\\n",
        "                        self.traj_data_preparation(data, trainval_dm)\n",
        "                    grid_idcs.to(device)\n",
        "                    plan_e.to(device)\n",
        "\n",
        "                    # Reward Model inference\n",
        "                    _, _, img_feats = self.reward_model(x_img_static, motion_feats)\n",
        "\n",
        "                    # Trajectory Prediction\n",
        "                    scene_feats, agent_feats = \\\n",
        "                        extract_plan_features(grid_idcs, img_feats, agents)\n",
        "                    traj_pred = self.traj_generator(past, plan_e, scene_feats, agent_feats)\n",
        "\n",
        "                    # Logging\n",
        "                    loss_val = loss(traj_pred, gt).item()\n",
        "                    val_losses.append(loss_val)\n",
        "                    print(\"[TG (pre) %d] %d - val loss = %f\" % (e, b, loss_val))                    \n",
        "\n",
        "            # Validation logging\n",
        "            self.pre_tg_logger.add_scalar('val loss', np.array(val_losses).mean(), e)\n",
        "\n",
        "        self.pre_tg_logger.close()\n",
        "        print(\"Trajectory Generator pre-trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    def traj_data_preparation(self, data, datamodule):\n",
        "        \"\"\" Prepare data for Trajectory Generator training. Useful to avoid repeated code \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data: batch of data containing state, scene, future and index\n",
        "        datamodule: PL DataModule contatining trainval data\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        gt: future of the agent (ground truth)\n",
        "        past: history of the agent \n",
        "        motion_feats: motion and position features for the reward model\n",
        "        plan_e: coords of the grid cells relative to the SVF (expert)\n",
        "        agents: tensor with states of other agents in the scene\n",
        "        grid_idcs: grid coords of the SVF (expert)\n",
        "        \"\"\"\n",
        "        _, x_img_static, _, gt, _ = data\n",
        "        past, _, motion_feats, plan_e, agents, grid_idcs = \\\n",
        "            extract_expert_data(datamodule.train_dataset, data)\n",
        "        past = past.to(device)\n",
        "        motion_feats = motion_feats.to(device)\n",
        "        agents = agents.to(device)\n",
        "        x_img_static = x_img.to(device)\n",
        "        return (x_img_static, gt, past, motion_feats, \n",
        "                plan_e, agents, grid_idcs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" P2T inference \"\"\"\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def kmeans_cluster(data, n_clusters):\n",
        "        cluster_data = KMeans(n_clusters, n_init=1, max_iter=100).fit(data)\n",
        "        return cluster_data.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Reinforcement Learning part\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "def max_entropy_rl(mpd, initial_state, reward_path, reward_goal):\n",
        "    pass\n",
        "\n",
        "def sample_policy(mdp, initial_state, policy, policy_samples, grid_extent, img_feats, agents):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RewardModel(pl.LightningModule):\n",
        "    \"\"\" Model to extract rewards for Max-Ent RL\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = resnet34(pretrained=True)\n",
        "        self.cnn_feat = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu, backbone.maxpool, backbone.layer1)\n",
        "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
        "        self.cnn_1 = nn.Conv2d(in_channels=32+3, out_channels=32, kernel_size=1)\n",
        "        self.cnn_2 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
        "        # self.cnn_g_1 = nn.Conv2d(in_channels=32+3, out_channels=32, kernel_size=1)\n",
        "        # self.cnn_g_2 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
        "        self.cnn_p = nn.Sequential(self.cnn_1, self.cnn_2)\n",
        "        self.cnn_g = nn.Sequential(self.cnn_1, self.cnn_2)\n",
        "        \n",
        "        self.log_sig = nn.LogSigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        # TODO: define motion feats x and y\n",
        "        \"\"\"\n",
        "            y = (np.linspace(self.grid_extent[3] - grid_size_m/(self.grid_dim*2),\n",
        "                            self.grid_extent[2] + grid_size_m/(self.grid_dim*2),\n",
        "                            self.grid_dim)).reshape(-1, 1).repeat(self.grid_dim, axis=1)\n",
        "            x = (np.linspace(self.grid_extent[0] + grid_size_m/(self.grid_dim*2),\n",
        "                            self.grid_extent[1] - grid_size_m/(self.grid_dim*2),\n",
        "                            self.grid_dim)).reshape(-1, 1).repeat(self.grid_dim, axis=1).transpose()\n",
        "        \"\"\"\n",
        "        self.y = torch.linspace(40 - 50/(25*2),\n",
        "            -10 + 50/(25*2),\n",
        "            25).reshape(-1, 1).repeat_interleave(25, 1)\n",
        "        self.x = torch.linspace(-25 + 50/(25*2),\n",
        "            25 - 50/(25*2),\n",
        "            25).reshape(-1, 1).repeat_interleave(25, 1).transpose()\n",
        "        self.motion_feats = torch.zeros((3, 25, 25))\n",
        "\n",
        "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Network inference \"\"\"\n",
        "        # TODO: change state extraction\n",
        "        img, motion_feats = x\n",
        "\n",
        "        img_feats = self.cnn_feat(img)\n",
        "        img_feats = self.conv1(img_feats)\n",
        "        # TODO: check if relu needed\n",
        "        img_feats = self.relu(img_feats)\n",
        "\n",
        "        x = torch.cat([img_feats, motion_feats], 1)\n",
        "        r_path = self.log_sig(self.cnn_p(x))\n",
        "        r_goal = self.log_sig(self.cnn_g(x))\n",
        "        \n",
        "        return r_path, r_goal, img_feats\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\" Training step of the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: batch of data\n",
        "        batch_idx: index of the actual batch (from 0 to len(dataset))\n",
        "        \"\"\"\n",
        "        # Collect data\n",
        "        x_state, x_img, gt, _ = batch\n",
        "        print(x_state.shape())\n",
        "        v = x_state[:,0]\n",
        "        if np.isnan(v):\n",
        "            v = 0\n",
        "        self.motion_feats[0] = v\n",
        "        self.motion_feats[1] = self.x/50\n",
        "        self.motion_feats[2] = self.y/50\n",
        "\n",
        "        # Inference\n",
        "        y_hat = self((x_img, self.motion_feats))\n",
        "        \n",
        "        # TODO: log-likelihood loss\n",
        "        loss = None \n",
        "        # Log\n",
        "        self.log('train_loss', loss.item(), on_step=True)\n",
        "            \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: check all shapes and correct working\n",
        "# NOTE: in particular, check if permutation (1, 0, 2) is a problem or should be done\n",
        "class Trajectory_Generator(nn.Module):\n",
        "    \"\"\" Trajectory Generator class \"\"\"\n",
        "    def __init__(self, pretrain=True, traj_hidden_size=TRAJ_HIDDEN_SIZE, plan_hidden_size=PLAN_HIDDEN_SIZE,\n",
        "                 att_hidden_size=ATT_HIDDEN_SIZE, pos_embedding_size=POS_EMBEDDING_SIZE, \n",
        "                 scene_embedding_size=SCENE_EMBEDDING_SIZE, agent_embedding_size=AGENT_EMBEDDING_SIZE,\n",
        "                 scene_features_size=SCENE_FEATURES_SIZE, agent_features_size=AGENT_FEATURES_SIZE, \n",
        "                 dyn_features_size=DYN_FEATURES_SIZE, slope=ACTIVATION_SLOPE,\n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON, lr=TRAJ_GEN_LR_PRE):\n",
        "        \"\"\" Trajectory Generator initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        pretrain: True if the model should be first pretrained to speed up convergence\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        att_hidden_size: size of the hidden layer of the attention part in the final decoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding \n",
        "        scene_embedding_size: size of the linear layer for the scene embedding\n",
        "        agent_embedding_size: size of the linear layer for the agent embedding\n",
        "        scene_features_size: size of the scene features at each grid location\n",
        "        agent_features_size: size of the agent features at each grid location\n",
        "        dynamic_features_size: additional motion features to add to the embedding layer\n",
        "            0 -> x, y\n",
        "            1 -> x, y, velocity\n",
        "            2 -> x, y, velocity, acceleration\n",
        "            3 -> x, y, velocity, acceleration, yaw rate\n",
        "        slope: slope (positive or negative) of the activation function\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        self.scene_feat_size = scene_features_size\n",
        "        self.agent_feat_size = agent_features_size\n",
        "        super(Trajectory_Generator, self).__init__()\n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "        # ---------------------------------------| Generator Structure |-------------------------------------- # \n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "        self.motion_encoder = MotionEncoder(\n",
        "            traj_hidden_size, pos_embedding_size, \n",
        "            dyn_features_size, slope\n",
        "        )\n",
        "        self.plan_encoder = PlanEncoder(\n",
        "            plan_hidden_size, pos_embedding_size, \n",
        "            scene_embedding_size, agent_embedding_size,\n",
        "            scene_features_size, agent_features_size, slope\n",
        "        )\n",
        "        self.att_decoder = AttentionDecoder(\n",
        "            att_hidden_size, traj_hidden_size, plan_hidden_size, traj_samples\n",
        "        )\n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    def forward(self, hist_motion, plan, scene_feats, agent_feats):\n",
        "        \"\"\" Trajectory Generator inference\n",
        "        Input:\n",
        "            - hist_motion: history motion tensor\n",
        "            - plan: tensor of waypoints\n",
        "            - scene_feats: scene features\n",
        "            - agent_feats: agent features\n",
        "        Output: trajectory -> decoded trajectory tensor\n",
        "        \"\"\"\n",
        "        enc_motion = self.motion_encoder(hist_motion)\n",
        "        enc_plan = self.plan_encoder(plan, scene_feats, agent_feats)\n",
        "        trajectory = self.att_decoder(hist_motion, enc_motion, enc_plan)\n",
        "        return trajectory\n",
        "\n",
        "\n",
        "class MotionEncoder(nn.Module):\n",
        "    \"\"\" Motion Encoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, traj_hidden_size, pos_embedding_size, dyn_features_size, slope=ACTIVATION_SLOPE):\n",
        "        \"\"\" Motion Encoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding (x-y)\n",
        "        dyn_features_size: additional motion features to add to the embedding layer\n",
        "            0 -> x, y\n",
        "            1 -> x, y, velocity\n",
        "            2 -> x, y, velocity, acceleration\n",
        "            3 -> x, y, velocity, acceleration, yaw rate\n",
        "        slope: slope (positive or negative) of the activation function \n",
        "        \"\"\"\n",
        "        self.embedding = nn.Linear(2+dyn_features_size, pos_embedding_size)\n",
        "        self.activation = nn.LeakyReLU(slope)\n",
        "        self.encoder = nn.GRU(pos_embedding_size, traj_hidden_size)\n",
        "\n",
        "    def forward(self, hist_motion) -> torch.Tensor:\n",
        "        \"\"\" Motion Encoder inference \n",
        "\n",
        "        Input: hist_motion -> history motion tensor\n",
        "        Output: enc_motion -> encoded motion tensor\n",
        "        \"\"\"\n",
        "        emb_features = self.activation(self.embedding(hist_motion))\n",
        "        output, enc_motion = self.encoder(emb_features)\n",
        "        return enc_motion\n",
        "\n",
        "\n",
        "class PlanEncoder(nn.Module):\n",
        "    \"\"\" Plan Encoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, plan_hidden_size, \n",
        "                 pos_embedding_size, scene_embedding_size, agent_embedding_size, \n",
        "                 scene_features_size, agent_features_size, slope=ACTIVATION_SLOPE):\n",
        "        \"\"\" Plan Encoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding \n",
        "        scene_embedding_size: size of the linear layer for the scene embedding\n",
        "        agent_embedding_size: size of the linear layer for the agent embedding\n",
        "        scene_features_size: size of the scene features at each grid location\n",
        "        agent_features_size: size of the agent features at each grid location\n",
        "        slope: slope (positive or negative) of the activation function \n",
        "        \"\"\"\n",
        "        self.pos_embedding = nn.Linear(2, pos_embedding_size)\n",
        "        self.scene_embedding = nn.Linear(scene_features_size, scene_embedding_size)\n",
        "        self.agent_embedding = nn.Linear(agent_features_size, agent_embedding_size)\n",
        "        self.activation = nn.LeakyReLU(slope)\n",
        "        self.encoder = nn.GRU(\n",
        "            pos_embedding_size + scene_embedding_size + agent_embedding_size,\n",
        "            plan_hidden_size, bidirectional=True)\n",
        "    \n",
        "    def forward(self, plan, scene_feats, agent_feats) -> torch.Tensor:\n",
        "        \"\"\" Plan Encoder inference\n",
        "        Input:\n",
        "            - plan: tensor of waypoints\n",
        "            - scene_feats: scene features\n",
        "            - agent_feats: agent features\n",
        "        Output: enc_plan -> encoded plan tensot\n",
        "        \"\"\"\n",
        "        # Embedding\n",
        "        emb_features = self.activation(torch.cat((\n",
        "            self.pos_embedding(plan),\n",
        "            self.scene_embedding(scene_feats),\n",
        "            self.agent_embedding(agent_feats)),\n",
        "            dim=2\n",
        "        ))\n",
        "        # Reorganizing plans\n",
        "        plan_sum = torch.sum(torch.abs(plan), dim=2)\n",
        "        plan_lengths = torch.sum(plan_sum[1:, :]!=0, dim=0) + 1\n",
        "        plan_lengths_sorted, indices = torch.sort(plan_lengths, descending=True)\n",
        "        # Reorganizing embeddings\n",
        "        emb_packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            emb_features[:, indices, :], plan_lengths_sorted.cpu(), batch_first=False)\n",
        "        # Encoding\n",
        "        enc_plan_packed, output = self.encoder(emb_packed)\n",
        "        enc_plan_unpacked, _ = nn.utils.rnn.pad_packed_sequence(enc_plan_packed)\n",
        "        enc_plan = enc_plan_unpacked[:, indices.sort(), :]\n",
        "        return enc_plan\n",
        "\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\" Attention Decoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, att_hidden_size, traj_hidden_size, plan_hidden_size, \n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON):\n",
        "        \"\"\" Attention Decoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        att_hidden_size: size of the hidden layer of the attention part in the final decoder\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        self.traj_samples = traj_samples\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(2*plan_hidden_size + traj_hidden_size, att_hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(att_hidden_size, 1),\n",
        "            nn.Softmax(dim=0)\n",
        "        )\n",
        "        self.dec_state_op = nn.Linear(traj_hidden_size, 2)\n",
        "        self.decoder = nn.GRUCell(2*plan_hidden_size, traj_hidden_size)\n",
        "\n",
        "    def forward(self, hist_motion, enc_motion, enc_plan, device) -> torch.Tensor:\n",
        "        \"\"\" Attention Decoder inference\n",
        "        \n",
        "        Input: \n",
        "            - hist_motion: history motion tensor\n",
        "            - enc_motion: encoded motion tensor\n",
        "            - enc_plan: encoded plan tensor\n",
        "            - device: execution device (e.g. cuda:0)\n",
        "        Output: dec_traj -> decoded trajectory tensor\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        dec_traj = torch.empty(\n",
        "            (self.traj_samples, hist_motion.shape[1], 2), \n",
        "            dtype=torch.float32).to(device)\n",
        "        motion = enc_motion.squeeze()\n",
        "        # Attention loop\n",
        "        for s in range(self.traj_samples):\n",
        "            att_input = torch.cat((motion.repeat(enc_plan.shape[0], 1, 1), enc_plan), dim=2)\n",
        "            att_features = self.attention(att_input)\n",
        "            dec_input = (att_features.repeat(1, 1, enc_plan.shape[2])*enc_plan).sum(dim=0)\n",
        "            motion = self.decoder(dec_input, motion)\n",
        "            dec_traj[s] = self.dec_state_op(motion)\n",
        "        return dec_traj.permute(1, 0, 2)\n",
        "\n",
        "\n",
        "# TODO: handle the presence of no clusters with np.inf\n",
        "class TrajGenLoss(nn.Module):\n",
        "    \"\"\" Loss for the Trajectory Generator training \"\"\"\n",
        "    def __init__(self, phase='train', loss_fun='min_ade_k'):\n",
        "        \"\"\" Trajectory Generator loss initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase: 'pretrain' or 'train'\n",
        "        loss_fun: function to compute loss; available ['min_ade_k']\n",
        "        \"\"\"\n",
        "        if phase == 'pretrain':\n",
        "            self.loss_fun = nn.MSELoss()\n",
        "            if loss_fun is not None:\n",
        "                warnings.warn(\"In pretrain phase only MSELoss is available\")\n",
        "        else:\n",
        "            if loss_fun == 'min_ade_k':\n",
        "                self.loss_fun = self.min_ade_k\n",
        "            else:\n",
        "                # TODO: implement also different kinds of loss\n",
        "                pass\n",
        "\n",
        "    def min_ade_k(self, pred, gt):\n",
        "        \"\"\" Min_Ade_K loss for the Trajectory Generator \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        pred: prediction\n",
        "        gt: ground truth\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Trajectory Generator loss computation \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_expert_data(dataset, batch):\n",
        "    \"\"\" Extract Expert data from the passed dataset \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset: dataset from which to extract expert data\n",
        "    batch: batch of data at current time\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def extract_plan_features(plan, raster_feats, agent_feats):\n",
        "    \"\"\" Given a plan, extract location coordinates and map/agent features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    plan: plan from which to extract data (single or batch)\n",
        "    raster_feats: tensor of scene features\n",
        "    agent_feats: tensor of agent features\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(predictions: List[data_classes.Prediction], ground_truths: List[np.ndarray], \n",
        "                    helper, aggregators=AGGREGATORS) -> Dict[str, Any]:#Dict[str, Dict[str, List[float]]]:\n",
        "    \"\"\" Utility eval function to compute dataset metrics\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predictions: list of predictions made by the model (in Prediction class format)\n",
        "    ground_truths: the real trajectories of the agent (SHAPE -> [len(dataset), n_samples, state_dim])\n",
        "    helper: nuScenes dataset helper\n",
        "    aggregators: functions to aggregate metrics (e.g. mean)\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    metric_output: dictionary of the computed metrics:\n",
        "        - minADE_5: The average of pointwise L2 distances between the predicted trajectory \n",
        "                    and ground truth over the 5 most likely predictions.\n",
        "        - minADE_10: The average of pointwise L2 distances between the predicted trajectory \n",
        "                    and ground truth over the 10 most likely predictions.\n",
        "        - missRateTop_2_5: Proportion of misses relative to the 5 most likely trajectories\n",
        "                        over all agents\n",
        "        - missRateTop_2_10: Proportion of misses relative to the 10 most likely trajectories\n",
        "                        over all agents\n",
        "        - minFDE_1: The final displacement error (FDE) is the L2 distance \n",
        "                    between the final points of the prediction and ground truth, computed\n",
        "                    on the most likely trajectory\n",
        "        - offRoadRate: the fraction of trajectories that are not entirely contained\n",
        "                    in the drivable area of the map.\n",
        "    \"\"\"\n",
        "    # 1. Define metrics\n",
        "    print(\"\\t - Metrics definition ...\")\n",
        "    aggregators = \\\n",
        "        [metrics.deserialize_aggregator(agg) for agg in aggregators]\n",
        "    min_ade = metrics.MinADEK([5, 10], aggregators)\n",
        "    miss_rate = metrics.MissRateTopK([5, 10], aggregators)\n",
        "    min_fde = metrics.MinFDEK([1], aggregators)\n",
        "    if helper is not None:\n",
        "        # FIXME: instantiating offRoadRate class makes RAM explode\n",
        "        #offRoadRate = metrics.OffRoadRate(self.helper, self.aggregators)\n",
        "        pass\n",
        "    else:\n",
        "        offRoadRate = None\n",
        "\n",
        "    # 2. Compute metrics\n",
        "    metric_list = []\n",
        "    print(\"\\t - Effective metrics computation ...\")\n",
        "    for p, pred in enumerate(tqdm(predictions)):\n",
        "        # TODO: check for argument shapes\n",
        "        minADE_5 = min_ade(ground_truths[p], pred)[0][0]\n",
        "        minADE_10 = min_ade(ground_truths[p], pred)[0][1]\n",
        "        missRateTop_2_5 = miss_rate(ground_truths[p], pred)[0][0]\n",
        "        missRateTop_2_10 = miss_rate(ground_truths[p], pred)[0][1]\n",
        "        minFDE_1 = min_fde(ground_truths[p], pred)\n",
        "        #offRoadRate = offRoadRate(ground_truth[i], prediction)\n",
        "        metric = {'minADE_5': minADE_5, 'missRateTop_2_5': missRateTop_2_5,\n",
        "                  'minADE_10': minADE_10, 'missRateTop_2_10': missRateTop_2_10,\n",
        "                  'minFDE_1': minFDE_1}#, 'offRoadRate': offRoadRate}\n",
        "        metric_list.append(metric)\n",
        "\n",
        "    # 3. Aggregate\n",
        "    print(\"\\t - Metrics aggregation ...\")\n",
        "    aggregations: Dict[str, Dict[str, List[float]]] = defaultdict(dict)\n",
        "    metric_names = list(metric_list[0].keys())\n",
        "    metrics_dict = {name: np.array([metric_list[i][name] for i in range(len(metric_list))]) \n",
        "                    for name in metric_names}\n",
        "    for metric in metric_names:\n",
        "        for agg in aggregators:\n",
        "            aggregations[metric][agg.name] = agg(metrics_dict[metric])\n",
        "\n",
        "    return aggregations    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_train_data(train_iterations, val_iterations, epoches, train_losses, val_losses):\n",
        "    \"\"\" Plot a graph with the training trend\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_iterations: number of iterations for each epoch [train]\n",
        "    val_iterations: number of iterations for each epoch [val]\n",
        "    epoches: actual epoch number (starting from 1)\n",
        "    train_losses: array of loss values [train]\n",
        "    val_losses: array of loss values [val]\n",
        "    \"\"\"\n",
        "    # Data preparation\n",
        "    train_iterations_list = list(range(epoches*(train_iterations)))\n",
        "    val_iterations_list = list(range(epoches*(val_iterations)))\n",
        "    epoches_list = list(range(epoches))\n",
        "\n",
        "    # Adjust validation array dimension\n",
        "    val_error = len(val_losses) - len(val_iterations_list)\n",
        "    if val_error > 0:\n",
        "        val_losses = val_losses[:-val_error]\n",
        "\n",
        "    # Per-iteration plot\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-iteration Loss [train]')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Value')\n",
        "    l1, = plt.plot(train_iterations_list, train_losses, c='blue')\n",
        "    plt.legend(handles=[l1], labels=['Train loss'], loc='best')\n",
        "    plt.show()\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-iteration Loss [val]')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Value')\n",
        "    l2, = plt.plot(val_iterations_list, val_losses, c='red')\n",
        "    plt.legend(handles=[l2], labels=['Validation loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # Per-epoch plot\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-epoch Loss')\n",
        "    plt.xlabel('Epoches')\n",
        "    plt.ylabel('Value')\n",
        "    train_avg_losses = [np.array(train_losses[i:i+train_iterations]).mean() \n",
        "                        for i in range(0, len(train_losses), train_iterations)]\n",
        "    val_avg_losses = [np.array(val_losses[i:i+val_iterations]).mean() \n",
        "                      for i in range(0, len(val_losses), val_iterations)]\n",
        "    l1, = plt.plot(epoches_list, train_avg_losses, c='blue')\n",
        "    l2, = plt.plot(epoches_list, val_avg_losses, c='red')\n",
        "    plt.legend(handles=[l1, l2], labels=['Train loss', 'Validation loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "def plot_agent_future(raster, future, agent_pos=(0,0), reference_frame='local', color='green'):\n",
        "    \"\"\" Plot agent's future trajectory\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raster: raster map tensor (image)\n",
        "    future: future trajectory of the agent (predicted or GT) [x,y]\n",
        "    agent_pos: position of the agent (needed in case of local coords)\n",
        "    reference_frame: frame to which future coordinates refer\n",
        "    color: color of the plotted trajectory\n",
        "    \"\"\"\n",
        "    # Show raster map\n",
        "    plt.imshow(raster.permute(1, 2, 0))\n",
        "\n",
        "    # Show trajectory\n",
        "    x, y = [], []\n",
        "    for i in range(len(future)):\n",
        "        point = (agent_pos[0], agent_pos[1]) if i == 0 else future[i].numpy()\n",
        "        if reference_frame == 'local' and i > 0:\n",
        "            point = (point[0] + agent_pos[0], -point[1] + agent_pos[1])\n",
        "        x.append(point[0])\n",
        "        y.append(point[1])\n",
        "    \n",
        "    plt.plot(x, y, color=color, markersize=10, linewidth=5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIMN2u3AkE3"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DiJYZKAuhZ"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875,
          "referenced_widgets": [
            "fa8c974115444ba7a12c90c8a8d87516",
            "c30dc3da002f4e7c82a8ac5a68a5ffb5",
            "455963cc7508493e8ebfae33153fecd5",
            "5e92b547a76b4553b5db52dc0ed2cfb5",
            "4bac5714ea1e4ccb9e01324e21141988",
            "f39c30c9d8b14c1e94903cefe2a39729",
            "016a7c13e8c04e839b62f41277bbef0b",
            "b5b47bdfa1a54ecfa4bf83af9cc5a0a8",
            "917719585d5c4cde891fa9d24c1c452d",
            "8e30339b13f5456ca91124789f04c3c1",
            "eb628135b5d249d287fbfec0e012c810"
          ]
        },
        "id": "lsGEsxRVAuLI",
        "outputId": "eaa7d5e4-4c49-4640-8ae1-31b03b4d504a"
      },
      "outputs": [],
      "source": [
        "# ---------- Dataset initialization ---------- #\n",
        "# Initialize nuScenes helper\n",
        "\n",
        "print(\"nuScenes Helper initialization ...\")\n",
        "start_time = time.time()\n",
        "pl.seed_everything(PL_SEED)\n",
        "if ENVIRONMENT == 'local':\n",
        "    \n",
        "    if PREPARE_DATASET:\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)\n",
        "        with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'wb') as f:\n",
        "            pickle.dump(nusc, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    elif not 'nusc' in locals():\n",
        "        if HELPER_NEEDED:\n",
        "            with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'rb') as f:\n",
        "                nusc = pickle.load(f)\n",
        "elif ENVIRONMENT == 'colab':\n",
        "    if PREPARE_DATASET or HELPER_NEEDED:\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)\n",
        "helper = PredictHelper(nusc) if HELPER_NEEDED else None\n",
        "print(\"nuScenes Helper initialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# Initialize dataset and data module\n",
        "print(\"\\nDataset and Data Module initialization ...\")\n",
        "start_time = time.time()\n",
        "train_dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED, split='train')\n",
        "val_dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED, split='val')\n",
        "trainval_dm = nuScenesDataModule(train_dataset, val_dataset, num_workers=NUM_WORKERS)\n",
        "trainval_dm.setup(stage='fit')\n",
        "print(\"Dataset and Data Module initialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# ---------- Network initialization ---------- #\n",
        "start_time = time.time()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if PREDICTION_MODEL == 'CoverNet':\n",
        "    print(\"\\nCoverNet model initialization ...\")\n",
        "    model = CoverNet(K_SIZE, EPSILON, TRAJ_LINK, TRAJ_DIR, device)\n",
        "    print(\"CoverNet model intialization done in %f s\\n\" % (time.time() - start_time))\n",
        "elif PREDICTION_MODEL == 'P2T':\n",
        "    print(\"\\nP2T model initialization ...\")\n",
        "    model = P2T()\n",
        "    print(\"P2T model intialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# ---------- Training initialization ---------- #\n",
        "print(\"\\nTrainer initialization ...\")\n",
        "start_time = time.time()\n",
        "GPUS = min(1, torch.cuda.device_count())\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=CHECKPOINT_DIR,\n",
        "                                      save_top_k=TOP_K_SAVE,\n",
        "                                      monitor=CHECKPOINT_MONITOR)\n",
        "trainer = pl.Trainer(callbacks=[checkpoint_callback],\n",
        "                     progress_bar_refresh_rate=PROGRESS_BAR_REFRESH_RATE, \n",
        "                     gpus=GPUS, max_epochs=TRAIN_EPOCHES)\n",
        "print(\"Trainer intialization done in %f s\\n\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y1_nMkNAw9A"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "93e2a2bf0452441ebab39d237c4181eb",
            "f013776f9b704391aff767c307a62cd8",
            "2fea7a80fe1241519ba0dd6a221f8703",
            "6c12ccb59ad945d3818e9f0087161e83",
            "140606ee7e89497fa83119ff988c223f",
            "9abf84629b8144a7a13eb0c5e6559eb6",
            "396e2e66ddce4c5aa88898c1b4bf29b0",
            "fb33fcf4191e4a60b78ffd71f067f29a",
            "6eceb15e9c8c43a2a7da57049071f180",
            "4b524f2d58934f3f811240dd2d8ea621",
            "c8fecad947374e859766802f0fdac494"
          ]
        },
        "id": "caBfMgTGAw1Y",
        "outputId": "7a84e58a-7233-4344-9410-771a2fd00791"
      },
      "outputs": [],
      "source": [
        "if PREDICTION_MODEL == 'CoverNet':\n",
        "    trainer.fit(model, trainval_dm)\n",
        "elif PREDICTION_MODEL == 'P2T':\n",
        "    # NOTE: we can handle training of all components in a single function\n",
        "    #P2T.train()\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataloader initialization\n",
        "print(\"Loading test dataloader ...\")\n",
        "trainval_dm.setup(stage='test')\n",
        "test_dataloader = trainval_dm.test_dataloader()\n",
        "test_generator = iter(test_dataloader)\n",
        "\n",
        "# Trained model initialization\n",
        "# TODO: istantiate kwargs for network in a better way\n",
        "print(\"\\nCoverNet trained model initialization ...\")\n",
        "checkpoint_name = 'epoch=19-step=80460.ckpt'\n",
        "net_args = {'K_size': K_SIZE, 'epsilon': EPSILON, 'traj_link': TRAJ_LINK, 'traj_dir': TRAJ_DIR, 'device': device}\n",
        "model = CoverNet.load_from_checkpoint(checkpoint_path=os.path.join(BEST_CHECKPOINT_DIR, checkpoint_name), \n",
        "                                      map_location=None, hparams_file=None, strict=True, \n",
        "                                      K_size=K_SIZE, epsilon=EPSILON, traj_link=TRAJ_LINK, traj_dir=TRAJ_DIR, device=device).to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------- CoverNet Metrics computation ---------- #\n",
        "# TODO: generalize metrics computation\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "start = time.time()\n",
        "reduced_traj = model.trajectories[:, :model.traj_samples].numpy()\n",
        "print(\"\\nCoverNet metrics computation ...\")\n",
        "print(\"1 - Producing predictions ...\")\n",
        "for i, token in enumerate(tqdm(val_dataset.tokens)):\n",
        "    with torch.no_grad():\n",
        "        x_state, _, x_img, gt, _ = val_dataset[i]\n",
        "        x_state = x_state.to(device)\n",
        "        x_img = x_img.to(device)\n",
        "        x_state = torch.unsqueeze(torch.flatten(x_state, 0, 1), 0)\n",
        "        x_img = torch.unsqueeze(x_img, 0)\n",
        "        pred_logits = model((x_img, x_state))\n",
        "        pred_probs = F.softmax(pred_logits, dim=1)[0]\n",
        "        top_indices = pred_probs.argsort()[-MAX_PREDICTED_MODES:]\n",
        "        cutted_probs = pred_probs[top_indices].cpu().numpy()\n",
        "        cutted_traj = reduced_traj[top_indices.cpu()]\n",
        "    i_t, s_t = token.split(\"_\")\n",
        "    ground_truths.append(gt.numpy())\n",
        "    predictions.append(data_classes.Prediction(i_t, s_t, cutted_traj, cutted_probs))\n",
        "print(\"2 - Computing metrics ...\")\n",
        "convernet_metrics = compute_metrics(predictions, ground_truths, helper)\n",
        "print(\"Metric computation done in %f s\" % (time.time() - start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'minADE_5': {'RowMean': 2.8969106674194336},\n",
              "             'missRateTop_2_5': {'RowMean': 0.7489215794712974},\n",
              "             'minADE_10': {'RowMean': 2.187161684036255},\n",
              "             'missRateTop_2_10': {'RowMean': 0.6338900564096892},\n",
              "             'minFDE_1': {'RowMean': [[11.689470291137695]]}})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Obtained with trajectory horizon = 6 seconds\n",
        "convernet_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(dict,\n",
              "            {'minADE_5': {'RowMean': 1.126030445098877},\n",
              "             'missRateTop_2_5': {'RowMean': 0.35128857427275745},\n",
              "             'minADE_10': {'RowMean': 0.8562875986099243},\n",
              "             'missRateTop_2_10': {'RowMean': 0.23891162482026324},\n",
              "             'minFDE_1': {'RowMean': [[4.1349053382873535]]}})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Obtained with trajectory horizon = 3 seconds\n",
        "convernet_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code Debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training loop** (manual - debug only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if DEBUG_MODE:\n",
        "\n",
        "    # Dataset preparation\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)\n",
        "\n",
        "    # Training preparation\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Plotting preparation\n",
        "    train_loss_arr = []\n",
        "    val_loss_arr = []\n",
        "    train_iterations = len(train_dataset) // BATCH_SIZE\n",
        "    val_iterations = len(val_dataset) // BATCH_SIZE\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(TRAIN_EPOCHES):\n",
        "        print(\"-------- Epoch %d --------\" % i)\n",
        "        model.train()\n",
        "\n",
        "        # Training\n",
        "        for j, data in enumerate(train_dataloader):\n",
        "            \n",
        "            # Data preparation\n",
        "            x_state, x_img_static, x_img_dynamic, gt, idx = data\n",
        "            x_state = x_state.to(device)\n",
        "            x_img_static = x_img_static.to(device)\n",
        "            x_img_dynamic = x_img_dynamic.to(device)\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            with torch.no_grad():\n",
        "                reduced_traj = model.trajectories[:, :SAMPLES_PER_SECOND*TRAJ_HORIZON]\n",
        "                y = get_positives(reduced_traj, gt)\n",
        "\n",
        "            # Inference\n",
        "            optimizer.zero_grad()\n",
        "            traj_logits = model((x_img_dynamic, x_state))\n",
        "            y = y.to(device)\n",
        "            loss = F.cross_entropy(traj_logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            loss_val = loss.item()\n",
        "            train_loss_arr.append(loss_val)\n",
        "            print(\"[%d] %d - train loss = %f\" % (i, j, loss_val))\n",
        "\n",
        "        # Validation\n",
        "        model.train(mode=False)\n",
        "        for j, data in enumerate(val_dataloader):\n",
        "\n",
        "            # Data preparation\n",
        "            x_state, x_img_static, x_img_dynamic, gt, idx = data\n",
        "            x_state = x_state.to(device)\n",
        "            x_img_static = x_img_static.to(device)\n",
        "            x_img_dynamic = x_img_dynamic.to(device)\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            reduced_traj = model.trajectories[:, :SAMPLES_PER_SECOND*TRAJ_HORIZON]\n",
        "            y = get_positives(reduced_traj, gt)\n",
        "\n",
        "            # Inference\n",
        "            traj_logits = model((x_img_dynamic, x_state))\n",
        "            y = y.to(device)\n",
        "            loss = F.cross_entropy(traj_logits, y)\n",
        "\n",
        "            # Logging\n",
        "            loss_val = loss.item()\n",
        "            val_loss_arr.append(loss_val)\n",
        "            print(\"[%d] %d - val loss = %f\" % (i, j, loss_val))\n",
        "\n",
        "        # Plotting\n",
        "        if (i+1) % PLOT_PERIOD == 0:\n",
        "            plot_train_data(train_iterations, val_iterations, i+1, train_loss_arr, val_loss_arr)\n",
        "            a = input(\"Press Enter to continue...\")\n",
        "            plt.close('all')\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7y8IMO_-479"
      },
      "source": [
        "**Dataset debugging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "S5uPp-rLC7qd"
      },
      "outputs": [],
      "source": [
        "# Initialize nuScenes\n",
        "HELPER_NEEDED = True\n",
        "if ENVIRONMENT == 'local':\n",
        "    if PREPARE_DATASET:\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)\n",
        "        with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'wb') as f:\n",
        "            pickle.dump(nusc, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    elif not 'nusc' in locals():\n",
        "        if HELPER_NEEDED:\n",
        "            with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'rb') as f:\n",
        "                nusc = pickle.load(f)\n",
        "elif ENVIRONMENT == 'colab':\n",
        "    if PREPARE_DATASET or HELPER_NEEDED:\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lTJuTk-A8jLI"
      },
      "outputs": [],
      "source": [
        "helper = PredictHelper(nusc)\n",
        "dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, BATCH_SIZE, True, num_workers=NUM_WORKERS)\n",
        "train_generator = iter(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MORS9Jw6MCE"
      },
      "outputs": [],
      "source": [
        "# Useful to check ideal number of workers and batch size\n",
        "x = time.time()\n",
        "try:\n",
        "    state, img_static, img_dynamic, gt, idxs = next(train_generator)\n",
        "except StopIteration:\n",
        "    train_generator = iter(train_dataloader)\n",
        "    state, img_static, img_dynamic, gt, idxs = next(train_generator)\n",
        "print(time.time() - x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR8Wpz-n8jLJ"
      },
      "outputs": [],
      "source": [
        "state, img_static, img_dynamic, gt, idx = dataset[np.random.randint(len(dataset))]\n",
        "plt.imshow(img_dynamic.permute(1, 2, 0))\n",
        "plt.show()\n",
        "print(\"State input size:\", state.shape)\n",
        "print(\"Ground truth size:\", gt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "instance_token, sample_token = dataset.tokens[idx].split(\"_\")\n",
        "long_gt = torch.Tensor(\n",
        "            dataset.helper.get_future_for_agent(instance_token, sample_token,\n",
        "                                                seconds=100, in_agent_frame=True))\n",
        "# TODO: check how to get agent position in the map                                        \n",
        "plot_agent_future(img_dynamic, long_gt, agent_pos=(250,400), reference_frame='local')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQWIhylL-_Po"
      },
      "source": [
        "**Network debugging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTWbk_F38jLS"
      },
      "outputs": [],
      "source": [
        "test_states, test_imgs_static, test_imgs_dynamic, test_gts, _ = next(train_generator)\n",
        "test_states = torch.flatten(test_states, 0, 1)\n",
        "\n",
        "print(test_imgs_dynamic.size())\n",
        "print(test_states.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ePJM5C7J8jLT"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "model = CoverNet(K_SIZE, EPSILON, TRAJ_LINK, TRAJ_DIR, device='cuda:0')\n",
        "traj_logits = model((test_imgs_dynamic, test_states))\n",
        "\n",
        "# Output 5 and 10 most likely trajectories for this batch\n",
        "top_5_trajectories = model.trajectories[traj_logits.argsort(descending=True)[:5]]\n",
        "top_10_trajectories = model.trajectories[traj_logits.argsort(descending=True)[:10]]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Trajectory_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "016a7c13e8c04e839b62f41277bbef0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "140606ee7e89497fa83119ff988c223f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2fea7a80fe1241519ba0dd6a221f8703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb33fcf4191e4a60b78ffd71f067f29a",
            "max": 4024,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eceb15e9c8c43a2a7da57049071f180",
            "value": 80
          }
        },
        "396e2e66ddce4c5aa88898c1b4bf29b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "455963cc7508493e8ebfae33153fecd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b47bdfa1a54ecfa4bf83af9cc5a0a8",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_917719585d5c4cde891fa9d24c1c452d",
            "value": 102530333
          }
        },
        "4b524f2d58934f3f811240dd2d8ea621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bac5714ea1e4ccb9e01324e21141988": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e92b547a76b4553b5db52dc0ed2cfb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e30339b13f5456ca91124789f04c3c1",
            "placeholder": "â",
            "style": "IPY_MODEL_eb628135b5d249d287fbfec0e012c810",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 126MB/s]"
          }
        },
        "6c12ccb59ad945d3818e9f0087161e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b524f2d58934f3f811240dd2d8ea621",
            "placeholder": "â",
            "style": "IPY_MODEL_c8fecad947374e859766802f0fdac494",
            "value": " 80/4024 [06:06&lt;5:01:13,  4.58s/it, loss=189, v_num=0]"
          }
        },
        "6eceb15e9c8c43a2a7da57049071f180": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e30339b13f5456ca91124789f04c3c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917719585d5c4cde891fa9d24c1c452d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93e2a2bf0452441ebab39d237c4181eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f013776f9b704391aff767c307a62cd8",
              "IPY_MODEL_2fea7a80fe1241519ba0dd6a221f8703",
              "IPY_MODEL_6c12ccb59ad945d3818e9f0087161e83"
            ],
            "layout": "IPY_MODEL_140606ee7e89497fa83119ff988c223f"
          }
        },
        "9abf84629b8144a7a13eb0c5e6559eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b47bdfa1a54ecfa4bf83af9cc5a0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30dc3da002f4e7c82a8ac5a68a5ffb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f39c30c9d8b14c1e94903cefe2a39729",
            "placeholder": "â",
            "style": "IPY_MODEL_016a7c13e8c04e839b62f41277bbef0b",
            "value": "100%"
          }
        },
        "c8fecad947374e859766802f0fdac494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb628135b5d249d287fbfec0e012c810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f013776f9b704391aff767c307a62cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9abf84629b8144a7a13eb0c5e6559eb6",
            "placeholder": "â",
            "style": "IPY_MODEL_396e2e66ddce4c5aa88898c1b4bf29b0",
            "value": "Epoch 0:   2%"
          }
        },
        "f39c30c9d8b14c1e94903cefe2a39729": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa8c974115444ba7a12c90c8a8d87516": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c30dc3da002f4e7c82a8ac5a68a5ffb5",
              "IPY_MODEL_455963cc7508493e8ebfae33153fecd5",
              "IPY_MODEL_5e92b547a76b4553b5db52dc0ed2cfb5"
            ],
            "layout": "IPY_MODEL_4bac5714ea1e4ccb9e01324e21141988"
          }
        },
        "fb33fcf4191e4a60b78ffd71f067f29a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
