{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYwxiuChyg8u"
      },
      "source": [
        "## Deep Learining project\n",
        "\n",
        "\n",
        "*   Gianfranco Di Marco - 1962292\n",
        "*   Giacomo Colizzi Coin - 1794538\n",
        "\n",
        "\n",
        "\\\n",
        "**- Trajectory Prediction -**\n",
        "\n",
        "Is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.\n",
        "\n",
        "**- nuScenes Dataset -**\n",
        "\n",
        "Available at https://www.nuscenes.org/nuscenes. The nuScenes\n",
        "dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360Â° coverage\n",
        "\n",
        "\n",
        "> Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and Giancarlo Baldan and Oscar Beijbom: \"*nuScenes: A multimodal dataset for autonomous driving*\", arXiv preprint arXiv:1903.11027, 2019.\n",
        "\n",
        "The most important part of this dataset for our project is the Map Expansion Pack, which simplify the trajectory prediction problem\n",
        "\n",
        "**- Baseline 1: CoverNet -**\n",
        "\n",
        "One of the first models for the Trajectory Prediction problem with nuScenes dataset. Simply a CNN that extracts features from raster map images\n",
        "\n",
        "> Tung Phan-Minh, Elena Corina Grigore, Freddy A. Boulton, Oscar Beijbom, Eric M. Wolff: \"*CoverNet: Multimodal Behavior Prediction using Trajectory Sets*\", arXiv:1911.10298\n",
        ", 2019.\n",
        "\n",
        "\n",
        "**- Baseline 2: P2T -**\n",
        "\n",
        "One of the most recent SOTA (actually 10-th position in the nuScenes benchmark) approaches for the Trajectory Prediction problem with nuScenes dataset. A combination of Inverse Reinforcement Learning on a grid representation of the maps, with a trajectory generator which encodes motion and grid-based plans, then decode the trajectories (the name P2T stands for 'Plans 2 Trajectories')\n",
        "\n",
        "> Nachiket Deo, Mohan M. Trivedi: \"*Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based Plans*\", arXiv:2001.00735, 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3uPCs50bDU"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VbP-XZY0xvv"
      },
      "source": [
        "**Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ-Kwgkp0xog",
        "outputId": "2df0be23-0e60-43bd-93ed-f959f19e2ff9"
      },
      "outputs": [],
      "source": [
        "\"\"\" Necessary since Google Colab supports only Python 3.7\n",
        "Furthermore, \n",
        "   -> some libraries can be different between local and Colab\n",
        "   -> resource usage is different between local and Colab\n",
        "\"\"\"\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    ENVIRONMENT = 'colab'\n",
        "    %pip install tf-estimator-nightly==2.8.0.dev2021122109\n",
        "    %pip install folium==0.2.1\n",
        "except:\n",
        "    ENVIRONMENT = 'local'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_RVZEdYvzu"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4wvD942xC89",
        "outputId": "00192890-a398-4d01-fa0f-566e7dcae620"
      },
      "outputs": [],
      "source": [
        "%pip install nuscenes-devkit\n",
        "%pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8tTIrgJRPOjo"
      },
      "outputs": [],
      "source": [
        "# Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torchvision.models import resnet50, resnet34\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchmetrics import functional\n",
        "from sklearn.cluster import KMeans\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# Math\n",
        "from pyquaternion import Quaternion\n",
        "import numpy as np\n",
        "import scipy\n",
        "import math\n",
        "\n",
        "# Visual\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset\n",
        "from nuscenes.nuscenes import NuScenes\n",
        "from nuscenes.prediction import PredictHelper\n",
        "from nuscenes.prediction.helper import quaternion_yaw, angle_of_rotation\n",
        "from nuscenes.prediction.input_representation.utils import convert_to_pixel_coords, get_crops, get_rotation_matrix\n",
        "from nuscenes.prediction.input_representation.interface import InputRepresentation, AgentRepresentation\n",
        "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
        "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
        "from nuscenes.prediction.input_representation.combinators import Rasterizer\n",
        "from nuscenes.eval.prediction.config import PredictionConfig, load_prediction_config\n",
        "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
        "from nuscenes.eval.prediction import metrics, data_classes\n",
        "\n",
        "# File system\n",
        "import os\n",
        "import copy\n",
        "import gdown\n",
        "import shutil\n",
        "import pickle\n",
        "import zipfile\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# Generic\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from collections import defaultdict, namedtuple\n",
        "from abc import abstractmethod\n",
        "import multiprocessing as mp\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hftZeWZYE3"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5fNsDEfaMN3"
      },
      "source": [
        "**Generic Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HfRgW1VNZX-D"
      },
      "outputs": [],
      "source": [
        "# Environment-dependent parameters\n",
        "if ENVIRONMENT == 'colab':\n",
        "    ROOT = '/content'\n",
        "    MAX_NUM_WORKERS = 0\n",
        "    PROGRESS_BAR_REFRESH_RATE = 20\n",
        "    MINI_DATASET = True\n",
        "    if not MINI_DATASET:\n",
        "        ROOT = '/content/drive/MyDrive/DL/Trajectory-Prediction-PyTorch/'\n",
        "elif ENVIRONMENT == 'local':\n",
        "    ROOT = os.getcwd()\n",
        "    # BUG: VRAM problem with PL\n",
        "    if os.name == 'nt':\n",
        "        MAX_NUM_WORKERS = 0\n",
        "    else:\n",
        "        MAX_NUM_WORKERS = 8\n",
        "    PROGRESS_BAR_REFRESH_RATE = 10\n",
        "    MINI_DATASET = False\n",
        "else:\n",
        "    raise ValueError(\"Wrong 'environment' value\")\n",
        "\n",
        "# Generic Train parameters\n",
        "NUM_WORKERS = MAX_NUM_WORKERS\n",
        "PLOT_PERIOD = 1         # 1 = plot at each epoch\n",
        "CLEAR_PERIOD = 1        # 1 = clear output at each epoch\n",
        "PRINT_PERIOD = 1        # 1 = print at each batch iter\n",
        "CHECKPOINT_PERIOD = 1   # 1 = save model at each epoch\n",
        "CHECKPOINT_METHOD = 'best'\n",
        "CHECKPOINT_MONITOR = \"val_loss\"\n",
        "CHECKPOINT_DIR = os.path.join(ROOT, 'checkpoints')\n",
        "CHECKPOINTS_LINK = 'https://drive.google.com/u/0/uc?id=120Nwg8iqpi7NlOSqk0aUc_y-VvKzDCKc&export=download'\n",
        "BEST_FOLDER = 'best'\n",
        "TOP_K_SAVE = 10\n",
        "\n",
        "# Generic Test parameters\n",
        "DEBUG_MODE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rFOOPSWgiOG"
      },
      "source": [
        "**Network Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iS5EccU9giHb"
      },
      "outputs": [],
      "source": [
        "# _________ CoverNet Parameters _________ #\n",
        "# - Architecture parameters\n",
        "BACKBONE_WEIGHTS = 'ImageNet'\n",
        "BACKBONE_MODEL = 'ResNet18'\n",
        "K_SIZE = 20000\n",
        "# - Trajectory parameters\n",
        "TRAJ_LINK = 'https://www.nuscenes.org/public/nuscenes-prediction-challenge-trajectory-sets.zip'\n",
        "TRAJ_DIR = os.path.join(ROOT, 'trajectory_sets')\n",
        "EPSILON = 2\n",
        "# Train Parameters\n",
        "COVERNET_LR = 1e-4\n",
        "COVERNET_MOMENTUM = 0.9\n",
        "TRAIN_COVERNET_EPOCHES = 25\n",
        "TRAIN_EPOCHES = 25\n",
        "\n",
        "# _________ P2T Parameters _________ #\n",
        "# - RL parameters\n",
        "INITIAL_STATE = [19, 12]\n",
        "POLICY_SAMPLES = 200\n",
        "# - Reward Model parameters\n",
        "TRAIN_RM_EPOCHES = 25\n",
        "REWARD_MODEL_LR = 0.0001\n",
        "RM_LOGNAME = 'reward_model'\n",
        "# - Trajectory Generator parameters\n",
        "PRETRAIN_TG_EPOCHES = 80\n",
        "TRAIN_TG_EPOCHES = 400\n",
        "TRAJ_HIDDEN_SIZE = 32\n",
        "PLAN_HIDDEN_SIZE = 32\n",
        "ATT_HIDDEN_SIZE = 32\n",
        "POS_EMBEDDING_SIZE = 16\n",
        "SCENE_EMBEDDING_SIZE = 32\n",
        "AGENT_EMBEDDING_SIZE = 16\n",
        "SCENE_FEATURES_SIZE = 32\n",
        "AGENT_FEATURES_SIZE = 4\n",
        "DYN_FEATURES_SIZE = 3\n",
        "ACTIVATION_SLOPE = 0.1\n",
        "TRAJ_GEN_LR_PRE = 0.001\n",
        "TRAJ_GEN_LR = 0.0001\n",
        "TRAJ_CLUSTERS = 10\n",
        "MAX_CLIP_NORM = 10\n",
        "PRE_TG_LOGNAME = 'traj_generator_pretrain'\n",
        "FT_TG_LOGNAME = 'traj_generator_finetune'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXoj0mI-aO--"
      },
      "source": [
        "**Dataset Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "btLY02YCaO4Y"
      },
      "outputs": [],
      "source": [
        "# Organization parameters\n",
        "HELPER_LOADING = False\n",
        "PREPARE_DATASET = True\n",
        "PREPROCESSED = True\n",
        "\n",
        "# File system parameters\n",
        "PL_SEED = 42\n",
        "DATAROOT = os.path.join(ROOT, 'data', 'sets', 'nuscenes')\n",
        "PREPROCESSED_FOLDER = 'preprocessed'\n",
        "PREPROCESSED_ADV_FOLDER = 'preprocessed_advanced'\n",
        "GT_SUFFIX = '-gt'\n",
        "ADD_SUFFIX = '-add'\n",
        "FILENAME_EXT = '.pt'\n",
        "ADDITIONAL_EXT = '.npy'\n",
        "AGGREGATORS = [{'name': \"RowMean\"}]\n",
        "DATASET_VERSION = 'v1.0-mini' if MINI_DATASET else 'v1.0-trainval'\n",
        "\n",
        "# NuScenes parameters\n",
        "SHORT_AGENT_HISTORY = 1\n",
        "LONG_AGENT_HISTORY = 2\n",
        "SHORT_TERM_HORIZON = 3\n",
        "LONG_TERM_HORIZON = 6\n",
        "TRAJ_HORIZON = SHORT_TERM_HORIZON\n",
        "AGENT_HISTORY = LONG_AGENT_HISTORY\n",
        "HORIZON_FOLDER = str(TRAJ_HORIZON) + '_seconds'\n",
        "\n",
        "# Advanced parameters\n",
        "GRID_SIDE = 25\n",
        "GRID_EXTENT = [-25, 25, -10, 40]\n",
        "RASTER_SIZE = 200\n",
        "IMG_RESOLUTION = 0.1\n",
        "MDP_HORIZON = 40\n",
        "\n",
        "# Train parameters\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Other parameters\n",
        "MAX_PREDICTED_MODES = 25\n",
        "SAMPLES_PER_SECOND = 2\n",
        "NORMALIZATION = 'imagenet'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fast setting (Google Colab only)**"
      ],
      "metadata": {
        "id": "hYrqPhoyGQ8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful parameters to set before a Google Colab testing session (default values have been already assigned before):\n",
        "\n",
        ">*   ```NUM_WORKERS```: the number of processes that collect data\n",
        ">*   ```BATCH_SIZE```: number of dataset elements in a batch\n",
        ">*   ```PLOT_PERIOD```: period of iterations to plot train data (debug only\n",
        ">*   ```PRINT_PERIOD```: period of iterations to print training trend\n",
        ">*   ```CHECKPOINT_PERIOD```: period of iterations to save model checkpoints\n",
        ">*   ```HELPER_NEEDED```: if you want to use the nuScenes helper to load the data (RAM consuming). It is necessary to preprocess data, so by default it's True; anyway, after data preprocessing this should be set to False and the runtime should be restarted if you want to use the new data instead of the Helper\n",
        ">*   ```PREPARE_DATASET```: if to prepare data on the disk. By default it is True, so the dataset will be prepared (if not already done) on the filesystem. Should be toggled to False after the first data preparation\n",
        ">*   ```PREPROCESSED```: if the data has already been preprocessed. If False, the initialized datasets will be preprocessed, so make sure to toggle this flag to True if you have already preprocessed the data or you are using a plug-and-play dataset (for example by mounting on Drive)\n",
        ">*   ```TRAJ_HORIZON```: the number of seconds of horizon to predict the trajectory of an agent\n",
        ">*   ```AGENT_HISTORY```: the number of seconds of past of an agent to take into account for trajectory generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W8V5vmEWQo7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ENVIRONMENT == 'colab':\n",
        "\n",
        "    #@markdown - Training Parameters\n",
        "    NUM_WORKERS = 0             #@param {type:\"slider\", min:0, max:8, step:1}\n",
        "    BATCH_SIZE = 16             #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "    PLOT_PERIOD = 1             #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    PRINT_PERIOD = 1            #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    CHECKPOINT_PERIOD = 1       #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    CHECKPOINT_METHOD = 'best'  #@param [\"best\", \"last\"] {type:\"string\"}\n",
        "\n",
        "    #@markdown - Dataset Parameters\n",
        "    HELPER_NEEDED = False       #@param {type: \"boolean\"}\n",
        "    PREPARE_DATASET = False     #@param {type: \"boolean\"}\n",
        "    PREPROCESSED = True         #@param {type: \"boolean\"}\n",
        "    TRAJ_HORIZON = 6 #@param [3, 6]\n",
        "    AGENT_HISTORY = 2 #@param [1, 2]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7RzpT-SIGPvW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYrqPhoyGQ8S"
      },
      "source": [
        "**Fast setting (Google Colab only)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8V5vmEWQo7H"
      },
      "source": [
        "Useful parameters to set before a Google Colab testing session (default values have been already assigned before):\n",
        "\n",
        ">*   ```NUM_WORKERS```: the number of processes that collect data\n",
        ">*   ```BATCH_SIZE```: number of dataset elements in a batch\n",
        ">*   ```PLOT_PERIOD```: period of iterations to plot train data (debug only\n",
        ">*   ```PRINT_PERIOD```: period of iterations to print training trend\n",
        ">*   ```CHECKPOINT_PERIOD```: period of iterations to save model checkpoints\n",
        ">*   ```HELPER_LOADING```: if you want to use the nuScenes helper to load the data (RAM consuming).\n",
        ">*   ```PREPARE_DATASET```: if to prepare data on the disk. By default it is True, so the dataset will be prepared (if not already done) on the filesystem. Should be toggled to False after the first data preparation\n",
        ">*   ```PREPROCESSED```: if the data has already been preprocessed. If False, the initialized datasets will be preprocessed, so make sure to toggle this flag to True if you have already preprocessed the data or you are using a plug-and-play dataset (for example by mounting on Drive)\n",
        ">*   ```TRAJ_HORIZON```: the number of seconds of horizon to predict the trajectory of an agent\n",
        ">*   ```AGENT_HISTORY```: the number of seconds of past of an agent to take into account for trajectory generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7RzpT-SIGPvW"
      },
      "outputs": [],
      "source": [
        "if ENVIRONMENT == 'colab':\n",
        "\n",
        "    #@markdown - Training Parameters\n",
        "    NUM_WORKERS = 0             #@param {type:\"slider\", min:0, max:8, step:1}\n",
        "    BATCH_SIZE = 16             #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "    PLOT_PERIOD = 1             #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    PRINT_PERIOD = 1            #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    CHECKPOINT_PERIOD = 1       #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "    CHECKPOINT_METHOD = 'best'  #@param [\"best\", \"last\"] {type:\"string\"}\n",
        "\n",
        "    #@markdown - Dataset Parameters\n",
        "    HELPER_LOADING = False       #@param {type: \"boolean\"}\n",
        "    PREPARE_DATASET = True     #@param {type: \"boolean\"}\n",
        "    PREPROCESSED = False         #@param {type: \"boolean\"}\n",
        "    TRAJ_HORIZON = 6 #@param [3, 6]\n",
        "    AGENT_HISTORY = 2 #@param [1, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Vxmwu00dEd"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfvjWlfxa6Gy"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixi3s7lryeiO"
      },
      "source": [
        "First dataset option [recommended]: The following code prepares the dataset files on the disk. With the *MINI_DATASET* option specified, it is possible to download a tiny split of the entire dataset in order to allow testing also on limiting runtimes (like the Google Colab one); this is the default option when running in Google Colab, because there isn't enough space to extract all the dataset.\n",
        "\n",
        "N.B: The download links in function *urllib.request.urlretrieve()* should be replaced periodically because they expire.\n",
        "\n",
        "Steps to download correctly (**Firefox**):\n",
        "\n",
        "1.   Dowload Map Expansion pack (or Trainval metadata) from the website\n",
        "2.   Stop the download\n",
        "3.   Right-click on the file -> copy download link\n",
        "4.   Paste the copied link into the first argument of the urlretrieve function. The second argument is the final name of the file\n",
        "\n",
        "Steps to download correctly (**Chrome**):\n",
        "\n",
        "1.   Dowload Map Expansion pack (or Trainval metadata) from the website\n",
        "2.   Click on \"Show All\" or go to the download section\n",
        "3.   Right-click on the file link -> copy download link\n",
        "4.   Paste the copied link into the first argument of the urlretrieve function. The second argument is the final name of the file\n",
        "\n",
        "WARNING: sometimes the 'maps' folder of the Map Expansion pack is overwritten by the 'maps' folder extracted from the Metadata; this results into an error when loading the dataset helper in Main function. If this happens, try to restart runtime and retry the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ6oVSHz0iCf",
        "outputId": "501aae52-b74e-4147-a29d-b8f86aa10cc0"
      },
      "outputs": [],
      "source": [
        "if PREPARE_DATASET:\n",
        "\n",
        "    # Creating dataset dir\n",
        "    start = time.time()\n",
        "    os.makedirs(DATAROOT, exist_ok=True)\n",
        "    os.chdir(DATAROOT)\n",
        "\n",
        "    # Downloading Map Expansion Pack\n",
        "    if 'maps' not in os.listdir():\n",
        "        os.mkdir('maps')\n",
        "        os.chdir('maps')\n",
        "        print(\"Downloading and extracting Map Expansion pack ...\")\n",
        "        urllib.request.urlretrieve('https://s3.amazonaws.com/data.nuscenes.org/public/v1.0/nuScenes-map-expansion-v1.3.zip?AWSAccessKeyId=AKIA6RIK4RRMFUKM7AM2&Signature=%2FCChAbQfdatWAmwsmZPdTRFawK0%3D&Expires=1664710314', 'nuScenes-map-expansion-v1.3.zip')\n",
        "        with zipfile.ZipFile('nuScenes-map-expansion-v1.3.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall(os.getcwd())\n",
        "        os.remove('nuScenes-map-expansion-v1.3.zip')\n",
        "        os.chdir('..')\n",
        "    \n",
        "    # If specified, use the mini dataset split\n",
        "    if MINI_DATASET:\n",
        "        # Downloading Mini-Trainval Metadata\n",
        "        if 'v1.0-mini' not in os.listdir():\n",
        "            print(\"Downloading and extracting Mini dataset ...\")\n",
        "            urllib.request.urlretrieve('https://s3.amazonaws.com/data.nuscenes.org/public/v1.0/v1.0-mini.tgz?AWSAccessKeyId=AKIA6RIK4RRMFUKM7AM2&Signature=Vgo81B4ygq4z2zaUru7xe2mDAh0%3D&Expires=1664710368', 'v1.0-mini.tgz')\n",
        "            with tarfile.open('v1.0-mini.tgz', 'r:gz') as tar_ref:\n",
        "                tar_ref.extractall(os.getcwd())\n",
        "            os.remove('v1.0-mini.tgz')\n",
        "            shutil.rmtree('samples')\n",
        "            shutil.rmtree('sweeps')\n",
        "            os.chdir(DATAROOT)\n",
        "    else:\n",
        "        # Downloading Trainval Metadata\n",
        "        if 'v1.0-trainval' not in os.listdir():\n",
        "            print(\"Downloading and extracting TrainVal metadata ...\")\n",
        "            urllib.request.urlretrieve('https://s3.amazonaws.com/data.nuscenes.org/public/v1.0/v1.0-trainval_meta.tgz?AWSAccessKeyId=AKIA6RIK4RRMFUKM7AM2&Signature=ZDr9UgOoV3UpYCI5RCY%2BNKiZVZ4%3D&Expires=1651142002', 'v1.0-trainval_meta.tgz')\n",
        "            with tarfile.open('v1.0-trainval_meta.tgz', 'r:gz') as tar_ref:\n",
        "                tar_ref.extractall(os.getcwd())\n",
        "            os.remove('v1.0-trainval_meta.tgz')\n",
        "            os.chdir(DATAROOT)\n",
        "\n",
        "    print(\"Dataset prepared in %f s\" % (time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIFGeADe7MOB"
      },
      "source": [
        "Second dataset option [not recommended]: in Google Colab, mount to Drive if you have access to the complete preprocessed dataset in a shared folder within your Google Drive filesystem.\n",
        "\n",
        "N.B: Training is not feasible with the complete dataset on Google Colab, so if your intention is to iterate over all the dataset to test the model in this runtime (where it requires about 2/3 hours for each epoch), note that the default option for *MINI_DATASET* is True, so this code will not execute. Remember that Mini Dataset is the best way to test the models in this environment.\n",
        "\n",
        "Otherwise, if you just want to test or debug the complete dataset and check how it works on Colab with few baseline iterations on small dataset batches, please make sure to have access to the aforementioned shared folder, then restart runtime and toggle the *MINI_DATASET* option to False in the Environment-dependent parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHQCO92UCoCD"
      },
      "outputs": [],
      "source": [
        "# Drive initialization\n",
        "if ENVIRONMENT == 'colab' and not MINI_DATASET:\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75UDbRMTbA9V"
      },
      "source": [
        "**Dataset definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pY0cfPueR3K"
      },
      "source": [
        "Here follow the PyTorch and PyTorch-Lightning dataset classes. The hierarchy is the following one:\n",
        "\n",
        "```\n",
        "|- TrajPredictionDataset\n",
        "|------ nuScenesDataset\n",
        "|----------- nuScenesAdvanced\n",
        "```\n",
        "\n",
        "```\n",
        "|- TrajPredictionDataModule\n",
        "|------ nuScenesDataModule\n",
        "```\n",
        "\n",
        "The first class of each hierarchy is just a base class with abstract methods only. \n",
        "\n",
        "The nuScenes Dataset class is the main dataset class for this project; it allows a basic data preprocessing and a batch is composed by few essential things like the agent state vector or the raster map image. \n",
        "\n",
        "The nuScenesAdvanced class, instead, is an advanced representation that allows to handle grid representations of raster maps, and is more complex than the previous one. In order to avoid problems in using either one dataset or the other, it's convenient to preprocess different datasets in different folders, despite the fact that a model trained on a basic dataset can also execute with an advanced dataset; this is because if the dataset are generated with different image resolution, there is data loss and is difficult to restore image original resolution at execution time.\n",
        "\n",
        "Regarding the PyTorch-Lightning data modules, instead, it's very convenient to use them, in order to train the models or just encapsulate all the different splits of a dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mHqZde4mbDkC"
      },
      "outputs": [],
      "source": [
        "class TrajPredictionDataset(torch.utils.data.Dataset):\n",
        "    \"\"\" Trajectory Prediction Dataset\n",
        "\n",
        "    Base Class for Trajectory Prediction Datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, name, data_type, preprocessed, split,\n",
        "                 dataroot, preprocessed_folder, horizon_folder, filename_ext,\n",
        "                 gt_suffix, traj_horizon, max_traj_horizon, num_workers):\n",
        "        \"\"\" Dataset Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset: the instantiated dataset\n",
        "        name: name of the dataset\n",
        "        data_type: data type of the dataset elements\n",
        "        preprocessed: True if data has already been preprocessed\n",
        "        split: the dataset split ('train', 'train_val', 'val')\n",
        "        dataroot: the root directory of the dataset\n",
        "        preprocessed_folder: the folder containing preprocessed data\n",
        "        horizon_folder: the folder relative to the actual trajectory horizon\n",
        "        filename_ext: the extension of the generated filenames\n",
        "        gt_suffix: the suffix added after each GT filename (before ext)\n",
        "        traj_horizon: horizon (in seconds) for the future trajectory\n",
        "        max_traj_horizon: maximum trajectory horizon possible (in seconds)\n",
        "        num_workers: num of processes that collect data\n",
        "        \"\"\"\n",
        "        super(TrajPredictionDataset, self).__init__()\n",
        "        # self.time = {}\n",
        "        self.dataset = dataset\n",
        "        self.name = name\n",
        "        self.data_type = data_type\n",
        "        self.preprocessed = preprocessed\n",
        "        self.split = split\n",
        "        self.dataroot = dataroot\n",
        "        self.preprocessed_folder = preprocessed_folder\n",
        "        self.horizon_folder = horizon_folder\n",
        "        self.filename_ext = filename_ext\n",
        "        self.gt_suffix = gt_suffix\n",
        "        self.traj_horizon = traj_horizon\n",
        "        self.max_traj_horizon = max_traj_horizon\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Return the size of the dataset \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Return an element of the dataset \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_data(self):\n",
        "        \"\"\" Data generation\n",
        "\n",
        "        If self.preprocessed, directly collect data.\n",
        "        Otherwise, generate data without preprocess it.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_raster(self, token):\n",
        "        \"\"\" Convert a token split into a raster\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token: token containing instance token and sample token\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        raster: the raster image\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R_yNBqDztAh"
      },
      "outputs": [],
      "source": [
        "class TrajPredictionDataModule(pl.LightningDataModule):\n",
        "    \"\"\" PyTorch Lightning Data Module for the Trajectory Prediction Problem \"\"\"\n",
        "    def __init__(self, train_dataset: TrajPredictionDataset, val_dataset: TrajPredictionDataset, \n",
        "                 test_dataset=None, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "        \"\"\" Data Module initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataset: instance of the train dataset class\n",
        "        nuscenes_val: instance of the validation dataset class \n",
        "        batch_size: number of samples to extract from the dataset at each step\n",
        "        num_workers: number of cores implied in data collection\n",
        "        \"\"\"\n",
        "        super(TrajPredictionDataModule, self).__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        if test_dataset is None:\n",
        "            self.test_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "    \n",
        "    @abstractmethod\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\" Setup the data module \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_dataloader(self):\n",
        "        \"\"\" Dataloader for the training part \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def val_dataloader(self):\n",
        "        \"\"\" Dataloader for the validation part \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def test_dataloader(self):\n",
        "        \"\"\" Dataloader for the testing part \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxtRE7FJztAh"
      },
      "outputs": [],
      "source": [
        "class nuScenesDataset(TrajPredictionDataset):\n",
        "    \"\"\" nuScenes Dataset for Trajectory Prediction challenge \"\"\"\n",
        "    def __init__(self, helper: PredictHelper, data_type='raster', preprocessed=False,\n",
        "                 split='train', dataroot=DATAROOT, preprocessed_folder=PREPROCESSED_FOLDER,\n",
        "                 horizon_folder=HORIZON_FOLDER, filename_ext=FILENAME_EXT, gt_suffix=GT_SUFFIX, \n",
        "                 include_static=False, img_resolution=IMG_RESOLUTION, traj_horizon=TRAJ_HORIZON, \n",
        "                 max_traj_horizon=LONG_TERM_HORIZON, samples_per_second=SAMPLES_PER_SECOND,\n",
        "                 normalization=NORMALIZATION, helper_loading=HELPER_LOADING, num_workers=NUM_WORKERS):\n",
        "        \"\"\" nuScenes Dataset Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        helper: the helper of the instantiated nuScenes dataset (None if not needed)\n",
        "        data_type: data type of the dataset elements\n",
        "        preprocessed: True if data has already been preprocessed\n",
        "        split: the dataset split ('train', 'train_val', 'val')\n",
        "        dataroot: the root directory of the dataset\n",
        "        preprocessed_folder: the folder containing preprocessed data\n",
        "        horizon_folder: the folder relative to the actual trajectory horizon\n",
        "        filename_ext: the extension of the generated filenames\n",
        "        gt_suffix: the suffix added after each GT filename (before ext)\n",
        "        include_static: if to return also static rasters in __getitem__\n",
        "        img_resolution: resolution of the generated raster image (in meters/pixel)\n",
        "        traj_horizon: horizon (in seconds) for the future trajectory\n",
        "        max_traj_horizon: maximum trajectory horizon possible (in seconds)\n",
        "        samples_per_second: sampling frequency (in Hertz)\n",
        "        normalization: which kind of normalization to apply to input\n",
        "        helper_loading: True if to load data only with helper\n",
        "        num_workers: num of processes that collect data\n",
        "        \"\"\"\n",
        "        # General initialization\n",
        "        super(nuScenesDataset, self).__init__(\n",
        "            None, 'nuScenes', data_type, preprocessed, split, dataroot, preprocessed_folder, \n",
        "            horizon_folder, filename_ext, gt_suffix, traj_horizon, max_traj_horizon, num_workers)\n",
        "        # self.time['training:get_item'] = 0\n",
        "        # self.time['get_item:indicize_token'] = 0\n",
        "        # self.time['get_item:get_agent_raster'] = 0\n",
        "        # self.time['get_agent_raster:load_files'] = 0\n",
        "        # self.time['get_agent_raster:tensor_conversion'] = 0\n",
        "        # self.time['get_item:load_gt'] = 0\n",
        "        # self.time['get_item:normalization'] = 0\n",
        "        self.tokens = get_prediction_challenge_split(split, dataroot=dataroot)\n",
        "        self.helper = helper\n",
        "        self.include_static = include_static\n",
        "        self.samples_per_second = samples_per_second\n",
        "        self.helper_loading = helper_loading\n",
        "        if data_type == 'raster':\n",
        "            if helper is not None:\n",
        "                self.static_layer_rasterizer = StaticLayerRasterizer(\n",
        "                    self.helper, resolution=img_resolution)\n",
        "                self.agent_rasterizer = AgentBoxesWithFadedHistory(\n",
        "                    self.helper, seconds_of_history=1, resolution=img_resolution)\n",
        "                self.input_representation = InputRepresentation(\n",
        "                    self.static_layer_rasterizer, self.agent_rasterizer, Rasterizer())\n",
        "            else:\n",
        "                self.static_layer_rasterizer = None\n",
        "                self.agent_rasterizer = None\n",
        "                self.input_representation = None\n",
        "        else:   # NOTE: possible also other type of input data\n",
        "            pass\n",
        "        if not self.preprocessed:\n",
        "            print(\"Generating and Preprocessing data ...\")\n",
        "            self.generate_data()\n",
        "            self.preprocessed = True\n",
        "            if not helper_loading:\n",
        "                self.helper = None\n",
        "\n",
        "        # Normalization function\n",
        "        if normalization == 'imagenet':\n",
        "            self.normalization = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        else:\n",
        "            raise ValueError(\"Available only 'imagenet' normalization\")\n",
        "            \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\" Return the size of the dataset \"\"\"\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
        "        \"\"\" Return an element of the dataset \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: index of the element\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        agent_state_vector: vector [velocity, acceleration, yaw rate] of the target agent\n",
        "        raster_img_static: raster map of the scene, with only static element (empty if not self.include_static)\n",
        "        raster_img_dynamic: raster map of the scene, complete with dynamic elements\n",
        "        gt_trajectory: ground truth of the agent (agent future)\n",
        "        idx: index of the element\n",
        "        \"\"\"\n",
        "        # self.time['training:get_item'] -= time.time()\n",
        "        # self.time['get_item:get_agent_raster'] -= time.time()\n",
        "        # Select subfolder\n",
        "        if idx < 0:\n",
        "            idx = len(self) + idx\n",
        "        subfolder = f'batch_{idx//128}'\n",
        "        i_t, s_t = self.tokens[idx].split(\"_\")\n",
        "\n",
        "        # Get agent state vector and raster\n",
        "        agent_state_vector, raster_img = self.get_agent_raster(i_t, s_t, subfolder)\n",
        "        # self.time['get_item:get_agent_raster'] += time.time()\n",
        "\n",
        "        # Load GT trajectory\n",
        "        # self.time['get_item:load_gt'] -= time.time()\n",
        "        gt_trajectory = self.get_agent_future(i_t, s_t, subfolder)\n",
        "        gt_trajectory = gt_trajectory[:(self.samples_per_second * self.traj_horizon)]\n",
        "        # self.time['get_item:load_gt'] += time.time()\n",
        "\n",
        "        # self.time['get_item:normalization'] -= time.time()\n",
        "        # Include static data and normalize\n",
        "        if not self.include_static:\n",
        "            raster_img_dynamic = raster_img\n",
        "            raster_img_static = torch.empty((0,))\n",
        "        else:\n",
        "            # NOTE: this should be adapted also to non-square images\n",
        "            raster_img_static, raster_img_dynamic = \\\n",
        "                raster_img.split(raster_img.shape[-1], dim=1)\n",
        "            raster_img_static = self.normalization(raster_img_static)\n",
        "        raster_img_dynamic = self.normalization(raster_img_dynamic)\n",
        "        # self.time['get_item:normalization'] += time.time()\n",
        "\n",
        "        # self.time['training:get_item'] += time.time()\n",
        "        return agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\" Data generation\n",
        "\n",
        "        If self.preprocessed, directly collect data.\n",
        "        Otherwise, generate data and preprocess it.\n",
        "        \"\"\"\n",
        "        # Generate directories if don't exist\n",
        "        preprocessed_dir = os.path.join(self.dataroot, self.preprocessed_folder)\n",
        "        split_dir = os.path.join(preprocessed_dir, self.split)\n",
        "        if self.preprocessed_folder not in os.listdir(self.dataroot):\n",
        "            os.mkdir(preprocessed_dir)\n",
        "        if self.split not in os.listdir(preprocessed_dir):\n",
        "            os.mkdir(split_dir)\n",
        "\n",
        "        # Variable useful to restore interrupted preprocessing\n",
        "        preprocessed_batches = os.listdir(split_dir)\n",
        "        already_preproc = \\\n",
        "            len([f for batch in preprocessed_batches \n",
        "                   for f in os.listdir(os.path.join(split_dir, batch))\n",
        "                   if os.path.isfile(os.path.join(split_dir, batch, f))])\n",
        "\n",
        "        # Create subfolders\n",
        "        if len(preprocessed_batches) == 0:\n",
        "            n_subfolders = len(self.tokens) // 128 + int(len(self.tokens) % 128 != 0)\n",
        "            for i in range(n_subfolders):\n",
        "                subfolder = 'batch_' + str(i)\n",
        "                os.mkdir(os.path.join(split_dir, subfolder))\n",
        "\n",
        "        # Generate data\n",
        "        if self.data_type == 'raster':\n",
        "            for i, t in enumerate(tqdm(self.tokens)):\n",
        "                subfolder = f'batch_{i//128}'\n",
        "                if i >= int(already_preproc/2):\n",
        "                    self.generate_raster_data(t, split_dir, subfolder)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def generate_raster_data(self, token, batches_dir, subfolder):\n",
        "        \"\"\" Generate raster and agent data from a dataset token\n",
        "\n",
        "        The generated input data consists in a tensor like this:\n",
        "            [raster map | agent state vector]\n",
        "        The generated ground truth data is the future agent trajectory tensor\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token: token containing instance token and sample token\n",
        "        batches_dir: the directory in which the batches will be generated\n",
        "        subfolder: the data is divided into subfolders in order to avoid Drive timeouts;\n",
        "            this parameter tells which is the actual subfolder towhere place data\n",
        "        \"\"\"\n",
        "        # Generate agent state, raster img and GT\n",
        "        # (raster img both static and dynamic if self.include_static = True)\n",
        "        instance_token, sample_token = token.split(\"_\")\n",
        "        agent_state_vector, raster_tensor = \\\n",
        "            self.get_agent_raster(instance_token, sample_token)\n",
        "        gt_trajectory = self.get_agent_future(\n",
        "            instance_token, sample_token, traj_horizon=self.max_traj_horizon)\n",
        "\n",
        "        # Concatenate raster and state, then save all to disk\n",
        "        raster_agent_tensor, _ = \\\n",
        "            self.tensor_io_conversion('write', raster_tensor, agent_state_vector)\n",
        "        torch.save(raster_agent_tensor, os.path.join(\n",
        "            batches_dir, subfolder, token + self.filename_ext))\n",
        "        torch.save(gt_trajectory, os.path.join(\n",
        "            batches_dir, subfolder, token + self.gt_suffix + self.filename_ext))\n",
        "\n",
        "    def get_agent_raster(self, i_t, s_t, subfolder=None, state_only=False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Return agent state vector and raster image\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        i_t: instance_token\n",
        "        s_t: sampl_token\n",
        "        subfolder: the subfolder in which is contained the element\n",
        "        state_only: True if to return only the agent state\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        agent_state_vector: vector [velocity, acceleration, yaw rate] of the target agent\n",
        "        raster_img: raster image (both static and dynamic if self.include_static=True)\n",
        "        \"\"\"\n",
        "        # self.time[\"get_agent_raster:load_files\"] -= time.time()\n",
        "        # Load files\n",
        "        if self.helper is not None:\n",
        "            # Generate agent state\n",
        "            agent_state_vector = torch.Tensor(\n",
        "                [[self.helper.get_velocity_for_agent(i_t, s_t),\n",
        "                  self.helper.get_acceleration_for_agent(i_t, s_t),\n",
        "                  self.helper.get_heading_change_rate_for_agent(i_t, s_t)]])\n",
        "            # Handle nan values\n",
        "            nan_mask = agent_state_vector != agent_state_vector\n",
        "            if nan_mask.any():\n",
        "                agent_state_vector[nan_mask] = 0\n",
        "            # Generate raster img if not only state\n",
        "            if state_only:\n",
        "                raster_tensor = torch.empty((0,))\n",
        "            else:\n",
        "                raster_img = self.input_representation.make_input_representation(i_t, s_t)\n",
        "                raster_tensor = torch.from_numpy(raster_img).permute(2, 0, 1) / 255.\n",
        "                # Generate also static raster image\n",
        "                if self.include_static:     \n",
        "                    raster_img_static = \\\n",
        "                        self.static_layer_rasterizer.make_representation(i_t, s_t)\n",
        "                    raster_tensor_static = torch.from_numpy(raster_img_static).permute(2, 0, 1) / 255.\n",
        "                    raster_tensor = torch.cat([raster_tensor_static, raster_tensor], dim=1)                \n",
        "        else:\n",
        "            # Load agent state and raster img from disk\n",
        "            complete_tensor = torch.load(\n",
        "                os.path.join(self.dataroot, self.preprocessed_folder, self.split,\n",
        "                            subfolder, i_t + '_' + s_t + self.filename_ext))\n",
        "            # self.time[\"get_agent_raster:tensor_conversion\"] -= time.time()\n",
        "            # Separate state and rasters\n",
        "            agent_state_vector, raster_tensor = self.tensor_io_conversion(\n",
        "                \"read\", None, None, complete_tensor)\n",
        "            # self.time[\"get_agent_raster:tensor_conversion\"] += time.time()\n",
        "        # self.time[\"get_agent_raster:load_files\"] += time.time()\n",
        "\n",
        "        return agent_state_vector, raster_tensor\n",
        "\n",
        "    def get_agent_future(self, i_t, s_t, subfolder=None,\n",
        "                         in_agent_frame=True, traj_horizon=LONG_TERM_HORIZON) -> torch.Tensor:\n",
        "        \"\"\" Return ground truth trajectory of the agent\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        i_t: instance_token\n",
        "        s_t: sampl_token\n",
        "        subfolder: the subfolder in which is contained the element\n",
        "        in_agent_frame: True if the coordinates are expressed in agent frame\n",
        "        traj_horizon: the horizon (in seconds) of the trajectory to generate\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        gt_trajectory: ground truth of the agent (agent future)\n",
        "        \"\"\"\n",
        "        if self.helper is not None:\n",
        "            # Generate ground truth trajectory\n",
        "            gt_trajectory = torch.tensor(self.helper.get_future_for_agent(\n",
        "                i_t, s_t, seconds=traj_horizon, \n",
        "                in_agent_frame=in_agent_frame))\n",
        "            # Handle incomplete GT\n",
        "            if traj_horizon <= self.max_traj_horizon:\n",
        "                while gt_trajectory.shape[0] < self.samples_per_second * self.max_traj_horizon:\n",
        "                    gt_trajectory = torch.concat((gt_trajectory, gt_trajectory[-1].unsqueeze(0)))\n",
        "        else:\n",
        "            # Load GT from disk\n",
        "            gt_trajectory = torch.load(\n",
        "                os.path.join(self.dataroot, self.preprocessed_folder, self.split, subfolder,\n",
        "                             i_t + '_' + s_t + self.gt_suffix + self.filename_ext))\n",
        "\n",
        "        return gt_trajectory\n",
        " \n",
        "    @staticmethod\n",
        "    def tensor_io_conversion(mode, big_t=None, small_t=None, complete_t=None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Utility IO function to concatenate tensors of different shape\n",
        "\n",
        "        Normally used to concatenate (or separate) raster map and agent state vector in order to speed up IO\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mode: 'write' (concatenate) or 'read' (separate)\n",
        "        big_t: the bigger tensor (None if we are going to separate tensors)\n",
        "        small_t: the smaller tensor (None if we are going to separate tensors)\n",
        "        complete_t: the concatenated tensor (None if we are going to concatenate tensors)\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        out1: small tensor (mode == 'read') or complete tensor (mode == 'write')\n",
        "        out2: big tensor (mode == 'read') or empty tensor (mode == 'write') \n",
        "        \"\"\"\n",
        "        out1, out2 = None, None\n",
        "        if mode == 'write':    # concatenate\n",
        "            if big_t is None or small_t is None:\n",
        "                raise ValueError(\"Wrong argument: 'big_t' and 'small_t' cannot be None\")\n",
        "            small_t = small_t.permute(1, 0).unsqueeze(2)\n",
        "            small_t = small_t.expand(-1, -1, big_t.shape[-1])\n",
        "            out1 = torch.cat((big_t, small_t), dim=1)\n",
        "            out2 = torch.empty(small_t.shape)\n",
        "        elif mode == 'read':    # separate\n",
        "            if complete_t is None:\n",
        "                raise ValueError(\"Wrong argument: 'complete_t' cannot be None\")\n",
        "            out1 = complete_t[..., -1, -1].unsqueeze(0)\n",
        "            out2 = complete_t[..., :-1, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Wrong argument 'mode'; available 'read' or 'write'\")\n",
        "        return out1, out2                \n",
        "\n",
        "    @staticmethod\n",
        "    def read_time_dict(time_dict: dict, overall:float = None, parent:str = None, \n",
        "                       indent:str = \"\", general:bool = False, res:str = \"\") -> str:\n",
        "        \"\"\" Recursively extracts a multiline string describing the time distribution \n",
        "            of each block of code measured by a dictionary\n",
        "            \n",
        "        Parameters\n",
        "        ----------\n",
        "        time_dict: the dictionary. Its keys are like 'parent:child' and its values are expressed in seconds\n",
        "        overall: time for whole computation if general else time for parent computation\n",
        "        parent: the parent name\n",
        "        indent: indentation character to use\n",
        "        general: wether if scaling all results wrt the whole or the parent's computation time\n",
        "        res: current output string \n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        res: final output string\n",
        "        \"\"\"\n",
        "        if time_dict == {}:\n",
        "            return res\n",
        "        l = list(time_dict.items())\n",
        "        for k, v in l:\n",
        "            if parent is None and \":\" not in k:\n",
        "                res += indent + f\"{k} = {v:.3f}s\\n\"\n",
        "                new_p = k\n",
        "                overall = v\n",
        "            elif parent is not None and k.startswith(parent):\n",
        "                if overall is None:\n",
        "                    res += indent + f\"{k.split(':')[1]} = {v:.3f}s\\n\"\n",
        "                else:\n",
        "                    res += indent + f\"{k.split(':')[1]} = {v/overall*100:.1f}%\\n\"\n",
        "                new_p = k.split(\":\")[1]\n",
        "            else:\n",
        "                continue\n",
        "            new_t = time_dict.copy()\n",
        "            new_t.pop(k, 0)\n",
        "            new_v = overall if general else v\n",
        "            res = nuScenesDataset.read_time_dict(new_t, new_v, new_p, indent + \"  \", general, res)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm4onBqHztAj"
      },
      "outputs": [],
      "source": [
        "class nuScenesAdvanced(nuScenesDataset):\n",
        "    \"\"\" nuScenes  AdvancedDataset class \"\"\"\n",
        "\n",
        "    def __init__(self, helper: PredictHelper, data_type='raster', preprocessed=False,\n",
        "                 split='train', dataroot=DATAROOT, preprocessed_folder=PREPROCESSED_ADV_FOLDER,\n",
        "                 horizon_folder=HORIZON_FOLDER, filename_ext=FILENAME_EXT,\n",
        "                 additional_ext=ADDITIONAL_EXT, gt_suffix=GT_SUFFIX, add_suffix=ADD_SUFFIX,\n",
        "                 include_static=True, img_size=RASTER_SIZE, traj_horizon=TRAJ_HORIZON,\n",
        "                 mdp_horizon=MDP_HORIZON, max_traj_horizon=LONG_TERM_HORIZON,\n",
        "                 agent_history=AGENT_HISTORY, max_agent_history=LONG_AGENT_HISTORY,\n",
        "                 samples_per_second=SAMPLES_PER_SECOND, normalization=NORMALIZATION, \n",
        "                 helper_loading=HELPER_LOADING, grid_side=GRID_SIDE, grid_extent=GRID_EXTENT, \n",
        "                 num_workers=NUM_WORKERS):\n",
        "        \"\"\" nuScenes Advanced Dataset Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        helper: the helper of the instantiated nuScenes dataset (None if not needed)\n",
        "        data_type: data type of the dataset elements\n",
        "        preprocessed: True if data has already been preprocessed\n",
        "        split: the dataset split ('train', 'train_val', 'val')\n",
        "        dataroot: the root directory of the dataset\n",
        "        preprocessed_folder: the folder containing preprocessed data\n",
        "        horizon_folder: the folder relative to the actual trajectory horizon\n",
        "        filename_ext: the extension of the generated filenames\n",
        "        additional_ext: the extenstion of the generated additional files\n",
        "        gt_suffix: the suffix added after each GT filename (before ext)\n",
        "        add_suffix: the suffix added after each ADD filename (before ext)\n",
        "        include_static: if to return also static rasters in __getitem__\n",
        "        img_size: size of the raster map extracted from the dataset\n",
        "        traj_horizon: horizon (in seconds) for the future trajectory\n",
        "        mdp_horizon: horizon (in seconds) for the MDP\n",
        "        max_traj_horizon: maximum trajectory horizon possible (in seconds)\n",
        "        agent_history: the seconds of considered agent history\n",
        "        max_agent_history: maximum history tracked for an agent (in seconds)\n",
        "        samples_per_second: sampling frequency (in Hertz)\n",
        "        normalization: which kind of normalization to apply to input\n",
        "        helper_loading: True if to load data only with helper\n",
        "        grid_side: side of the square grid\n",
        "        grid_extent: extension of the grid for eventual map discretization\n",
        "        num_workers: num of processes that collect data\n",
        "        \"\"\"\n",
        "        # Advanced setting\n",
        "        self.img_size = img_size\n",
        "        self.grid_side = grid_side\n",
        "        self.mdp_horizon = mdp_horizon\n",
        "        self.agent_history = agent_history\n",
        "        self.max_agent_history = max_agent_history\n",
        "        self.additional_ext = additional_ext\n",
        "        self.add_suffix = add_suffix\n",
        "        self.grid_extent = torch.tensor(grid_extent)\n",
        "        self.grid_size = grid_extent[1] - grid_extent[0]\n",
        "        self.img_resolution = self.grid_size / img_size\n",
        "        self.row_centers = torch.linspace(self.grid_extent[3] - self.grid_size / (self.grid_side * 2),\n",
        "                                          self.grid_extent[2] + self.grid_size / (self.grid_side * 2),\n",
        "                                          self.grid_side)\n",
        "        self.col_centers = torch.linspace(self.grid_extent[0] + self.grid_size / (self.grid_side * 2),\n",
        "                                          self.grid_extent[1] - self.grid_size / (self.grid_side * 2),\n",
        "                                          self.grid_side)\n",
        "        self.centers = torch.stack([self.col_centers, self.row_centers], 0)\n",
        "\n",
        "        # Basic initialization\n",
        "        super(nuScenesAdvanced, self).__init__(\n",
        "            helper, data_type, preprocessed, split, dataroot,\n",
        "            preprocessed_folder, horizon_folder, filename_ext, gt_suffix,\n",
        "            include_static, self.img_resolution, traj_horizon, max_traj_horizon,\n",
        "            samples_per_second, normalization, helper_loading, num_workers)\n",
        "        # self.time['get_item:nuscenesDataset'] = 0\n",
        "        # self.time['get_item:expert_data'] = 0\n",
        "        # self.time['loop:1'] = 0\n",
        "        # self.time['loop:2'] = 0\n",
        "        # self.time['loop:3'] = 0\n",
        "        # self.time['loop:4'] = 0\n",
        "        # self.time['loop:5'] = 0\n",
        "        # self.time['get_item:add_data'] = 0\n",
        "        # self.time['get_item:surrounding_agents'] = 0\n",
        "        # self.time['surrounding_agents:get_annotation'] = 0\n",
        "        # self.time['surrounding_agents:populate_grid_loop'] = 0\n",
        "        # self.time['populate_grid_loop:1'] = 0\n",
        "        # self.time['populate_grid_loop:2'] = 0\n",
        "        # self.time['populate_grid_loop:3'] = 0\n",
        "        # self.time['populate_grid_loop:4'] = 0\n",
        "        # self.time['surrounding_agents:representation_data'] = 0\n",
        "        # self.time['representation_data:3'] = 0\n",
        "        # self.time['get_item:agent_past'] = 0\n",
        "        # self.time['agent_past:adjust_shape'] = 0\n",
        "        # self.time['agent_past:set_tokens'] = 0\n",
        "        # self.time['agent_past:fill_past'] = 0\n",
        "        # self.time['agent_past:zero_padding'] = 0\n",
        "        # self.time['get_item:future_indefinite'] = 0\n",
        "        # self.time['get_item:motion_feats'] = 0\n",
        "\n",
        "        # Grid initialization\n",
        "        self.grid_representation = GridRepresentation(\n",
        "            helper, resolution=self.img_resolution,\n",
        "            meters_ahead=grid_extent[3], meters_behind=-grid_extent[2],\n",
        "            meters_left=-grid_extent[0], meters_right=grid_extent[1])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Return an element of the dataset \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: index of the element\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        agent_state_vector: vector [velocity, acceleration, yaw rate] of the target agent\n",
        "        raster_img_static: raster map of the scene, with only static element (empty if not self.include_static)\n",
        "        raster_img_dynamic: raster map of the scene, complete with dynamic elements\n",
        "        gt_trajectory: ground truth of the agent (agent future)\n",
        "        idx: index of the element\n",
        "        \"\"\"\n",
        "        # Get basic data\n",
        "        # self.time['get_item:nuscenesDataset'] -= time.time()\n",
        "        agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx = \\\n",
        "            super().__getitem__(idx)\n",
        "        # self.time['get_item:nuscenesDataset'] += time.time()\n",
        "        # self.time['training:get_item'] -= time.time()\n",
        "\n",
        "        # Get additional data\n",
        "        # self.time['get_item:add_data'] -= time.time()\n",
        "        if idx < 0:\n",
        "            idx = len(self) + idx\n",
        "        subfolder = f'batch_{idx//128}'\n",
        "        i_t, s_t = self.tokens[idx].split(\"_\")\n",
        "        add_data = self.get_add_data(i_t, s_t, subfolder)\n",
        "        # self.time['get_item:add_data'] += time.time()\n",
        "\n",
        "        # Get surrounding agents representation\n",
        "        # self.time['get_item:surrounding_agents'] -= time.time()\n",
        "        # self.grid_representation.time = self.time\n",
        "        surr_agents = self.grid_representation.make_representation(\n",
        "            i_t, s_t, agent_state_vector,\n",
        "            add_data['annotations'], add_data['ann_states'], add_data['sample_annotation'])\n",
        "        surr_agents = surr_agents.permute((2, 0, 1)).float()\n",
        "        # self.time = self.grid_representation.time\n",
        "        # self.time['get_item:surrounding_agents'] += time.time()\n",
        "\n",
        "        # Downsample to grid dimensions with avg pooling\n",
        "        surr_agents = F.avg_pool2d(\n",
        "            surr_agents.unsqueeze(0), self.img_size // self.grid_side)\n",
        "        surr_agents = \\\n",
        "            surr_agents.squeeze(0) * ((self.img_size // self.grid_side) ** 2)\n",
        "\n",
        "        # Manage agent's past\n",
        "        # self.time['get_item:agent_past'] -= time.time()\n",
        "        past = self.manage_past(\n",
        "            add_data['xy_past'], add_data['complete_past'], i_t, s_t)\n",
        "        # self.time['get_item:agent_past'] += time.time()\n",
        "\n",
        "        # Upsample future indefinite\n",
        "        # self.time['get_item:future_indefinite'] -= time.time()\n",
        "        future_indefinite = add_data['future_indefinite']\n",
        "        fp = np.concatenate(([[0,0]], future_indefinite))\n",
        "        x = np.linspace(0, future_indefinite.shape[0], future_indefinite.shape[0] * 10 + 1)\n",
        "        xp = np.linspace(0, future_indefinite.shape[0], future_indefinite.shape[0] + 1)\n",
        "        future_upsampled = torch.from_numpy(np.stack([\n",
        "            np.interp(x, xp, fp[..., 0]),\n",
        "            np.interp(x, xp, fp[..., 1])],\n",
        "            axis=-1))\n",
        "        # self.time['get_item:future_indefinite'] += time.time()\n",
        "\n",
        "        # Extract expert data\n",
        "        # self.time['get_item:expert_data'] -= time.time()\n",
        "        svf_e, plan_e, grid_idcs = self.extract_expert_data(future_upsampled)\n",
        "        # self.time['get_item:expert_data'] += time.time()\n",
        "\n",
        "        # Extract motion features\n",
        "        # self.time['get_item:motion_feats'] -= time.time()\n",
        "        motion_feats = self.extract_motion_feats(agent_state_vector[0][0])\n",
        "        # self.time['get_item:motion_feats'] += time.time()\n",
        "        # self.time['training:get_item'] += time.time()\n",
        "\n",
        "        return (agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx,\n",
        "                past, surr_agents, svf_e, plan_e, grid_idcs, motion_feats)\n",
        "\n",
        "    def manage_past(self, xy_past, complete_past, instance_token, sample_token) -> torch.Tensor:\n",
        "        \"\"\" Manage past and add dynamic features\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xy_past: agent's past consisting only in waypoints\n",
        "        complete_past: agent's past with complete informations\n",
        "        instance_token: Instance token\n",
        "        sample_token: Sample token\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        past: enhanced past tensor\n",
        "        \"\"\"\n",
        "        # self.time[\"agent_past:adjust_shape\"] -= time.time()\n",
        "        # Adjust past shape\n",
        "        num_samples = self.agent_history * self.samples_per_second\n",
        "        xy_past = xy_past[0:min(xy_past.shape[0], num_samples)]\n",
        "        complete_past = complete_past[0:min(len(complete_past), num_samples)]\n",
        "        # self.time[\"agent_past:adjust_shape\"] += time.time()\n",
        "\n",
        "        # self.time[\"agent_past:set_tokens\"] -= time.time()\n",
        "        # Set tokens from the past\n",
        "        i_ts = [complete_past[i]['instance_token']\n",
        "                for i in range(len(complete_past))]\n",
        "        s_ts = [complete_past[i]['sample_token']\n",
        "                for i in range(len(complete_past))]\n",
        "        i_ts = [instance_token] + i_ts\n",
        "        s_ts = [sample_token] + s_ts\n",
        "        # self.time[\"agent_past:set_tokens\"] += time.time()\n",
        "\n",
        "        # self.time[\"agent_past:fill_past\"] -= time.time()\n",
        "        # Initialize past\n",
        "        past = torch.zeros((xy_past.shape[0], 5), dtype=torch.float32)\n",
        "        past[:, 0:2] = xy_past\n",
        "\n",
        "        # Fill the past with dynamic values\n",
        "        for i in range(xy_past.shape[0]):\n",
        "            i_t = i_ts[i]\n",
        "            s_t = s_ts[i]\n",
        "            try:\n",
        "                agent_state_vector = self.get_agent_raster(i_t, s_t, state_only=True)[0]\n",
        "                past[i, 2:5] = agent_state_vector[0]\n",
        "            except:\n",
        "                agent_state_vector = None \n",
        "                continue\n",
        "        # self.time[\"agent_past:fill_past\"] += time.time()\n",
        "\n",
        "        # self.time[\"agent_past:zero_padding\"] -= time.time()\n",
        "        # Zero-padding when tracked history < agent history\n",
        "        past_zeropadded = torch.cat(\n",
        "            (past, torch.zeros((num_samples - past.shape[0], 5))), dim=0)\n",
        "        past_zeropadded_flipped = past_zeropadded.flip([0])\n",
        "        # self.time[\"agent_past:zero_padding\"] += time.time()\n",
        "\n",
        "        return past_zeropadded_flipped\n",
        "\n",
        "    def extract_expert_data(self, future_upsampled: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Extract expert SVF and plan, looking at the future trajectory\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        future_upsampled: future trajectory, upsampled by a factor of 10\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        svf_e: expert State Visitation Frequencies tensor\n",
        "        plan_e: expert Plan (waypoints) tensor\n",
        "        grid_idcs: tensor of grid indices related to the plan\n",
        "        \"\"\"\n",
        "        # Expert data intialization\n",
        "        svf_e = torch.zeros((2, self.grid_side, self.grid_side))\n",
        "        plan_e = torch.zeros((self.mdp_horizon, 2))\n",
        "        grid_idcs = torch.zeros((self.mdp_horizon, 2))\n",
        "\n",
        "        # Other intializations\n",
        "        column_old = None\n",
        "        row_old = None\n",
        "        count = 0\n",
        "\n",
        "        # Expert data extraction\n",
        "        \n",
        "        # self.time['loop:2'] -= time.time()\n",
        "        min_grid = self.grid_extent[[0, 2]]\n",
        "        max_grid = self.grid_extent[[1, 3]]\n",
        "        # self.time['loop:3'] -= time.time()\n",
        "        # out of frame points in future_upsampled\n",
        "        out_grid = torch.logical_or(\n",
        "            future_upsampled < min_grid, future_upsampled > max_grid)\n",
        "        # future upsampled reduction to first n points in frame\n",
        "        try:\n",
        "            split_index = torch.arange(0, future_upsampled.shape[0])[\n",
        "                out_grid.any(1)][0]\n",
        "            reduced_future = torch.split(future_upsampled, split_index)[0]\n",
        "        except:\n",
        "            reduced_future = future_upsampled\n",
        "        # closest indices to centers in future_upsampled\n",
        "        grid_new = torch.argmin(\n",
        "            torch.abs(reduced_future.unsqueeze(-1) - self.centers), dim=-1).type(torch.int64)\n",
        "        # self.time['loop:3'] += time.time()\n",
        "        \n",
        "\n",
        "        # slowest portion (probably unboostable)\n",
        "        # self.time['loop:4'] -= time.time()\n",
        "        # remove consecutive occurences\n",
        "        reduced_grid_new = torch.unique_consecutive(grid_new, dim=0) \n",
        "        # self.time['loop:4'] += time.time()\n",
        "\n",
        "        # self.time['loop:5'] -= time.time()\n",
        "        # plan and grid_idcs assignments\n",
        "        if self.mdp_horizon > reduced_grid_new.shape[0]:\n",
        "            plan_e[:reduced_grid_new.shape[0]] = torch.gather(\n",
        "                self.centers, 1, reduced_grid_new.permute(1, 0)).flip(0).permute(1, 0)\n",
        "            grid_idcs[:reduced_grid_new.shape[0]] = reduced_grid_new.flip(1)\n",
        "        else:\n",
        "            plan_e = torch.gather(self.centers, 1, reduced_grid_new[:self.mdp_horizon].permute(\n",
        "                1, 0)).flip(0).permute(1, 0)\n",
        "            grid_idcs = reduced_grid_new[:self.mdp_horizon].flip(1)\n",
        "\n",
        "        # Plan svf\n",
        "        svf_e[0][reduced_grid_new[:, 1], reduced_grid_new[:, 0]] = 1\n",
        "        # Goal svf\n",
        "        svf_e[1][tuple(reduced_grid_new[-1].flip(0))] = 1\n",
        "\n",
        "        # self.time['loop:5'] += time.time()\n",
        "        # self.time['loop:2'] += time.time()\n",
        "        # self.time['expert_data:loop'] += time.time()\n",
        "\n",
        "        return svf_e, plan_e.float(), grid_idcs.float()\n",
        "\n",
        "    def extract_motion_feats(self, vel) -> torch.Tensor:\n",
        "        \"\"\" Extract position and velocity informations, related to the grid\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        vel: current velocity of the agent\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        motion_feats: position and velocity informations over the grid\n",
        "        \"\"\"\n",
        "        # Extract x and y coordinates\n",
        "        x = self.col_centers.reshape(-1, 1).repeat_interleave(self.grid_side, dim=1).T\n",
        "        y = self.row_centers.reshape(-1, 1).repeat_interleave(self.grid_side, dim=1)\n",
        "\n",
        "        motion_feats = torch.Tensor([[[vel]*self.grid_side]*self.grid_side, \n",
        "                                      x/self.grid_size, y/self.grid_size])\n",
        "\n",
        "        return motion_feats\n",
        "\n",
        "    def generate_raster_data(self, token, batches_dir, subfolder):\n",
        "        \"\"\" Generate raster and agent data from a dataset token\n",
        "\n",
        "        The generated input data consists in a tensor like this:\n",
        "            [raster map | agent state vector]\n",
        "        The generated ground truth data is the future agent trajectory tensor\n",
        "        The generated additional data is a dictionary with fields:\n",
        "            - 'future_indefinite': future with indefinite horizon (300 s)\n",
        "            - 'xy_past': history of the agent (only xy)\n",
        "            - 'complete_past': complete history of the agent\n",
        "            - 'annotations': annotations relative to the given token\n",
        "            - 'sample_annotation': sample annotation of the given token\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        token: token containing instance token and sample token\n",
        "        batches_dir: the directory in which the batches will be generated\n",
        "        subfolder: the data is divided into subfolders in order to avoid Drive timeouts;\n",
        "            this parameter tells which is the actual subfolder towhere place data\n",
        "        \"\"\"\n",
        "        # Generate basic data\n",
        "        super().generate_raster_data(token, batches_dir, subfolder)\n",
        "        instance_token, sample_token = token.split(\"_\")\n",
        "\n",
        "        # Generate additional data\n",
        "        add_data = self.get_add_data(instance_token, sample_token, subfolder)\n",
        "        \n",
        "        # Save to disk\n",
        "        np.save(os.path.join(batches_dir, subfolder, token +\n",
        "                self.add_suffix + self.additional_ext), add_data)\n",
        "    \n",
        "    def get_add_data(self, i_t, s_t, subfolder=None):\n",
        "        \"\"\" Return additional data for the agent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        i_t: instance_token\n",
        "        s_t: sampl_token\n",
        "        subfolder: the subfolder in which is contained the element\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        add_data: the additional data dict\n",
        "        \"\"\"\n",
        "        if self.helper is not None:\n",
        "            # Generate additional data\n",
        "            future_indefinite = self.get_agent_future(i_t, s_t, traj_horizon=300)\n",
        "            xy_past = torch.Tensor(\n",
        "                self.helper.get_past_for_agent(i_t, s_t, \n",
        "                    seconds=self.max_agent_history, in_agent_frame=True))\n",
        "            complete_past = self.helper.get_past_for_agent(i_t, s_t,\n",
        "                seconds=self.max_agent_history, in_agent_frame=True, just_xy=False)\n",
        "            annotations = self.helper.get_annotations_for_sample(s_t)\n",
        "            sample_annotation = self.helper.get_sample_annotation(i_t, s_t)\n",
        "            ann_states = []\n",
        "            for ann in annotations:\n",
        "                agent_state_vector = self.get_agent_raster(\n",
        "                    ann['instance_token'], ann['sample_token'], state_only=True)[0]\n",
        "                ann_states.append(agent_state_vector)\n",
        "            # Incapsulate in a dictionary\n",
        "            add_data = {\n",
        "                'future_indefinite': future_indefinite,\n",
        "                'xy_past': xy_past,\n",
        "                'complete_past': complete_past,\n",
        "                'annotations': annotations,\n",
        "                'ann_states': ann_states,\n",
        "                'sample_annotation': sample_annotation\n",
        "            }\n",
        "        else:\n",
        "            # Load additional data from disk\n",
        "            add_data = np.load(os.path.join(\n",
        "                self.dataroot, self.preprocessed_folder, self.split,\n",
        "                subfolder, i_t + '_' + s_t + self.add_suffix + self.additional_ext),\n",
        "                allow_pickle=True).item()\n",
        "\n",
        "        return add_data\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_plan_features(plans, scene_tensor, agent_tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Extract scene features and agent features, given plans\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        plans: waypoints (on grid) from which to extract features\n",
        "        scene_tensor: raster map features from the reward model\n",
        "        agent_tensor: surrounding agents informations tensor\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        scene_feats: scene features along plan\n",
        "        agent_feats: agent features along plan\n",
        "        \"\"\"\n",
        "        scene_side = scene_tensor.shape[2]\n",
        "        scene_tensor = scene_tensor.flatten(-2)\n",
        "        agent_tensor = agent_tensor.flatten(-2)\n",
        "        plans = plans[..., :, 0] * scene_side + plans[..., :, 1]\n",
        "        plans_s = plans.unsqueeze(1).repeat(1, scene_tensor.shape[1], 1).long()\n",
        "        plans_a = plans.unsqueeze(1).repeat(1, agent_tensor.shape[1], 1).long()\n",
        "        scene_feats = torch.gather(scene_tensor, 2, plans_s).permute(0, 2, 1)\n",
        "        agent_feats = torch.gather(agent_tensor, 2, plans_a).permute(0, 2, 1)\n",
        "        return scene_feats, agent_feats\n",
        "\n",
        "\n",
        "class GridRepresentation(AgentRepresentation):\n",
        "    \"\"\" Grid Representation class for the advanced dataset \"\"\"\n",
        "\n",
        "    def __init__(self, helper: PredictHelper, resolution: float = 0.1,  # meters / pixel\n",
        "                 meters_ahead: float = 40, meters_behind: float = 10,\n",
        "                 meters_left: float = 25, meters_right: float = 25):\n",
        "        \"\"\" Grid Representation initialization (follows the original NS interface)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        helper: the helper of the instantiated nuScenes dataset (None if not needed)\n",
        "        resolution: meters / pixel\n",
        "        meters_ahead: map extention value (in meters) -> ahead \n",
        "        meters_behinf: map extention value (in meters) -> behind \n",
        "        meters_left: map extention value (in meters) -> left\n",
        "        meters_right: map extention value (in meters) -> right \n",
        "        \"\"\"\n",
        "        self.helper = helper\n",
        "        self.resolution = resolution\n",
        "        self.meters_ahead = meters_ahead\n",
        "        self.meters_behind = meters_behind\n",
        "        self.meters_left = meters_left\n",
        "        self.meters_right = meters_right\n",
        "\n",
        "    def make_representation(self, instance_token: str, sample_token: str, state = None, \n",
        "                            annotations = None, ann_states = None, sample_ann = None) -> torch.Tensor:\n",
        "        \"\"\" Make the Grid Representation for the current token\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance_token: Instance token\n",
        "        sample_token: Sample token\n",
        "        state: vector [velocity, acceleration, yaw rate] of the target agent\n",
        "        annotations: annotations related to the target agent\n",
        "        ann_states: states for each annotation in annotations\n",
        "        sample_ann: sample annotation related to the target agent\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        grid_representation: array containing the grid representation (4-channel image)\n",
        "        \"\"\"\n",
        "        # Taking radius around track before to ensure all actors are in image\n",
        "        buffer = max([self.meters_ahead, self.meters_behind,\n",
        "                      self.meters_left, self.meters_right]) * 2\n",
        "\n",
        "        image_side_length = int(buffer / self.resolution)\n",
        "\n",
        "        # We will center the track in the image\n",
        "        central_track_pixels = (image_side_length / 2, image_side_length / 2)\n",
        "        base_image = torch.zeros((image_side_length, image_side_length, 4))\n",
        "\n",
        "        # Grid population\n",
        "        # self.time['surrounding_agents:get_annotation'] -= time.time()\n",
        "        if self.helper is None and annotations is not None and sample_ann is not None:\n",
        "            # helper-less modality\n",
        "            # obtain annotations and states from basic and additional data\n",
        "            annotations = annotations\n",
        "            center_agent_annotation = sample_ann\n",
        "            [velocity, acceleration, yaw_rate] = state[0]\n",
        "        elif self.helper is not None:\n",
        "            # helper-bound modality\n",
        "            # obtain annotations and states from the helper\n",
        "            annotations = self.helper.get_annotations_for_sample(sample_token)\n",
        "            center_agent_annotation = self.helper.get_sample_annotation(\n",
        "                instance_token, sample_token)\n",
        "            velocity = self.helper.get_velocity_for_agent(\n",
        "                instance_token, sample_token)\n",
        "            acceleration = self.helper.get_acceleration_for_agent(\n",
        "                instance_token, sample_token)\n",
        "            yaw_rate = self.helper.get_heading_change_rate_for_agent(\n",
        "                instance_token, sample_token)\n",
        "            # check for nan values\n",
        "            velocity = velocity if velocity == velocity else 0.\n",
        "            acceleration = acceleration if acceleration == acceleration else 0.\n",
        "            yaw_rate = yaw_rate if yaw_rate == yaw_rate else 0.\n",
        "        # self.time['surrounding_agents:get_annotation'] += time.time()\n",
        "        grid = self.populate_grid(base_image, annotations, ann_states, \n",
        "            center_agent_annotation, central_track_pixels)\n",
        "\n",
        "        # Rotate and crop representation\n",
        "        # self.time['surrounding_agents:representation_data'] -= time.time()\n",
        "        center_agent_yaw = quaternion_yaw(\n",
        "            Quaternion(center_agent_annotation['rotation']))\n",
        "        rotation_mat = get_rotation_matrix(grid.shape, center_agent_yaw)\n",
        "\n",
        "        # self.time['representation_data:3'] -= time.time()  # slowest segment\n",
        "        rotated_grid = torch.from_numpy(cv2.warpAffine(np.array(grid), rotation_mat, (grid.shape[1],\n",
        "                                                                                  grid.shape[0])))\n",
        "        # self.time['representation_data:3'] += time.time()\n",
        "\n",
        "        row_crop, col_crop = get_crops(self.meters_ahead, self.meters_behind,\n",
        "                                       self.meters_left, self.meters_right, self.resolution,\n",
        "                                       image_side_length)\n",
        "\n",
        "        # self.time['surrounding_agents:representation_data'] += time.time()\n",
        "        return rotated_grid[row_crop, col_crop]\n",
        "\n",
        "    def populate_grid(self, grid, annotations, ann_states, sample_annotation, central_track_pixels) -> torch.Tensor:\n",
        "        \"\"\" Populate the grid with agent states\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        grid: the grid (4-channel image) to populate\n",
        "        annotations: annotations related to the token\n",
        "        ann_states: states for each annotation in annotations\n",
        "        sample_annotation: sample annotation of the token\n",
        "        central_track_pixels: track of pixels centered in the image (?)\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        grid: the grid (4-channel image) populated\n",
        "        \"\"\"\n",
        "        agent_x, agent_y = sample_annotation['translation'][:2]\n",
        "        # grid population loop\n",
        "        # self.time['surrounding_agents:populate_grid_loop'] -= time.time()\n",
        "        ann_states = torch.cat(ann_states, 0)\n",
        "        grid_pixels_dict = {}\n",
        "        # self.time['populate_grid_loop:1'] -= time.time()\n",
        "        for i, annotation in enumerate(annotations):\n",
        "            if annotation['instance_token'] != sample_annotation['instance_token']:\n",
        "                # get pixels to populate\n",
        "                location = annotation['translation'][:2]\n",
        "                row_pixel, column_pixel = convert_to_pixel_coords(\n",
        "                    location, (agent_x, agent_y), central_track_pixels, self.resolution)\n",
        "                # self.time['populate_grid_loop:3'] -= time.time()\n",
        "                grid_pixels_dict[(row_pixel, column_pixel)] = grid_pixels_dict.get((row_pixel, column_pixel), []) + [i]\n",
        "                # self.time['populate_grid_loop:3'] += time.time()\n",
        "\n",
        "        # self.time['populate_grid_loop:1'] += time.time()\n",
        "        if grid_pixels_dict != {}:\n",
        "            # self.time['populate_grid_loop:2'] -= time.time()\n",
        "            grid_pixels = torch.tensor([[*k, len(l)] for k, l in grid_pixels_dict.items()]).type(torch.long)\n",
        "            max_len = torch.max(grid_pixels[:, 2]).item()\n",
        "            idcs = torch.tensor([l + [l[0]]*(max_len-len(l)) for l in grid_pixels_dict.values()]).type(torch.long)\n",
        "            indexes = torch.logical_and(grid_pixels[:,:2] >= torch.zeros(\n",
        "                1, 2), grid_pixels[:,:2] < torch.tensor(grid.shape[:2])).all(1)\n",
        "\n",
        "            temp_ann_states = torch.min(ann_states[idcs], 1).values\n",
        "            temp_ann_states = temp_ann_states[indexes]\n",
        "\n",
        "            grid[grid_pixels[indexes][:, 0], grid_pixels[indexes][:, 1]] = torch.cat((grid_pixels[indexes][:,2].unsqueeze(1), temp_ann_states), 1)\n",
        "            # self.time['populate_grid_loop:2'] += time.time()\n",
        "\n",
        "        # self.time['surrounding_agents:populate_grid_loop'] += time.time()\n",
        "\n",
        "        return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18vos9kjztAl"
      },
      "outputs": [],
      "source": [
        "class nuScenesDataModule(TrajPredictionDataModule):\n",
        "    \"\"\" PyTorch Lightning Data Module for the nuScenes dataset \"\"\"\n",
        "    def __init__(self, nuscenes_train, nuscenes_val, nuscenes_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "        \"\"\" Data Module initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        nuscenes_train: instance of the nuScenesDataset class (split='train')\n",
        "        nuscenes_val: instance of the nuScenesDataset class (split='train_val')\n",
        "        nuscenes_test: instance of the nuScenesDataset class (split='val')\n",
        "        batch_size: number of samples to extract from the dataset at each step\n",
        "        num_workers: number of cores implied in data collection\n",
        "        \"\"\"\n",
        "        super(nuScenesDataModule, self).__init__(\n",
        "            nuscenes_train, nuscenes_val, nuscenes_test, batch_size, num_workers)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\" Setup the data module \"\"\"\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.nusc_train = self.train_dataset\n",
        "            self.nusc_val = self.val_dataset\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.nusc_test = self.test_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\" Dataloader for the training part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_train, self.batch_size, shuffle=True,\n",
        "                                           num_workers=self.num_workers, drop_last=True, pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\" Dataloader for the validation part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_val, self.batch_size, shuffle=False, \n",
        "                                           num_workers=self.num_workers, drop_last=True, pin_memory=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\" Dataloader for the testing part \"\"\"\n",
        "        return torch.utils.data.DataLoader(self.nusc_test, self.batch_size, shuffle=False,\n",
        "                                           num_workers=self.num_workers, drop_last=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isw61mh9-IZD"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blyIPM3r-TgN"
      },
      "source": [
        "#### **Covernet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHVA5dSwiaSR"
      },
      "source": [
        "TODO: describe briefly CoverNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: describe briefly CoverNet"
      ],
      "metadata": {
        "id": "HHVA5dSwiaSR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g_OGzCNn-TZA"
      },
      "outputs": [],
      "source": [
        "class CoverNet(pl.LightningModule):\n",
        "    \"\"\" CoverNet model for Trajectory Prediction \"\"\"\n",
        "    def __init__(self, K_size, epsilon, traj_link, traj_dir, device, \n",
        "                 lr=COVERNET_LR, momentum=COVERNET_MOMENTUM,\n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON):\n",
        "        \"\"\" CoverNet initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        K_size: number of modes (trajectories) (needed ?)\n",
        "        epsilon: value (in meters) relative to the space coverage\n",
        "        traj_link: link from which to download the trajectories\n",
        "        device: target device of the model (e.g. 'cuda:0')\n",
        "        lr: learning rate of the optimizer\n",
        "        momentum: momentum of the optimizer\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.K_size = K_size\n",
        "        self.convModel = resnet50(pretrained=True)\n",
        "        self.activation = {}\n",
        "        def get_activation(name):\n",
        "            def hook(model, input, output):\n",
        "                self.activation[name] = output\n",
        "            return hook\n",
        "        self.convModel.layer4.register_forward_hook(get_activation('layer4'))\n",
        "        self.trajectories = prepare_trajectories(epsilon, traj_link, traj_dir)\n",
        "        self.fc1 = nn.Linear(2051, 4096)\n",
        "        self.fc2 = nn.Linear(4096, self.trajectories.size()[0])\n",
        "        self.traj_samples = traj_samples\n",
        "        self.tgt_device = device\n",
        "        self.momentum = momentum\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        \"\"\" Network inference \"\"\"\n",
        "        img, state = x\n",
        "        self.convModel(img)\n",
        "        resnet_output = torch.flatten(self.convModel.avgpool(self.activation['layer4']),start_dim=1)\n",
        "        x = torch.cat([resnet_output, state], 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\" Training step of the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: batch of data\n",
        "        batch_idx: index of the actual batch (from 0 to len(dataset))\n",
        "        \"\"\"\n",
        "        # Collect data\n",
        "        x_state, _, x_img, gt, _ = batch\n",
        "        x_state = torch.flatten(x_state, 0, 1)\n",
        "        reduced_traj = self.trajectories[:, :self.traj_samples]\n",
        "        # Prepare positive samples\n",
        "        with torch.no_grad():\n",
        "            y = get_positives(reduced_traj, gt.to('cpu'))\n",
        "            y = y.to(self.tgt_device)\n",
        "        # Inference\n",
        "        y_hat = self((x_img, x_state))\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        # Log\n",
        "        self.log('train_loss', loss.item(), on_step=True)\n",
        "            \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\" Validation step of the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: batch of data\n",
        "        batch_idx: index of the actual batch (from 0 to len(dataset))\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Collect data\n",
        "            x_state, _, x_img, gt, _ = batch\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            reduced_traj = self.trajectories[:, :self.traj_samples]\n",
        "            # Prepare positive samples\n",
        "            y = get_positives(reduced_traj, gt.to('cpu'))\n",
        "            y = y.to(self.tgt_device)\n",
        "            # Inference\n",
        "            y_hat = self((x_img, x_state))\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "        # Log\n",
        "        self.log('val_loss', loss.item(), on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\" Set the optimizer for the model \"\"\"\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
        "\n",
        "def get_positives(trajectories, ground_truth) -> torch.Tensor:\n",
        "    \"\"\" Get positive samples wrt the actual GT\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trajectories: the pre-generated set of trajectories\n",
        "    ground_truth: the future trajectory for the agent\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    positive_traj: as defined in the original CoverNet paper, \n",
        "        'positive samples determined by the element in the trajectory set\n",
        "        closest to the actual ground truth in minimum average \n",
        "        of point-wise Euclidean distances'\n",
        "    \"\"\"\n",
        "    euclidean_dist = torch.stack([torch.pow(torch.sub(trajectories, gt), 2) \n",
        "                                  for gt in ground_truth]).sum(dim=3).sqrt() \n",
        "    mean_euclidean_dist = euclidean_dist.mean(dim=2)\n",
        "    positive_traj = mean_euclidean_dist.argmin(dim=1)\n",
        "    return positive_traj\n",
        "\n",
        "def prepare_trajectories(epsilon, download_link, directory) -> torch.Tensor:\n",
        "    \"\"\" Function to download and extract trajectory sets for CoverNet \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    epsilon: value (in meters) relative to the space coverage\n",
        "    download_link: link from which to download trajectory sets\n",
        "    directory: directory where to download trajectory sets\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    trajectories: tensor of the trajectory set for the specified epsilon\n",
        "    \"\"\"\n",
        "    # 1. Download and extract trajectories\n",
        "    filename_zip = 'nuscenes-prediction-challenge-trajectory-sets.zip'\n",
        "    filename = filename_zip[:-4]\n",
        "    filename_dir = os.path.join(directory, filename)\n",
        "    filename_zipdir = os.path.join(directory, filename_zip)\n",
        "    if (not os.path.isdir(filename_dir) \n",
        "        or any(e not in os.listdir(filename_dir)\n",
        "               for e in ['epsilon_2.pkl', 'epsilon_4.pkl', 'epsilon_8.pkl'])):\n",
        "        print(\"Downloading trajectories ...\")\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        urllib.request.urlretrieve(download_link, filename_zipdir)\n",
        "        with zipfile.ZipFile(filename_zipdir, 'r') as archive:\n",
        "            archive.extractall(directory)\n",
        "        os.remove(filename_zipdir)\n",
        "\n",
        "    # 2. Generate trajectories\n",
        "    traj_set_path = os.path.join(filename_dir, 'epsilon_' + str(epsilon) + '.pkl')\n",
        "    trajectories = pickle.load(open(traj_set_path, 'rb'))\n",
        "    return torch.Tensor(trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJhwEHw4E8c"
      },
      "source": [
        "#### **P2T**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMb2vKnyif2A"
      },
      "source": [
        "TODO: describe briefly P2T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v7lqVTjy4E8c"
      },
      "outputs": [],
      "source": [
        "class MDP():\n",
        "    def __init__(self, horizon, grid_dim: int, device):\n",
        "        \"\"\" Markov Decision Problem class Initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        horizon: trjectory horizon for the problem\n",
        "        grid_dim: the number of squares per side of the grid\n",
        "        device: where to load tensors\n",
        "        \"\"\"\n",
        "        self.grid_dim = grid_dim\n",
        "        self.horizon = horizon\n",
        "        self.tgt_device = device\n",
        "        self.gamma = 0.99\n",
        "        self.goal = 1\n",
        "        self.path = 0\n",
        "        self.action_space = torch.tensor(\n",
        "            [(1, 0), (0, 1), (-1, 0), (0, -1), (0, 0)])\n",
        "        self.actions = len(self.action_space)\n",
        "\n",
        "        self.next_states = torch.zeros(\n",
        "            (3, self.actions, 2, grid_dim, grid_dim)).long().to(self.tgt_device)\n",
        "        self.next_inv = torch.zeros(\n",
        "            (self.actions, 2, grid_dim, grid_dim)).to(self.tgt_device)\n",
        "        self.prev_states = torch.zeros(\n",
        "            (3, self.actions, 2, grid_dim, grid_dim)).long().to(self.tgt_device)\n",
        "        self.prev_inv = torch.ones(\n",
        "            (self.actions, 2, grid_dim, grid_dim)).to(self.tgt_device)\n",
        "        for a in range(self.actions):\n",
        "            for t in range(2):\n",
        "                for x in range(grid_dim):\n",
        "                    for y in range(grid_dim):\n",
        "                        next = self.transition_function(\n",
        "                            torch.tensor([t, x, y]), a)\n",
        "                        prev = self.transition_function(\n",
        "                            torch.tensor([t, x, y]), a, True)\n",
        "                        if torch.all(next == -1):\n",
        "                            self.next_inv[a, t, x, y] = -math.inf\n",
        "                        else:\n",
        "                            self.next_states[:, a, t, x, y] = next\n",
        "                        if torch.all(prev == -1):\n",
        "                            self.prev_inv[a, t, x, y] = 0\n",
        "                        else:\n",
        "                            self.prev_states[:, a, t, x, y] = prev\n",
        "\n",
        "        self.next_states.requires_grad = False\n",
        "        self.next_inv.requires_grad = False\n",
        "        self.prev_states.requires_grad = False\n",
        "        self.prev_inv.requires_grad = False\n",
        "\n",
        "    def transition_function(self,\n",
        "                            state: torch.Tensor,\n",
        "                            action: int,\n",
        "                            reverse: bool = False) -> torch.Tensor:\n",
        "        \"\"\" transition function for MDP\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: a tuple containing information about the type of state and cell location\n",
        "        action: an action expressed as a string\n",
        "        reverse: whether if navigating the actions-states sequence backwards\n",
        "        \"\"\"\n",
        "        if state[0] == self.goal:\n",
        "            if reverse:\n",
        "                if action == self.actions-1:\n",
        "                    return torch.tensor([self.path, *state[1:]])\n",
        "            return torch.tensor([-1, -1, -1])\n",
        "        if action == self.actions-1:\n",
        "            if reverse:\n",
        "                return torch.tensor([-1, -1, -1])\n",
        "            else:\n",
        "                return torch.tensor([self.goal, *state[1:]])\n",
        "        if reverse:\n",
        "            new_state = state[1:]-self.action_space[action]\n",
        "        else:\n",
        "            new_state = state[1:]+self.action_space[action]\n",
        "\n",
        "        if 0 <= new_state[0] < self.grid_dim and 0 <= new_state[1] < self.grid_dim:\n",
        "            return torch.tensor([self.path, *new_state])\n",
        "        else:\n",
        "            return torch.tensor([-1, -1, -1])\n",
        "\n",
        "    def value_iteration(self, r_path: torch.Tensor, r_goal: torch.Tensor):\n",
        "        \"\"\" algorithm 3 from paper: computes policy based on reward\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        r_path: reward tensor for path states (shape [batch_size, 1, grid_dim, grid_dim])\n",
        "        r_goal: reward tensor for goal states (shape [batch_size, 1, grid_dim, grid_dim])\n",
        "        \"\"\"\n",
        "        r_path = r_path.squeeze(1)\n",
        "        r_goal = r_goal.squeeze(1)\n",
        "        r = torch.stack([r_path, r_goal], 1)\n",
        "        v = torch.zeros_like(r)\n",
        "        v[:, self.path] = -math.inf\n",
        "        v[:, self.goal] = r_goal\n",
        "\n",
        "        q = torch.zeros(\n",
        "            ((v.shape[0], self.actions) + v.shape[-3:])).to(self.tgt_device)\n",
        "        pi = torch.zeros(\n",
        "            ((v.shape[0], self.horizon) + q.shape[-4:])).to(self.tgt_device)\n",
        "\n",
        "        q.requires_grad = False\n",
        "        pi.requires_grad = False\n",
        "        v.requires_grad = False\n",
        "\n",
        "        for n in range(self.horizon-1, -1, -1):\n",
        "            q = r.unsqueeze(1)\\\n",
        "                + self.gamma * v[:, self.next_states[0], self.next_states[1], self.next_states[2]]\\\n",
        "                + self.next_inv\n",
        "            v = torch.logsumexp(q, 1)\n",
        "            v[:, self.goal] = r_goal\n",
        "            pi[:, n] = torch.exp(q-v.unsqueeze(1))\n",
        "        pi[torch.isnan(pi)] = 0\n",
        "\n",
        "        self.pi = pi\n",
        "        self.pi.requires_grad = False\n",
        "\n",
        "    def policy_propagation(self, initial_state):\n",
        "        \"\"\" algorithm 4 from paper: computes the state visitation frequency based on the current policy\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        initial_state: the starting block in the grid\n",
        "        \"\"\"\n",
        "        svf_t = torch.zeros(\n",
        "            (self.pi.shape[0], self.horizon, 2, self.grid_dim, self.grid_dim)).to(self.tgt_device)\n",
        "        svf_t.requires_grad = False\n",
        "        svf_t[:, 0, 0, initial_state[0], initial_state[1]] = 1\n",
        "\n",
        "        a_idcs = torch.arange(self.actions).repeat(\n",
        "            2, self.grid_dim, self.grid_dim, 1).permute(3, 0, 1, 2)\n",
        "\n",
        "        for n in range(1, self.horizon):\n",
        "            prev_pi = self.pi[:, n-1, a_idcs, self.prev_states[0],\n",
        "                              self.prev_states[1], self.prev_states[2]]\n",
        "            prev_svf = svf_t[:, n-1, self.prev_states[0],\n",
        "                             self.prev_states[1], self.prev_states[2]]\n",
        "            svf_t[:, n] = torch.sum(prev_pi*prev_svf*self.prev_inv, 1)\n",
        "\n",
        "        self.svf = torch.sum(svf_t, 1)\n",
        "        self.svf.requires_grad = False\n",
        "\n",
        "    def sample_policy(self, num_samples, initial_state, scene_tensor: torch.Tensor,\n",
        "                      agent_tensor: torch.Tensor, centers: torch.Tensor):\n",
        "        \"\"\" selects a batch of most probable policies based on scene and agent state\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_samples:    number of samples to choose from\n",
        "        initial_state:  the starting block in the grid\n",
        "        scene_tensor:   tensor containing data from the scene\n",
        "        agent_tensor:   tensor containing data from the agent\n",
        "        centers:        tensor to convert continuous position in positions in the grid\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        waypts:         sequence of blocks visited using the sampled policies\n",
        "        scene_feats:    scene features extracted\n",
        "        agent_feats:    agent features extracted\n",
        "        \"\"\"\n",
        "\n",
        "        pi = self.pi.permute(0, 1, 3, 4, 5, 2)\n",
        "        pi[:, :, 1, :, :, -1] = 1\n",
        "        next_states = self.next_states.permute(0, 2, 3, 4, 1)\n",
        "        next_states[:, 1, :, :, -1] = self.next_states[:, -1, 0]\n",
        "\n",
        "        pi = pi.to(self.tgt_device)\n",
        "        next_states = next_states.to(self.tgt_device)\n",
        "        batch_size = pi.shape[0]\n",
        "        grid_idcs = torch.zeros(batch_size * num_samples,\n",
        "                                self.horizon, 2).to(self.tgt_device)\n",
        "        waypts = torch.zeros_like(grid_idcs).to(self.tgt_device)\n",
        "        s = torch.tensor([0, *initial_state]).unsqueeze(1).repeat(1,\n",
        "                                                                  batch_size * num_samples).long().to(self.tgt_device)\n",
        "\n",
        "        for n in range(self.horizon):\n",
        "            # Populate grid_idcs and waypts\n",
        "            t = s[0]\n",
        "            r = s[1]\n",
        "            c = s[2]\n",
        "            r = r * (1 - t)\n",
        "            c = c * (1 - t)\n",
        "\n",
        "            waypts[:, n, 0] = (centers[1, r.long()] * (1 - t))\n",
        "            waypts[:, n, 1] = (centers[0, c.long()] * (1 - t))\n",
        "            grid_idcs[:, n, 0] = r\n",
        "            grid_idcs[:, n, 1] = c\n",
        "\n",
        "            # Sample next actions\n",
        "            idcs = torch.tensor(range(batch_size)).unsqueeze(0).repeat(\n",
        "                num_samples, 1).permute(1, 0).reshape(-1).to(self.tgt_device)\n",
        "            pi_s = pi[idcs, n, s[0], s[1], s[2]]\n",
        "            a = Categorical(pi_s).sample()\n",
        "\n",
        "            # Obtain next states\n",
        "            s = next_states[:, s[0], s[1], s[2], a]\n",
        "\n",
        "        scene_tensor = scene_tensor.unsqueeze(0).repeat(\n",
        "            num_samples, 1, 1, 1, 1).permute(1, 0, 2, 3, 4)\n",
        "        scene_tensor = scene_tensor.reshape(\n",
        "            -1, scene_tensor.shape[2], scene_tensor.shape[3], scene_tensor.shape[4])\n",
        "        agent_tensor = agent_tensor.unsqueeze(0).repeat(\n",
        "            num_samples, 1, 1, 1, 1).permute(1, 0, 2, 3, 4)\n",
        "        agent_tensor = agent_tensor.reshape(\n",
        "            -1, agent_tensor.shape[2], agent_tensor.shape[3], agent_tensor.shape[4])\n",
        "        scene_feats, agent_feats = nuScenesAdvanced.extract_plan_features(\n",
        "            grid_idcs, scene_tensor, agent_tensor)\n",
        "        scene_feats = scene_feats.reshape(\n",
        "            batch_size, num_samples, self.horizon, scene_feats.shape[2])\n",
        "        agent_feats = agent_feats.reshape(\n",
        "            batch_size, num_samples, self.horizon, agent_feats.shape[2])\n",
        "        waypts = waypts.reshape(batch_size, num_samples, self.horizon, 2)\n",
        "\n",
        "        return waypts, scene_feats, agent_feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PKluuhRd4E8d"
      },
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\" Model to extract rewards for Max-Ent RL\"\"\"\n",
        "\n",
        "    def __init__(self, mdp: MDP):\n",
        "        \"\"\" Reward model initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mdp: the Markov decision problem instance that belongsto the model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        backbone = resnet34(pretrained=True)\n",
        "        self.log_sig = nn.LogSigmoid()\n",
        "\n",
        "        self.cnn_feat = nn.Sequential(\n",
        "            backbone.conv1, backbone.bn1, backbone.relu, backbone.maxpool, backbone.layer1)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
        "        self.cnn_1 = nn.Conv2d(\n",
        "            in_channels=32+3, out_channels=32, kernel_size=1)\n",
        "        self.cnn_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=1)\n",
        "\n",
        "        self.cnn_out = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
        "\n",
        "        self.cnn_p = nn.Sequential(self.cnn_1, self.cnn_out)\n",
        "        self.cnn_g = copy.deepcopy(self.cnn_p)\n",
        "\n",
        "        self.y = torch.linspace(40 - 50/(25*2),\n",
        "                                -10 + 50/(25*2),\n",
        "                                25).reshape(-1, 1).repeat_interleave(25, 1)\n",
        "        self.x = torch.linspace(-25 + 50/(25*2),\n",
        "                                25 - 50/(25*2),\n",
        "                                25).reshape(-1, 1).repeat_interleave(25, 1).transpose(0, 1)\n",
        "        self.motion_feats = torch.zeros((3, 25, 25))\n",
        "\n",
        "        self.mdp = mdp\n",
        "        self.initial_state = np.random.randint(25, size=2)\n",
        "        self.loss = 0\n",
        "        self.activation = {}\n",
        "\n",
        "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Network inference \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: the input tensor (raster image)\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        r_path: computed reward for path states\n",
        "        r_goal: computed reward for goal states\n",
        "        img_feats: intermediate result (features extracted from the image)\n",
        "        \"\"\"\n",
        "        img = x\n",
        "        motion_feats = self.motion_feats\n",
        "\n",
        "        img_feats = self.cnn_feat(img)\n",
        "        img_feats = self.conv1(img_feats)\n",
        "\n",
        "        x = torch.cat([img_feats, motion_feats], 1)\n",
        "        r_path = self.log_sig(self.cnn_p(x))\n",
        "        r_goal = self.log_sig(self.cnn_g(x))\n",
        "        return r_path, r_goal, img_feats\n",
        "\n",
        "    def get_activation(self, name):\n",
        "        \"\"\" Obtain intermediate layer output from the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        name: the name of the layer to extract\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        hook: function to be used to extract the layer's output\n",
        "        \"\"\"\n",
        "        def hook(model, input, output):\n",
        "            self.activation[name] = output.detach().cpu()\n",
        "        return hook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "brLIzHPl4E8i"
      },
      "outputs": [],
      "source": [
        "class Trajectory_Generator(nn.Module):\n",
        "    \"\"\" Trajectory Generator class \"\"\"\n",
        "    def __init__(self, traj_device, pretrain=True, traj_hidden_size=TRAJ_HIDDEN_SIZE, plan_hidden_size=PLAN_HIDDEN_SIZE,\n",
        "                 att_hidden_size=ATT_HIDDEN_SIZE, pos_embedding_size=POS_EMBEDDING_SIZE, \n",
        "                 scene_embedding_size=SCENE_EMBEDDING_SIZE, agent_embedding_size=AGENT_EMBEDDING_SIZE,\n",
        "                 scene_features_size=SCENE_FEATURES_SIZE, agent_features_size=AGENT_FEATURES_SIZE, \n",
        "                 dyn_features_size=DYN_FEATURES_SIZE, slope=ACTIVATION_SLOPE,\n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON, lr=TRAJ_GEN_LR_PRE):\n",
        "        \"\"\" Trajectory Generator initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        traj_device: device of the trajectory tensor in the attention decoder\n",
        "        pretrain: True if the model should be first pretrained to speed up convergence\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        att_hidden_size: size of the hidden layer of the attention part in the final decoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding \n",
        "        scene_embedding_size: size of the linear layer for the scene embedding\n",
        "        agent_embedding_size: size of the linear layer for the agent embedding\n",
        "        scene_features_size: size of the scene features at each grid location\n",
        "        agent_features_size: size of the agent features at each grid location\n",
        "        dynamic_features_size: additional motion features to add to the embedding layer\n",
        "            0 -> x, y\n",
        "            1 -> x, y, velocity\n",
        "            2 -> x, y, velocity, acceleration\n",
        "            3 -> x, y, velocity, acceleration, yaw rate\n",
        "        slope: slope (positive or negative) of the activation function\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        super(Trajectory_Generator, self).__init__()\n",
        "        self.scene_feat_size = scene_features_size\n",
        "        self.agent_feat_size = agent_features_size\n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "        # ---------------------------------------| Generator Structure |-------------------------------------- # \n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "        self.motion_encoder = MotionEncoder(\n",
        "            traj_hidden_size, pos_embedding_size, \n",
        "            dyn_features_size, slope\n",
        "        )\n",
        "        self.plan_encoder = PlanEncoder(\n",
        "            plan_hidden_size, pos_embedding_size, \n",
        "            scene_embedding_size, agent_embedding_size,\n",
        "            scene_features_size, agent_features_size, slope\n",
        "        )\n",
        "        self.att_decoder = AttentionDecoder(\n",
        "            traj_device, att_hidden_size, traj_hidden_size, plan_hidden_size, traj_samples\n",
        "        )\n",
        "        # ---------------------------------------------------------------------------------------------------- #\n",
        "\n",
        "    def forward(self, hist_motion, plan, scene_feats, agent_feats):\n",
        "        \"\"\" Trajectory Generator inference\n",
        "        Input:\n",
        "            - hist_motion: history motion tensor\n",
        "            - plan: tensor of waypoints\n",
        "            - scene_feats: scene features\n",
        "            - agent_feats: agent features\n",
        "        Output: trajectory -> decoded trajectory tensor\n",
        "        \"\"\"\n",
        "        enc_motion = self.motion_encoder(hist_motion)\n",
        "        enc_plan = self.plan_encoder(plan, scene_feats, agent_feats)\n",
        "        trajectory = self.att_decoder(hist_motion, enc_motion, enc_plan)\n",
        "        return trajectory\n",
        "\n",
        "\n",
        "class MotionEncoder(nn.Module):\n",
        "    \"\"\" Motion Encoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, traj_hidden_size, pos_embedding_size, dyn_features_size, slope=ACTIVATION_SLOPE):\n",
        "        \"\"\" Motion Encoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding (x-y)\n",
        "        dyn_features_size: additional motion features to add to the embedding layer\n",
        "            0 -> x, y\n",
        "            1 -> x, y, velocity\n",
        "            2 -> x, y, velocity, acceleration\n",
        "            3 -> x, y, velocity, acceleration, yaw rate\n",
        "        slope: slope (positive or negative) of the activation function \n",
        "        \"\"\"\n",
        "        super(MotionEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(2+dyn_features_size, pos_embedding_size)\n",
        "        self.activation = nn.LeakyReLU(slope)\n",
        "        self.encoder = nn.GRU(pos_embedding_size, traj_hidden_size)\n",
        "\n",
        "    def forward(self, hist_motion) -> torch.Tensor:\n",
        "        \"\"\" Motion Encoder inference \n",
        "\n",
        "        Input: hist_motion -> history motion tensor\n",
        "        Output: enc_motion -> encoded motion tensor\n",
        "        \"\"\"\n",
        "        emb_features = self.activation(self.embedding(hist_motion))\n",
        "        output, enc_motion = self.encoder(emb_features)\n",
        "        return enc_motion\n",
        "\n",
        "\n",
        "class PlanEncoder(nn.Module):\n",
        "    \"\"\" Plan Encoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, plan_hidden_size, \n",
        "                 pos_embedding_size, scene_embedding_size, agent_embedding_size, \n",
        "                 scene_features_size, agent_features_size, slope=ACTIVATION_SLOPE):\n",
        "        \"\"\" Plan Encoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        pos_embedding_size: size of the linear layer for the position embedding \n",
        "        scene_embedding_size: size of the linear layer for the scene embedding\n",
        "        agent_embedding_size: size of the linear layer for the agent embedding\n",
        "        scene_features_size: size of the scene features at each grid location\n",
        "        agent_features_size: size of the agent features at each grid location\n",
        "        slope: slope (positive or negative) of the activation function \n",
        "        \"\"\"\n",
        "        super(PlanEncoder, self).__init__()\n",
        "        self.pos_embedding = nn.Linear(2, pos_embedding_size)\n",
        "        self.scene_embedding = nn.Linear(scene_features_size, scene_embedding_size)\n",
        "        self.agent_embedding = nn.Linear(agent_features_size, agent_embedding_size)\n",
        "        self.activation = nn.LeakyReLU(slope)\n",
        "        self.encoder = nn.GRU(\n",
        "            pos_embedding_size + scene_embedding_size + agent_embedding_size,\n",
        "            plan_hidden_size, bidirectional=True)\n",
        "    \n",
        "    def forward(self, plan, scene_feats, agent_feats) -> torch.Tensor:\n",
        "        \"\"\" Plan Encoder inference\n",
        "        Input:\n",
        "            - plan: tensor of waypoints\n",
        "            - scene_feats: scene features\n",
        "            - agent_feats: agent features\n",
        "        Output: enc_plan -> encoded plan tensot\n",
        "        \"\"\"\n",
        "        # Embedding\n",
        "        emb_features = self.activation(torch.cat((\n",
        "            self.pos_embedding(plan),\n",
        "            self.scene_embedding(scene_feats),\n",
        "            self.agent_embedding(agent_feats)),\n",
        "            dim=2\n",
        "        ))\n",
        "        # Reorganizing plans\n",
        "        plan_sum = torch.sum(torch.abs(plan), dim=2)\n",
        "        plan_lengths = torch.sum(plan_sum[1:, :]!=0, dim=0) + 1\n",
        "        plan_lengths_sorted, indices = torch.sort(plan_lengths, descending=True)\n",
        "        # Reorganizing embeddings\n",
        "        emb_packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            emb_features[:, indices, :], plan_lengths_sorted.cpu(), batch_first=False)\n",
        "        # Encoding\n",
        "        enc_plan_packed, output = self.encoder(emb_packed)\n",
        "        enc_plan_unpacked, _ = nn.utils.rnn.pad_packed_sequence(enc_plan_packed)\n",
        "        enc_plan = enc_plan_unpacked[:, indices.sort()[1], :]\n",
        "        return enc_plan\n",
        "\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\" Attention Decoder class for Trajectory Generator \"\"\"\n",
        "    def __init__(self, traj_device, att_hidden_size, traj_hidden_size, plan_hidden_size, \n",
        "                 traj_samples=SAMPLES_PER_SECOND*TRAJ_HORIZON):\n",
        "        \"\"\" Attention Decoder initialization\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        traj_device: device of the trajectory tensor\n",
        "        att_hidden_size: size of the hidden layer of the attention part in the final decoder\n",
        "        traj_hidden_size: size of the hidden layer of the GRU trajectory encoder/decoder\n",
        "        plan_hidden_size: size of the hidden layer of the GRU plan encoder\n",
        "        traj_samples: number of samples to consider in the trajectory\n",
        "        \"\"\"\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.traj_device = traj_device\n",
        "        self.traj_samples = traj_samples\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(2*plan_hidden_size + traj_hidden_size, att_hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(att_hidden_size, 1),\n",
        "            nn.Softmax(dim=0)\n",
        "        )\n",
        "        self.dec_state_op = nn.Linear(traj_hidden_size, 2)\n",
        "        self.decoder = nn.GRUCell(2*plan_hidden_size, traj_hidden_size)\n",
        "\n",
        "    def forward(self, hist_motion, enc_motion, enc_plan) -> torch.Tensor:\n",
        "        \"\"\" Attention Decoder inference\n",
        "        \n",
        "        Input: \n",
        "            - hist_motion: history motion tensor\n",
        "            - enc_motion: encoded motion tensor\n",
        "            - enc_plan: encoded plan tensor\n",
        "        Output: dec_traj -> decoded trajectory tensor\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        dec_traj = torch.empty(\n",
        "            (self.traj_samples, hist_motion.shape[1], 2), \n",
        "            dtype=torch.float32).to(self.traj_device)\n",
        "        motion = enc_motion.squeeze()\n",
        "        # Attention loop\n",
        "        for s in range(self.traj_samples):\n",
        "            att_input = torch.cat((motion.repeat(enc_plan.shape[0], 1, 1), enc_plan), dim=2)\n",
        "            att_features = self.attention(att_input)\n",
        "            dec_input = (att_features.repeat(1, 1, enc_plan.shape[2])*enc_plan).sum(dim=0)\n",
        "            motion = self.decoder(dec_input, motion)\n",
        "            dec_traj[s] = self.dec_state_op(motion)\n",
        "        return dec_traj.permute(1, 0, 2)\n",
        "\n",
        "\n",
        "# NOTE: the presence of no clusters can be handled with np.inf\n",
        "class TrajGenLoss(nn.Module):\n",
        "    \"\"\" Loss for the Trajectory Generator training \"\"\"\n",
        "    def __init__(self, phase='train', loss_fun=None):\n",
        "        \"\"\" Trajectory Generator loss initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase: 'pretrain' or 'train'\n",
        "        loss_fun: function to compute loss; available ['min_ade_k']\n",
        "        \"\"\"\n",
        "        super(TrajGenLoss, self).__init__()\n",
        "        if phase == 'pretrain':\n",
        "            self.loss_fun = nn.MSELoss()\n",
        "            if loss_fun is not None:\n",
        "                warnings.warn(\"In pretrain phase only MSELoss is available\")\n",
        "        else:\n",
        "            if loss_fun == None or loss_fun == 'min_ade_k':\n",
        "                self.loss_fun = self.min_ade_k\n",
        "            else:\n",
        "                raise ValueError(\"Wrong argument loss_fun: available 'None' or 'min_ade_k\")\n",
        "\n",
        "    def min_ade_k(self, pred, gt):\n",
        "        \"\"\" Min_Ade_K loss for the Trajectory Generator \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        pred: clustered trajectories predicted\n",
        "        gt: ground truth\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        min_ade: mean over the batch of the min ade loss\n",
        "        \"\"\"\n",
        "        diff = torch.sub(gt.unsqueeze(1), pred)\n",
        "        norm = torch.sqrt(torch.sum(diff ** 2, 3))\n",
        "        ade = torch.mean(norm, 2)\n",
        "        min_ade, _ = torch.min(ade, 1)\n",
        "        return min_ade.mean()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\" Trajectory Generator loss computation \"\"\"\n",
        "        return self.loss_fun(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y-WPvNEKpFWm"
      },
      "outputs": [],
      "source": [
        "class P2T(nn.Module):\n",
        "    \"\"\" P2T Model for trajectory prediction \"\"\"\n",
        "\n",
        "    def __init__(self, mdp: MDP, device, centers, \n",
        "                 mdp_horizon=MDP_HORIZON, initial_state=INITIAL_STATE,\n",
        "                 policy_samples=POLICY_SAMPLES, traj_clusters=TRAJ_CLUSTERS,\n",
        "                 from_checkpoint=[], checkpoint_period=CHECKPOINT_PERIOD,\n",
        "                 checkpoint_dir=CHECKPOINT_DIR, checkpoint_method=CHECKPOINT_METHOD, \n",
        "                 clear_period=CLEAR_PERIOD, print_period=PRINT_PERIOD, pretrain_traj_gen=True,\n",
        "                 train_rm_epoches=TRAIN_RM_EPOCHES, pretrain_tg_epoches=PRETRAIN_TG_EPOCHES,\n",
        "                 train_tg_epoches=TRAIN_TG_EPOCHES, reward_model_lr=REWARD_MODEL_LR,\n",
        "                 traj_gen_lr_pre=TRAJ_GEN_LR_PRE, traj_gen_lr=TRAJ_GEN_LR, rm_logname=RM_LOGNAME,\n",
        "                 pre_tg_logname=PRE_TG_LOGNAME, ft_tg_logname=FT_TG_LOGNAME, max_norm=MAX_CLIP_NORM,\n",
        "                 grid_extent=GRID_EXTENT, horizon_folder=HORIZON_FOLDER):\n",
        "        \"\"\" P2T Model initialization\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mdp: Markov Decision Process class instance\n",
        "        device: device of execution [e.g. CUDA:0]\n",
        "        centers: row centers and column centers, needed for the inference\n",
        "        mdp_horizon: horizon (in seconds) of the MDP\n",
        "        initial_state: initial state of the agent\n",
        "        policy_samples: how many samples to extract from the policy\n",
        "        traj_clusters: num of trajectory clusters\n",
        "        from_checkpoint: options -> [], ['rm'], ['rm', 'tg_pre'], ['rm', 'tg']\n",
        "        checkpoint_period: saving checkpoints period\n",
        "        checkpoint_dir: directory to save checkpoints\n",
        "        checkpoint_method: method for checkpoint saving (e.g. 'last', 'best')\n",
        "        clear_period: clear output period\n",
        "        print_period: print loss values period\n",
        "        pretrain_traj_gen: True if the Trajectory Generator should be pretrained\n",
        "        train_rm_epoches: # epoches to train the Reward Model\n",
        "        pretrain_tg_epoches: # epoches to pre-train the Trajectory Generator\n",
        "        train_tg_epoches: # epoches to train (fine-tune) the Trajectory Generator\n",
        "        reward_model_lr: learning rate of the Reward Model\n",
        "        traj_gen_lr_pre: learning rate of the Trajectory Generator (pre-train)\n",
        "        traj_gen_lr: learning rate of the Trajectory Generator (fine-tune)\n",
        "        max_norm: maximum norm to clip gradients\n",
        "        grid_extent: (convenience parameter) grid extension\n",
        "        horizon_folder: (convenience parameter) horizon folder for checkpoints\n",
        "        \"\"\"\n",
        "        # Basic initialization\n",
        "        super(P2T, self).__init__()\n",
        "        # self.time = {}\n",
        "        # self.time[\"training\"] = 0\n",
        "        # self.time[\"training:cpu\"] = 0\n",
        "        # self.time[\"training:inference\"] = 0\n",
        "        # self.time[\"inference:reward_model\"] = 0\n",
        "        # self.time[\"inference:value_iteration\"] = 0\n",
        "        # self.time[\"inference:policy_propagation\"] = 0\n",
        "        # self.time[\"inference:zero_grad\"] = 0\n",
        "        # self.time[\"inference:backward\"] = 0\n",
        "        # self.time[\"inference:step\"] = 0\n",
        "        # self.time[\"training:logging\"] = 0\n",
        "        self.mdp = mdp\n",
        "        self.tgt_device = device\n",
        "        self.mdp_horizon = mdp_horizon\n",
        "        self.initial_state = initial_state\n",
        "        self.policy_samples = policy_samples\n",
        "        self.from_checkpoint = from_checkpoint\n",
        "        self.traj_clusters = traj_clusters\n",
        "        self.checkpoint_period = checkpoint_period\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.checkpoint_method = checkpoint_method\n",
        "        self.clear_period = clear_period\n",
        "        self.print_period = print_period\n",
        "        self.rm_logger = SummaryWriter(comment =f'_{rm_logname}')\n",
        "        self.pre_tg_logger = SummaryWriter(comment = f'_{pre_tg_logname}')\n",
        "        self.ft_tg_logger = SummaryWriter(comment = f'_{ft_tg_logname}')\n",
        "        self.pretrain_traj_gen = pretrain_traj_gen\n",
        "        self.pretrain_tg_epoches = pretrain_tg_epoches\n",
        "        self.train_rm_epoches = train_rm_epoches\n",
        "        self.train_tg_epoches = train_tg_epoches\n",
        "        self.reward_model_lr = reward_model_lr\n",
        "        self.traj_gen_lr_pre = traj_gen_lr_pre\n",
        "        self.traj_gen_lr = traj_gen_lr\n",
        "        self.max_norm = max_norm\n",
        "\n",
        "        # Model initialization\n",
        "        self.reward_model = RewardModel(mdp).to(device)\n",
        "        self.traj_generator = Trajectory_Generator(device, pretrain_traj_gen).to(device)\n",
        "\n",
        "        # Convenience initialization\n",
        "        self.grid_extent = grid_extent\n",
        "        self.centers = centers.to(self.tgt_device)\n",
        "        self.horizon_folder = horizon_folder\n",
        "\n",
        "        # Checkpoint loading\n",
        "        self.rm_checkpoint = None\n",
        "        self.tg_checkpoint = None\n",
        "        self.from_checkpoint = from_checkpoint\n",
        "        if 'rm' in from_checkpoint:\n",
        "            print(\"Reward Model checkpoint loading ...\")\n",
        "            checkpoint_path = \\\n",
        "                os.path.join(self.checkpoint_dir, 'P2T', self.horizon_folder, 'reward_model')\n",
        "            self.rm_checkpoint = self.select_checkpoint(checkpoint_path, checkpoint_method)\n",
        "            if self.rm_checkpoint is not None:\n",
        "                self.reward_model.load_state_dict(self.rm_checkpoint['reward_model_state'])\n",
        "                print(\"Reward Model checkpoint loaded from epoch\", self.rm_checkpoint['epoch'], \"with loss\", self.rm_checkpoint['loss'])\n",
        "        if 'tg_pre' in from_checkpoint or 'tg' in from_checkpoint:\n",
        "            print(\"Trajectory Generator checkpoint loading ...\")\n",
        "            if 'tg_pre' in from_checkpoint:\n",
        "                checkpoint_path = \\\n",
        "                    os.path.join(self.checkpoint_dir, 'P2T', self.horizon_folder, 'traj_generator', 'pretrain')\n",
        "            else:\n",
        "                checkpoint_path = \\\n",
        "                    os.path.join(self.checkpoint_dir, 'P2T', self.horizon_folder, 'traj_generator', 'finetune')\n",
        "            self.tg_checkpoint = self.select_checkpoint(checkpoint_path, checkpoint_method)\n",
        "            if self.tg_checkpoint is not None:\n",
        "                self.traj_generator.load_state_dict(self.tg_checkpoint['traj_generator_state'])\n",
        "                print(\"Trajectory Generator checkpoint loaded from epoch\", self.tg_checkpoint['epoch'], \"with loss\", self.tg_checkpoint['loss'])    \n",
        "\n",
        "    def train(self, trainval_dm: nuScenesDataModule):\n",
        "        \"\"\" P2T function for training all components \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        # Train initialization\n",
        "        self.horizon_folder = trainval_dm.train_dataset.horizon_folder\n",
        "\n",
        "        # Train starting\n",
        "        print(\"Starting P2T training\")\n",
        "        start = time.time()\n",
        "        self.train_reward_model(trainval_dm)\n",
        "        self.train_traj_generator(trainval_dm)\n",
        "        print(\"P2T Model trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    def train_reward_model(self, trainval_dm: nuScenesDataModule):\n",
        "        \"\"\" Reward Model training \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        print(\"Starting P2T Reward Model training\")\n",
        "        start = time.time()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.reward_model.parameters(), lr=self.reward_model_lr)\n",
        "        train_dataloader = trainval_dm.train_dataloader()\n",
        "        val_dataloader = trainval_dm.val_dataloader()\n",
        "        start_epoch = 0\n",
        "\n",
        "        if self.rm_checkpoint is not None:\n",
        "            print(\"Reward Model optimizer checkpoint loading ...\")\n",
        "            optimizer.load_state_dict(self.rm_checkpoint['optimizer_state'])\n",
        "            start_epoch = self.rm_checkpoint['epoch'] + 1\n",
        "        if ((start_epoch == self.train_rm_epoches) or\n",
        "            ((self.checkpoint_method == 'best') and len(self.from_checkpoint) > 1)):\n",
        "            print(\"Reward Model has already been trained completely\")\n",
        "            return\n",
        "\n",
        "        with tqdm(total=len(train_dataloader)) as pbar:\n",
        "            for e in range(start_epoch, self.train_rm_epoches):\n",
        "                pbar.set_description(f\"[RM] Epoch {e} - training\")\n",
        "                pbar.reset(len(train_dataloader))\n",
        "                # self.time[\"training\"] -= time.time()\n",
        "                # self.time[\"training:cpu\"] -= time.thread_time()\n",
        "\n",
        "                # Train\n",
        "                self.reward_model.train()\n",
        "                train_iter = iter(train_dataloader)\n",
        "                (_, next_x_img_static, _, _, _, _, _, next_svf_e, _, _, next_motion_feats) = train_iter.next()\n",
        "                pbar.update(1)\n",
        "                next_svf_e = next_svf_e.float().to(self.tgt_device, non_blocking=True)\n",
        "                next_x_img_static = next_x_img_static.float().to(self.tgt_device, non_blocking=True)\n",
        "                next_motion_feats = next_motion_feats.float().to(self.tgt_device, non_blocking=True)\n",
        "                for b, data in enumerate(train_dataloader, start=1):\n",
        "                # for b in range(len(train_dataloader)):\n",
        "                    svf_e = next_svf_e\n",
        "                    x_img_static = next_x_img_static\n",
        "                    motion_feats = next_motion_feats\n",
        "                    if b < len(train_dataloader):\n",
        "                        (_, next_x_img_static, _, _, _, _, _, next_svf_e, _, _, next_motion_feats) = data\n",
        "                        next_svf_e = next_svf_e.float().to(self.tgt_device, non_blocking=True)\n",
        "                        next_x_img_static = next_x_img_static.float().to(self.tgt_device, non_blocking=True)\n",
        "                        next_motion_feats = next_motion_feats.float().to(self.tgt_device, non_blocking=True)\n",
        "                    # (_, x_img_static, _, _, _, _, _, svf_e, _, _, motion_feats) = data\n",
        "                    # svf_e = svf_e.float().to(self.tgt_device, non_blocking=True)\n",
        "                    # x_img_static = x_img_static.float().to(self.tgt_device, non_blocking=True)\n",
        "                    # motion_feats = motion_feats.float().to(self.tgt_device, non_blocking=True)\n",
        "                    # self.time[\"training:inference\"] -= time.time()\n",
        "\n",
        "                    self.reward_model.motion_feats = motion_feats\n",
        "                    \n",
        "                    # self.time[\"inference:reward_model\"] -= time.time()\n",
        "                    r_path, r_goal, _ = self.reward_model(x_img_static)\n",
        "                    # self.time[\"inference:reward_model\"] += time.time()\n",
        "\n",
        "                    # self.time[\"inference:value_iteration\"] -= time.time()\n",
        "                    self.mdp.value_iteration(r_path.detach(), r_goal.detach())\n",
        "                    # self.time[\"inference:value_iteration\"] += time.time()\n",
        "\n",
        "                    # self.time[\"inference:policy_propagation\"] -= time.time()\n",
        "                    self.mdp.policy_propagation(self.initial_state)\n",
        "                    # self.time[\"inference:policy_propagation\"] += time.time()\n",
        "\n",
        "                    svf = self.mdp.svf.to(self.tgt_device)\n",
        "\n",
        "                    # svf_grad = svf_e - svf\n",
        "                    svf_grad = svf - svf_e \n",
        "\n",
        "                    # self.time[\"inference:zero_grad\"] -= time.time()\n",
        "                    optimizer.zero_grad()\n",
        "                    # self.time[\"inference:zero_grad\"] += time.time()\n",
        "                    # self.time[\"inference:backward\"] -= time.time()\n",
        "                    torch.autograd.backward(torch.cat([r_path, r_goal],1), svf_grad)\n",
        "                    # self.time[\"inference:backward\"] += time.time()\n",
        "                    a = torch.nn.utils.clip_grad_norm_(self.reward_model.parameters(), self.max_norm)\n",
        "                    # self.time[\"inference:step\"] -= time.time()\n",
        "                    optimizer.step()\n",
        "                    # self.time[\"inference:step\"] += time.time()\n",
        "                    # self.time[\"training:inference\"] += time.time()\n",
        "\n",
        "                    iterations = e*len(train_dataloader) + b\n",
        "                    # Logging\n",
        "\n",
        "                    # self.time[\"training:logging\"] -= time.time()\n",
        "                    if b % 25 == 0:\n",
        "                        with plt.ioff():\n",
        "                            f, a = plt.subplots(8, 7, figsize=(20, 20))\n",
        "                            a[0, 0].set_title('original image', fontsize=16)\n",
        "                            a[0, 1].set_title('path reward', fontsize=16)\n",
        "                            a[0, 2].set_title('goal reward', fontsize=16)\n",
        "                            a[0, 3].set_title('learned path svf', fontsize=16)\n",
        "                            a[0, 4].set_title('learned goal svf', fontsize=16)\n",
        "                            a[0, 5].set_title('true path svf', fontsize=16)\n",
        "                            a[0, 6].set_title('true goal svf', fontsize=16)\n",
        "                            for i in range(8):\n",
        "                                a[i, 0].imshow(torch.clip(x_img_static[i].detach().cpu().permute(1,2,0), 0, 1))\n",
        "                                a[i, 1].imshow(r_path[i].squeeze(1).detach().cpu(), cmap='viridis')\n",
        "                                a[i, 2].imshow(r_goal[i].squeeze(1).detach().cpu(), cmap='viridis')\n",
        "                                a[i, 3].imshow(svf[i, 0].detach().cpu(), cmap='viridis')\n",
        "                                a[i, 4].imshow(svf[i, 1].detach().cpu(), cmap='viridis')\n",
        "                                a[i, 5].imshow(svf_e[i, 0].detach().cpu(), cmap='viridis')\n",
        "                                a[i, 6].imshow(svf_e[i, 1].detach().cpu(), cmap='viridis')\n",
        "                            \n",
        "                            self.rm_logger.add_figure('reward model overview', f, iterations)\n",
        "\n",
        "                    self.rm_logger.add_scalar('svf grad path', torch.mean(torch.abs(svf_grad[:,0])).item(), iterations)\n",
        "                    self.rm_logger.add_scalar('svf grad goal', torch.mean(torch.abs(svf_grad[:,1])).item(), iterations)\n",
        "                    # self.time[\"training:logging\"] += time.time()\n",
        "                    # tqdm bar update\n",
        "                    pbar_desc = f\"[RM] Epoch {e} - training - svf grad path = {torch.mean(torch.abs(svf_grad[:,0])).item():.2E}, \"\n",
        "                    pbar_desc += f\"svf grad goal = {torch.mean(torch.abs(svf_grad[:,1])).item():.2E}\"\n",
        "                    pbar.set_description(pbar_desc)\n",
        "                    pbar.update(1)\n",
        "                # Val\n",
        "                val_losses = []\n",
        "                self.reward_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    svf_grad_path_sum = 0\n",
        "                    svf_grad_goal_sum = 0\n",
        "                    val_batch_count = 0\n",
        "\n",
        "                    pbar.reset(len(val_dataloader))\n",
        "                    pbar.set_description(f\"[RM] Epoch {e} - validation\")\n",
        "\n",
        "                    for b, data in enumerate(val_dataloader):\n",
        "                        (_, x_img_static, _, _, _, _, _, svf_e, _, _, motion_feats) = data\n",
        "                        svf_e = svf_e.float().to(self.tgt_device)\n",
        "                        x_img_static = x_img_static.float().to(self.tgt_device)\n",
        "                        motion_feats = motion_feats.float().to(self.tgt_device)\n",
        "\n",
        "                        # self.time[\"training:inference\"] -= time.time()\n",
        "                        self.reward_model.motion_feats = motion_feats\n",
        "\n",
        "                        # self.time[\"inference:reward_model\"] -= time.time()\n",
        "                        r_path, r_goal, _ = self.reward_model(x_img_static)\n",
        "                        # self.time[\"inference:reward_model\"] += time.time()\n",
        "\n",
        "                        # self.time[\"inference:value_iteration\"] -= time.time()\n",
        "                        self.mdp.value_iteration(r_path.detach(), r_goal.detach())\n",
        "                        # self.time[\"inference:value_iteration\"] += time.time()\n",
        "                        # self.time[\"inference:policy_propagation\"] -= time.time()\n",
        "                        self.mdp.policy_propagation(self.initial_state)\n",
        "                        # self.time[\"inference:policy_propagation\"] += time.time()\n",
        "\n",
        "                        svf = self.mdp.svf.to(self.tgt_device)\n",
        "\n",
        "                        # svf_grad = svf_e - svf\n",
        "                        svf_grad = svf - svf_e\n",
        "\n",
        "                        svf_grad_path_sum += torch.mean(torch.abs(svf_grad[:,0])).item()\n",
        "                        svf_grad_goal_sum += torch.mean(torch.abs(svf_grad[:,1])).item()\n",
        "                        val_batch_count += 1\n",
        "\n",
        "                        val_loss = svf_grad_path_sum / val_batch_count\n",
        "                        val_losses.append(val_loss)\n",
        "\n",
        "                        # self.time[\"training:inference\"] += time.time()\n",
        "\n",
        "                        # tqdm bar update\n",
        "                        pbar.set_description(\n",
        "                            f\"[RM] Epoch {e} - validation - val loss = {val_loss:.2E}\")\n",
        "                        pbar.update(1)\n",
        "\n",
        "\n",
        "                    mean_loss = np.array(val_losses).mean()\n",
        "                    self.rm_logger.add_scalar('val loss', mean_loss, e)\n",
        "\n",
        "                    # Save weights\n",
        "                    if (e+1) % self.checkpoint_period == 0:\n",
        "                        print(\"Saving weights ...\")\n",
        "                        torch.save({\n",
        "                            'epoch': e,\n",
        "                            'reward_model_state': self.reward_model.state_dict(),\n",
        "                            'optimizer_state': optimizer.state_dict(),\n",
        "                            'loss': mean_loss\n",
        "                        }, os.path.join(self.checkpoint_dir, 'P2T', \n",
        "                                        self.horizon_folder, 'reward_model', f'{e}.pt'))\n",
        "                print(f\"[RM] val loss for epoch {e} = {mean_loss}\")\n",
        "\n",
        "                # Clear output periodically\n",
        "                if (e+1) % self.clear_period == 0:\n",
        "                    print(\"Output clearing ...\")\n",
        "                    time.sleep(1)\n",
        "                    clear_output()\n",
        "\n",
        "                # self.time[\"training\"] += time.time()\n",
        "                # self.time[\"training:cpu\"] += time.thread_time()\n",
        "                # trainval_dm.train_dataset.time.update(self.time)\n",
        "                # torch.save(trainval_dm.train_dataset.time, os.path.join(ROOT, \"time.pt\"))\n",
        "                # with open(os.path.join(ROOT, \"time.txt\"), 'w') as f:\n",
        "                #     f.write(nuScenesDataset.read_time_dict(trainval_dm.train_dataset.time))\n",
        "        self.rm_logger.close()\n",
        "        print(\"Reward Model trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    # TODO: handle training with the TQDM bar\n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def train_traj_generator(self, trainval_dm: nuScenesDataModule):\n",
        "        \"\"\" Trajectory Generator training\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        # Training initialization\n",
        "        print(\"Starting P2T Trajectory Generator training\")\n",
        "        start = time.time()\n",
        "        self.reward_model.eval()\n",
        "        loss = TrajGenLoss('train')\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.traj_generator.parameters(), lr=self.traj_gen_lr)\n",
        "        train_dataloader = trainval_dm.train_dataloader()\n",
        "        val_dataloader = trainval_dm.val_dataloader()\n",
        "        batches_train = len(train_dataloader)\n",
        "        batches_val = len(val_dataloader)\n",
        "        start_epoch = 0\n",
        "\n",
        "        # Trajectory Generator pre-training\n",
        "        if self.pretrain_traj_gen:\n",
        "            self.pretrain_traj_generator(trainval_dm)\n",
        "\n",
        "        # Checkpoint loading\n",
        "        if self.tg_checkpoint is not None and 'tg' in self.from_checkpoint:\n",
        "            print(\"Trajectory Generator optimizer checkpoint loading ...\")\n",
        "            optimizer.load_state_dict(self.tg_checkpoint['optimizer_state'])\n",
        "            start_epoch = self.tg_checkpoint['epoch'] + 1\n",
        "        if start_epoch == self.pretrain_tg_epoches:\n",
        "            print(\"Trajectory Generator has already been trained completely\")\n",
        "            return\n",
        "\n",
        "        # Training Loop\n",
        "        #with mp.Pool(8) as process_pool:\n",
        "        for e in range(start_epoch, self.train_tg_epoches):\n",
        "            print(\"-------- [TG] Epoch %d --------\" % e)\n",
        "\n",
        "            # Train\n",
        "            self.traj_generator.train()\n",
        "            for b, data in enumerate(train_dataloader):\n",
        "\n",
        "                # Data preparation\n",
        "                (x_img_static, gt, past, motion_feats, _, agents, _) = \\\n",
        "                    self.traj_data_preparation(data, self.tgt_device)\n",
        "                model_input = (x_img_static, past, motion_feats, agents)\n",
        "\n",
        "                # Inference\n",
        "                traj_clust, _ = self(model_input)\n",
        "\n",
        "                # Learning\n",
        "                batch_loss = loss(traj_clust, gt)\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                a = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.traj_generator.parameters(), self.max_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Logging\n",
        "                loss_val = batch_loss.item()\n",
        "                iterations = e*len(train_dataloader) + b\n",
        "                self.ft_tg_logger.add_scalar(\n",
        "                    'train loss', loss_val, iterations)\n",
        "                if (b+1) % self.print_period == 0:\n",
        "                    print(\"[TG %d] %d/%d - train loss = %f\" % (e, b, batches_train, loss_val))\n",
        "\n",
        "            # Val\n",
        "            val_losses = []\n",
        "            self.traj_generator.eval()\n",
        "            with torch.no_grad():\n",
        "                for b, data in enumerate(val_dataloader):\n",
        "\n",
        "                    # Data preparation\n",
        "                    (x_img_static, gt, past, motion_feats, _, agents, _) = \\\n",
        "                        self.traj_data_preparation(data, self.tgt_device)\n",
        "                    model_input = (x_img_static, past, motion_feats, agents)\n",
        "\n",
        "                    # Inference\n",
        "                    traj_clust, _ = self(model_input)\n",
        "\n",
        "                    # Logging\n",
        "                    loss_val = loss(traj_clust, gt).item()\n",
        "                    val_losses.append(loss_val)\n",
        "                    if (b+1) % self.print_period == 0:\n",
        "                        print(\"[TG %d] %d/%d - val loss = %f\" % (e, b, batches_val, loss_val))\n",
        "\n",
        "            # Validation logging\n",
        "            mean_loss = np.array(val_losses).mean()\n",
        "            self.ft_tg_logger.add_scalar('val loss', mean_loss, e)\n",
        "\n",
        "            # Save weights\n",
        "            if (e+1) % self.checkpoint_period == 0:\n",
        "                print(\"Saving weights ...\")\n",
        "                torch.save({\n",
        "                    'epoch': e,\n",
        "                    'traj_generator_state': self.traj_generator.state_dict(),\n",
        "                    'optimizer_state': optimizer.state_dict(),\n",
        "                    'loss': mean_loss\n",
        "                }, os.path.join(self.checkpoint_dir, 'P2T', self.horizon_folder, \n",
        "                                'traj_generator', 'finetune', str(e) + '.tar'))\n",
        "\n",
        "            # Clear output periodically\n",
        "            if (e+1) % self.clear_period == 0:\n",
        "                print(\"Output clearing ...\")\n",
        "                time.sleep(1)\n",
        "                clear_output()\n",
        "\n",
        "        self.ft_tg_logger.close()\n",
        "        print(\"Trajectory Generator trained in %f s\" % (time.time() - start))\n",
        "\n",
        "    def pretrain_traj_generator(self, trainval_dm: nuScenesDataModule):\n",
        "        \"\"\" Trajectory Generator pre-training\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        trainval_dm: PyTorch Lightning datamodule containing trainval data\n",
        "        \"\"\"\n",
        "        # Training initialization\n",
        "        print(\"Starting P2T Trajectory Generator pre-training\")\n",
        "        start = time.time()\n",
        "        loss = TrajGenLoss('pretrain')\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.traj_generator.parameters(), lr=self.traj_gen_lr_pre)\n",
        "        train_dataloader = trainval_dm.train_dataloader()\n",
        "        val_dataloader = trainval_dm.val_dataloader()\n",
        "        batches_train = len(train_dataloader)\n",
        "        batches_val = len(val_dataloader)\n",
        "        start_epoch = 0\n",
        "\n",
        "        # Checkpoint loading\n",
        "        if self.tg_checkpoint is not None and 'tg_pre' in self.from_checkpoint:\n",
        "            print(\"Trajectory Generator [pre] optimizer checkpoint loading ...\")\n",
        "            optimizer.load_state_dict(self.tg_checkpoint['optimizer_state'])\n",
        "            start_epoch = self.tg_checkpoint['epoch'] + 1\n",
        "        if ((start_epoch == self.pretrain_tg_epoches) or 'tg' in self.from_checkpoint):\n",
        "            print(\"Trajectory Generator has already been pre-trained completely\")\n",
        "            return\n",
        "\n",
        "        # Training loop\n",
        "        for e in range(start_epoch, self.pretrain_tg_epoches):\n",
        "            print(\"-------- [TG (Pre)] Epoch %d --------\" % e)\n",
        "\n",
        "            # Train\n",
        "            self.traj_generator.train()\n",
        "            for b, data in enumerate(train_dataloader):\n",
        "\n",
        "                # Data preparation\n",
        "                (x_img_static, gt, past, motion_feats, plan_e, agents, grid_idcs) = \\\n",
        "                    self.traj_data_preparation(data, self.tgt_device)\n",
        "                grid_idcs = grid_idcs.to(self.tgt_device)\n",
        "                plan_e = plan_e.to(self.tgt_device)\n",
        "\n",
        "                # Reward Model inference\n",
        "                self.reward_model.motion_feats = motion_feats\n",
        "                _, _, img_feats = self.reward_model(x_img_static)\n",
        "\n",
        "                # Trajectory Prediction\n",
        "                scene_feats, agent_feats = \\\n",
        "                    nuScenesAdvanced.extract_plan_features(grid_idcs, img_feats, agents)\n",
        "                scene_feats = scene_feats.permute(1, 0, 2)\n",
        "                agent_feats = agent_feats.permute(1, 0, 2)\n",
        "                traj_pred = self.traj_generator(\n",
        "                    past, plan_e, scene_feats, agent_feats)\n",
        "\n",
        "                # Learning\n",
        "                batch_loss = loss(traj_pred, gt)\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                a = torch.nn.utils.clip_grad_norm_(\n",
        "                    self.traj_generator.parameters(), self.max_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Logging\n",
        "                loss_val = batch_loss.item()\n",
        "                iterations = e*len(train_dataloader) + b\n",
        "                self.pre_tg_logger.add_scalar(\n",
        "                    'train loss', loss_val, iterations)\n",
        "                if (b+1) % self.print_period == 0: \n",
        "                    print(\"[TG (pre) %d] %d/%d - train loss = %f\" % (e, b, batches_train, loss_val))\n",
        "                \n",
        "\n",
        "            # Val\n",
        "            val_losses = []\n",
        "            self.traj_generator.eval()\n",
        "            with torch.no_grad():\n",
        "                for b, data in enumerate(val_dataloader):\n",
        "\n",
        "                    # Data preparation\n",
        "                    (x_img_static, gt, past, motion_feats, plan_e, agents, grid_idcs) = \\\n",
        "                        self.traj_data_preparation(data, self.tgt_device)\n",
        "                    grid_idcs = grid_idcs.to(self.tgt_device)\n",
        "                    plan_e = plan_e.to(self.tgt_device)\n",
        "\n",
        "                    # Reward Model inference\n",
        "                    self.reward_model.motion_feats = motion_feats\n",
        "                    _, _, img_feats = self.reward_model(x_img_static)\n",
        "\n",
        "                    # Trajectory Prediction\n",
        "                    scene_feats, agent_feats = \\\n",
        "                        nuScenesAdvanced.extract_plan_features(grid_idcs, img_feats, agents)\n",
        "                    scene_feats = scene_feats.permute(1, 0, 2)\n",
        "                    agent_feats = agent_feats.permute(1, 0, 2)\n",
        "                    traj_pred = self.traj_generator(\n",
        "                        past, plan_e, scene_feats, agent_feats)\n",
        "\n",
        "                    # Logging\n",
        "                    loss_val = loss(traj_pred, gt).item()\n",
        "                    val_losses.append(loss_val)\n",
        "                    if (b+1) % self.print_period == 0:\n",
        "                        print(\"[TG (pre) %d] %d/%d - val loss = %f\" % (e, b, batches_val, loss_val))\n",
        "\n",
        "            # Validation logging\n",
        "            mean_loss = np.array(val_losses).mean()\n",
        "            self.pre_tg_logger.add_scalar('val loss', mean_loss, e)\n",
        "\n",
        "            # Save weights\n",
        "            if (e+1) % self.checkpoint_period == 0:\n",
        "                print(\"Saving weights ...\")\n",
        "                torch.save({\n",
        "                    'epoch': e,\n",
        "                    'traj_generator_state': self.traj_generator.state_dict(),\n",
        "                    'optimizer_state': optimizer.state_dict(),\n",
        "                    'loss': mean_loss\n",
        "                }, os.path.join(self.checkpoint_dir, 'P2T', self.horizon_folder,\n",
        "                                'traj_generator', 'pretrain', str(e) + '.tar'))\n",
        "\n",
        "            # Clear output periodically\n",
        "            if (e+1) % self.clear_period == 0:\n",
        "                print(\"Output clearing ...\")\n",
        "                time.sleep(1)\n",
        "                clear_output()\n",
        "\n",
        "        self.pre_tg_logger.close()\n",
        "        print(\"Trajectory Generator pre-trained in %f s\" %\n",
        "              (time.time() - start))\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\" Sets the modules in evaluation mode \"\"\"\n",
        "        self.reward_model.eval()\n",
        "        self.traj_generator.eval()\n",
        "\n",
        "    def forward(self, x, kmeans_pool=None):\n",
        "        \"\"\" P2T complete model inference\n",
        "        Input: x tuple with fields:\n",
        "            - x_img_static: raster map img with only static elements\n",
        "            - past: history of the agent\n",
        "            - motion_feats: motion and position features for the reward model\n",
        "            - agents: tensor with states of other agents in the scene\n",
        "        Pool: kmeans_pool is a process pool to speed-up kmeans execution\n",
        "        Output: traj_clust -> tensor with clustered trajectories\n",
        "        \"\"\"\n",
        "        # Data extraction\n",
        "        x_img_static, past, motion_feats, agents = x\n",
        "\n",
        "        # Reward Model inference\n",
        "        self.reward_model.motion_feats = motion_feats\n",
        "        rew_path, rew_goal, img_feats = self.reward_model(x_img_static)\n",
        "\n",
        "        # MaxEntropy Reinforcement Learning\n",
        "        grid_extent = self.grid_extent\n",
        "        self.mdp.value_iteration(rew_path.detach(), rew_goal.detach())\n",
        "        self.mdp.policy_propagation(self.initial_state)\n",
        "\n",
        "        svf = self.mdp.svf\n",
        "        policy = self.mdp.pi\n",
        "\n",
        "        plan, scene_feats, agent_feats = self.mdp.sample_policy(\n",
        "            self.policy_samples, self.initial_state, img_feats, agents, self.centers)\n",
        "\n",
        "        # Trajectory Prediction\n",
        "        sc_feat_size = self.traj_generator.scene_feat_size\n",
        "        ag_feat_size = self.traj_generator.agent_feat_size\n",
        "        plan = plan.reshape(-1, self.mdp.horizon,\n",
        "                            2).permute(1, 0, 2).to(self.tgt_device)\n",
        "        scene_feats = scene_feats.reshape(\n",
        "            -1, self.mdp.horizon, sc_feat_size).permute(1, 0, 2).to(self.tgt_device)\n",
        "        agent_feats = agent_feats.reshape(\n",
        "            -1, self.mdp.horizon, ag_feat_size).permute(1, 0, 2).to(self.tgt_device)\n",
        "        past = past.unsqueeze(2).repeat(1, 1, self.policy_samples, 1)\n",
        "        past = past.reshape(past.shape[0], -1, past.shape[3])\n",
        "        traj_pred = self.traj_generator(past, plan, scene_feats, agent_feats)\n",
        "\n",
        "        # K-Means trajectory clustering\n",
        "        # TODO: find a way to exploit multiprocessing\n",
        "        # TODO: try to avoid loops in final clustering\n",
        "        traj_pred = traj_pred.reshape(\n",
        "            -1, self.policy_samples, traj_pred.shape[1], traj_pred.shape[2])\n",
        "        traj_flat = traj_pred.flatten(-2).detach().cpu().numpy()\n",
        "        clust_ids = [self.kmeans_cluster(t, self.traj_clusters) for t in traj_flat]\n",
        "        traj_clust = torch.zeros(traj_pred.shape[0], self.traj_clusters,\n",
        "                                 traj_pred.shape[2], traj_pred.shape[3])\n",
        "        probs_clust = torch.zeros(traj_pred.shape[0], self.traj_clusters)\n",
        "        for n in range(traj_pred.shape[0]):\n",
        "            clusters = np.array(list(set(clust_ids[n])))\n",
        "            clust_idcs = clust_ids[n] == clusters.reshape(clusters.shape[0], 1)\n",
        "            centroids = torch.zeros(\n",
        "                len(clusters), traj_pred.shape[2], traj_pred.shape[3])\n",
        "            probs = torch.zeros(len(clusters))\n",
        "            for cl in range(len(clusters)):\n",
        "                centroids[cl] = traj_pred[n, clust_idcs[cl]].mean(dim=0)\n",
        "                probs[cl] = clust_idcs[cl].sum()\n",
        "            traj_clust[n, :len(clusters)] = centroids\n",
        "            probs_clust[n, :len(clusters)] = probs\n",
        "        traj_clust = traj_clust.to(self.tgt_device)\n",
        "        return traj_clust, probs_clust\n",
        "\n",
        "    @staticmethod\n",
        "    def select_checkpoint(checkpoints_path, method='last'):\n",
        "        \"\"\" Select appropriate checkpoint from the path \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        checkpoints_path: path contatining all the checkpoints\n",
        "        method: method to choose the checkpoints (available 'last', 'best')\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        checkpoint_path: path of the checkpoint to load\n",
        "        \"\"\"\n",
        "        checkpoint_names = os.listdir(checkpoints_path)\n",
        "        if len(checkpoint_names) == 0:\n",
        "            return None\n",
        "        checkpoint_ext = checkpoint_names[0].split('.')[1]\n",
        "        checkpoint_vals = [int(c.split('.')[0]) for c in checkpoint_names if f'.{checkpoint_ext}' in c]\n",
        "        checkpoint = None\n",
        "        if method == 'last':\n",
        "            checkpoint_path = os.path.join(\n",
        "                checkpoints_path, f'{max(checkpoint_vals)}.{checkpoint_ext}')\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "        elif method == 'best':\n",
        "            for c in checkpoint_vals:\n",
        "                temp_c = torch.load(os.path.join(checkpoints_path, f'{c}.{checkpoint_ext}'))\n",
        "                if checkpoint is not None:\n",
        "                    if temp_c['loss'] < checkpoint['loss']:\n",
        "                        checkpoint = temp_c\n",
        "                else:\n",
        "                    checkpoint = temp_c\n",
        "        else:\n",
        "            raise ValueError(\"Wrong value of the argument 'method'. Available 'last', 'best'\")\n",
        " \n",
        "        return checkpoint\n",
        "\n",
        "    @staticmethod\n",
        "    def traj_data_preparation(data, device):\n",
        "        \"\"\" Prepare data for Trajectory Generator training. Useful to avoid repeated code \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: batch of data from the dataset\n",
        "        device: device of execution [e.g. CUDA:0]\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        x_img_static: raster map img with only static elements\n",
        "        gt: future of the agent (ground truth)\n",
        "        past: history of the agent \n",
        "        motion_feats: motion and position features for the reward model\n",
        "        plan_e: coords of the grid cells relative to the SVF (expert)\n",
        "        agents: tensor with states of other agents in the scene\n",
        "        grid_idcs: grid coords of the SVF (expert)\n",
        "        \"\"\"\n",
        "        (_, x_img_static, _, gt, _, past, surr_agents, \n",
        "         _, plan_e, grid_idcs, motion_feats) = data\n",
        "        gt = gt.float().to(device)\n",
        "        past = past.permute(1, 0, 2).to(device)\n",
        "        motion_feats = motion_feats.float().to(device)\n",
        "        plan_e = plan_e.permute(1, 0, 2)\n",
        "        surr_agents = surr_agents.to(device)\n",
        "        x_img_static = x_img_static.float().to(device)\n",
        "        return (x_img_static, gt, past, motion_feats,\n",
        "                plan_e, surr_agents, grid_idcs)    \n",
        "\n",
        "    @staticmethod\n",
        "    @ignore_warnings(category=ConvergenceWarning)\n",
        "    def kmeans_cluster(data, n_clusters):\n",
        "        cluster_data = KMeans(n_clusters, n_init=1, max_iter=100).fit(data)\n",
        "        return cluster_data.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "125cUEUY4E8k"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbrookoH4E8n"
      },
      "source": [
        "**Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAWMSS-2ispD"
      },
      "source": [
        "Useful function to compute metrics for the nuScenes dataset. It's possible also to use the built-in compute_metrics nuScenes function, but this one shows better results both in computation time and metric values, furthermore it works with a different kind of inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ahp9DTkU4E8n"
      },
      "outputs": [],
      "source": [
        "def compute_traj_metrics(predictions: List[data_classes.Prediction], ground_truths: List[np.ndarray], \n",
        "                    helper, aggregators=AGGREGATORS) -> Dict[str, Any]:\n",
        "    \"\"\" Utility eval function to compute dataset metrics\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    predictions: list of predictions made by the model (in Prediction class format)\n",
        "    ground_truths: the real trajectories of the agent (SHAPE -> [len(dataset), n_samples, state_dim])\n",
        "    helper: nuScenes dataset helper\n",
        "    aggregators: functions to aggregate metrics (e.g. mean)\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    metric_output: dictionary of the computed metrics:\n",
        "        - minADE_5: The average of pointwise L2 distances between the predicted trajectory \n",
        "                    and ground truth over the 5 most likely predictions.\n",
        "        - minADE_10: The average of pointwise L2 distances between the predicted trajectory \n",
        "                    and ground truth over the 10 most likely predictions.\n",
        "        - missRateTop_2_5: Proportion of misses relative to the 5 most likely trajectories\n",
        "                        over all agents\n",
        "        - missRateTop_2_10: Proportion of misses relative to the 10 most likely trajectories\n",
        "                        over all agents\n",
        "        - minFDE_1: The final displacement error (FDE) is the L2 distance \n",
        "                    between the final points of the prediction and ground truth, computed\n",
        "                    on the most likely trajectory\n",
        "        - offRoadRate: the fraction of trajectories that are not entirely contained\n",
        "                    in the drivable area of the map.\n",
        "    \"\"\"\n",
        "    # 1. Define metrics\n",
        "    print(\"\\t - Metrics definition ...\")\n",
        "    aggregators = \\\n",
        "        [metrics.deserialize_aggregator(agg) for agg in aggregators]\n",
        "    min_ade = metrics.MinADEK([5, 10], aggregators)\n",
        "    miss_rate = metrics.MissRateTopK([5, 10], aggregators)\n",
        "    min_fde = metrics.MinFDEK([1], aggregators)\n",
        "    if helper is not None:\n",
        "        # FIXME: instantiating offRoadRate class makes RAM explode\n",
        "        #offRoadRate = metrics.OffRoadRate(helper, aggregators)\n",
        "        pass\n",
        "    else:\n",
        "        offRoadRate = None\n",
        "\n",
        "    # 2. Compute metrics\n",
        "    metric_list = []\n",
        "    print(\"\\t - Effective metrics computation ...\")\n",
        "    for p, pred in enumerate(tqdm(predictions)):\n",
        "        minADE_5 = min_ade(ground_truths[p], pred)[0][0]\n",
        "        minADE_10 = min_ade(ground_truths[p], pred)[0][1]\n",
        "        missRateTop_2_5 = miss_rate(ground_truths[p], pred)[0][0]\n",
        "        missRateTop_2_10 = miss_rate(ground_truths[p], pred)[0][1]\n",
        "        minFDE_1 = min_fde(ground_truths[p], pred)\n",
        "        #offRoadRate = offRoadRate(ground_truth[p], pred)\n",
        "        metric = {'minADE_5': minADE_5, 'missRateTop_2_5': missRateTop_2_5,\n",
        "                  'minADE_10': minADE_10, 'missRateTop_2_10': missRateTop_2_10,\n",
        "                  'minFDE_1': minFDE_1}#, 'offRoadRate': offRoadRate}\n",
        "        metric_list.append(metric)\n",
        "\n",
        "    # 3. Aggregate\n",
        "    print(\"\\t - Metrics aggregation ...\")\n",
        "    aggregations: Dict[str, Dict[str, List[float]]] = defaultdict(dict)\n",
        "    metric_names = list(metric_list[0].keys())\n",
        "    metrics_dict = {name: np.array([metric_list[i][name] for i in range(len(metric_list))]) \n",
        "                    for name in metric_names}\n",
        "    for metric in metric_names:\n",
        "        for agg in aggregators:\n",
        "            aggregations[metric][agg.name] = agg(metrics_dict[metric])\n",
        "\n",
        "    return aggregations    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZDN6984E8o"
      },
      "source": [
        "**Plotting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3nw5TwjG_p"
      },
      "source": [
        "The first plot funcion should be used only for debug, because trainining graphs are obtained normally with Tensorboard. Anyway, in each model there is the possibility to use this plotting function during the training process to plot the training trend.\n",
        "\n",
        "The second plot function can be used to plot the agent future in a raster map representation, but usually it's made of few samples, so it's not sufficient to have a clear view of what is the way that the agent will follow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FBO9nC1b4E8p"
      },
      "outputs": [],
      "source": [
        "def plot_train_data(train_iterations, val_iterations, epoches, train_losses, val_losses):\n",
        "    \"\"\" Plot a graph with the training trend\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_iterations: number of iterations for each epoch [train]\n",
        "    val_iterations: number of iterations for each epoch [val]\n",
        "    epoches: actual epoch number (starting from 1)\n",
        "    train_losses: array of loss values [train]\n",
        "    val_losses: array of loss values [val]\n",
        "    \"\"\"\n",
        "    # Data preparation\n",
        "    train_iterations_list = list(range(epoches*(train_iterations)))\n",
        "    val_iterations_list = list(range(epoches*(val_iterations)))\n",
        "    epoches_list = list(range(epoches))\n",
        "\n",
        "    # Adjust validation array dimension\n",
        "    val_error = len(val_losses) - len(val_iterations_list)\n",
        "    if val_error > 0:\n",
        "        val_losses = val_losses[:-val_error]\n",
        "\n",
        "    # Per-iteration plot\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-iteration Loss [train]')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Value')\n",
        "    l1, = plt.plot(train_iterations_list, train_losses, c='blue')\n",
        "    plt.legend(handles=[l1], labels=['Train loss'], loc='best')\n",
        "    plt.show()\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-iteration Loss [val]')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Value')\n",
        "    l2, = plt.plot(val_iterations_list, val_losses, c='red')\n",
        "    plt.legend(handles=[l2], labels=['Validation loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # Per-epoch plot\n",
        "    fig = plt.figure()\n",
        "    plt.title('Per-epoch Loss')\n",
        "    plt.xlabel('Epoches')\n",
        "    plt.ylabel('Value')\n",
        "    train_avg_losses = [np.array(train_losses[i:i+train_iterations]).mean() \n",
        "                        for i in range(0, len(train_losses), train_iterations)]\n",
        "    val_avg_losses = [np.array(val_losses[i:i+val_iterations]).mean() \n",
        "                      for i in range(0, len(val_losses), val_iterations)]\n",
        "    l1, = plt.plot(epoches_list, train_avg_losses, c='blue')\n",
        "    l2, = plt.plot(epoches_list, val_avg_losses, c='red')\n",
        "    plt.legend(handles=[l1, l2], labels=['Train loss', 'Validation loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "def plot_agent_future(raster, future, agent_pos=(0,0), reference_frame='local', color='green'):\n",
        "    \"\"\" Plot agent's future trajectory\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raster: raster map tensor (image)\n",
        "    future: future trajectory of the agent (predicted or GT) [x,y]\n",
        "    agent_pos: position of the agent (needed in case of local coords)\n",
        "    reference_frame: frame to which future coordinates refer\n",
        "    color: color of the plotted trajectory\n",
        "    \"\"\"\n",
        "    # Show raster map\n",
        "    plt.imshow(raster.permute(1, 2, 0))\n",
        "\n",
        "    # Show trajectory\n",
        "    x, y = [], []\n",
        "    for i in range(len(future)):\n",
        "        point = (agent_pos[0], agent_pos[1]) if i == 0 else future[i].numpy()\n",
        "        if reference_frame == 'local' and i > 0:\n",
        "            point = (point[0] + agent_pos[0], -point[1] + agent_pos[1])\n",
        "        x.append(point[0])\n",
        "        y.append(point[1])\n",
        "    \n",
        "    plt.plot(x, y, color=color, markersize=10, linewidth=5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIMN2u3AkE3"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DiJYZKAuhZ"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4ovKDsXj1Hz"
      },
      "source": [
        "1.   Baseline selection -> chose whether to use '*P2T*' or '*CoverNet*' baselines\n",
        "2.   Dataset initialization -> initializes nuScenes helper (if needed), dataset with all the various splits, and datamodules\n",
        "3.   Network initialization -> initializes baselines, ready to be used. N.B: in order to load models with checkpoints, use ```CoverNet.load_from_checkpoint()``` for CoverNet class or ```from_checkpoint=['rm', 'tg']``` as parameter for the P2T class\n",
        "4.   Trainer initialization -> initializes the Trainer from PyTorch-Lightning for the training process. At this moment, it's only available for the CoverNet baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959,
          "referenced_widgets": [
            "2c143b3d29c6425f9741da7b5b3cde38",
            "d265a46cede848d19064babef8f6f9f0",
            "cfecfd4df3664336b5c3c6ea51aabd39",
            "46bb4c94c71844d0b67b206e1b4aac54",
            "8f79bd3668484bb6b2f7e205b4bc299d",
            "b99ab9d1c9974c22b1aa940dc6b225ef",
            "4c0bfd39521a4d5ba82ebd9e88653578",
            "a3563cebdfe149a3974fd669a51d2364",
            "068a6114b00e427f8393d85319962351",
            "f89a5e1a2ccb45ceb8cecac19bdc38e5",
            "f2c689524f83497a80d298d0c39cf097",
            "27029c803ab547beaf634de3205120c7",
            "beb36ff225df4b2f954b20591826328b",
            "57388def33e2422792567bb4340e1c73",
            "c09acfdd256b4e26ab9da2522b499819",
            "2f5674676aba46378d48a39e16979428",
            "f13d5142bf254781939e2b776371c5a6",
            "a6f4c00b5d6d46afa994bda5ef8281a9",
            "0a6c8a923ff242e4b4d94650801a38f1",
            "3f8a5fc026fc4a88914776fb49ad189a",
            "373e78e969d843a08d08010d064162b2",
            "f331b35392a14c3bbdc518982544ac06"
          ]
        },
        "id": "lsGEsxRVAuLI",
        "outputId": "ea201598-6ed7-4ed4-a83f-62922021f175"
      },
      "outputs": [],
      "source": [
        "# ---------- Baseline selection ---------- #\n",
        "prediction_model = 'P2T'\n",
        "\n",
        "# ---------- Dataset initialization ---------- #\n",
        "# Initialize nuScenes helper\n",
        "print(\"nuScenes Helper initialization ...\")\n",
        "start_time = time.time()\n",
        "nusc = None\n",
        "pl.seed_everything(PL_SEED)\n",
        "if ENVIRONMENT == 'local':\n",
        "    if PREPARE_DATASET and ('nuscenes_checkpoint'+FILENAME_EXT not in os.listdir(ROOT)):\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)\n",
        "        with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'wb') as f:\n",
        "            pickle.dump(nusc, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    elif not 'nusc' in locals():\n",
        "        if HELPER_LOADING or not PREPROCESSED:\n",
        "            with open(os.path.join(ROOT, 'nuscenes_checkpoint'+FILENAME_EXT), 'rb') as f:\n",
        "                nusc = pickle.load(f)\n",
        "elif ENVIRONMENT == 'colab' and ((not PREPROCESSED) or HELPER_LOADING):\n",
        "        nusc = NuScenes(version=DATASET_VERSION, dataroot=DATAROOT, verbose=True)\n",
        "helper = PredictHelper(nusc) if nusc is not None else None\n",
        "print(\"nuScenes Helper initialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# Initialize dataset and data module\n",
        "print(\"\\nDataset and Data Module initialization ...\")\n",
        "start_time = time.time()\n",
        "if MINI_DATASET:\n",
        "    train_split, val_split = 'mini_train', 'mini_val'\n",
        "    test_split, test_dataset = None, None\n",
        "else:\n",
        "    train_split, val_split = 'train', 'train_val'\n",
        "    test_split = 'val'\n",
        "if prediction_model == 'CoverNet':\n",
        "    train_dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED, split=train_split)\n",
        "    val_dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED, split=val_split)\n",
        "    if test_split is not None:\n",
        "        test_dataset = nuScenesDataset(helper, preprocessed=PREPROCESSED, split=test_split)\n",
        "elif prediction_model == 'P2T':\n",
        "    train_dataset = nuScenesAdvanced(helper, preprocessed=PREPROCESSED, split=train_split)\n",
        "    val_dataset = nuScenesAdvanced(helper, preprocessed=PREPROCESSED, split=val_split)\n",
        "    if test_split is not None:\n",
        "        test_dataset = nuScenesAdvanced(helper, preprocessed=PREPROCESSED, split=test_split)\n",
        "trainval_dm = nuScenesDataModule(train_dataset, val_dataset, test_dataset, num_workers=NUM_WORKERS)\n",
        "print(\"Dataset and Data Module initialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# ---------- Checkpoint files initialization ---------- #\n",
        "if ENVIRONMENT == 'colab' and MINI_DATASET:\n",
        "    if 'checkpoints' not in os.listdir(ROOT):\n",
        "        print(\"\\nCheckpoint files initialization ...\")\n",
        "        start_time = time.time()\n",
        "        checkpoints_archive = 'checkpoints.zip'\n",
        "        gdown.download(CHECKPOINTS_LINK, checkpoints_archive, quiet=False)\n",
        "        with zipfile.ZipFile(checkpoints_archive, 'r') as zip_ref:\n",
        "            zip_ref.extractall(ROOT)\n",
        "        os.remove(checkpoints_archive)\n",
        "        print(\"Checkpoint files initialized in %f s\\n\" % (time.time() - start_time))\n",
        "        \n",
        "# ---------- Network initialization ---------- #\n",
        "start_time = time.time()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if prediction_model == 'CoverNet':\n",
        "    print(\"\\nCoverNet model initialization ...\")\n",
        "    model = CoverNet(K_SIZE, EPSILON, TRAJ_LINK, TRAJ_DIR, device)\n",
        "    print(\"CoverNet model intialization done in %f s\\n\" % (time.time() - start_time))\n",
        "elif prediction_model == 'P2T':\n",
        "    print(\"\\nP2T model initialization ...\")\n",
        "    mdp = MDP(MDP_HORIZON, GRID_SIDE, device)\n",
        "    model = P2T(mdp, device, train_dataset.centers, from_checkpoint=[])\n",
        "    print(\"P2T model intialization done in %f s\\n\" % (time.time() - start_time))\n",
        "\n",
        "# ---------- Trainer initialization ---------- #\n",
        "print(\"\\nTrainer initialization ...\")\n",
        "start_time = time.time()\n",
        "GPUS = min(1, torch.cuda.device_count())\n",
        "covernet_dirpath = os.path.join(CHECKPOINT_DIR, 'CoverNet', str(TRAJ_HORIZON))\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=covernet_dirpath,\n",
        "                                      save_top_k=TOP_K_SAVE,\n",
        "                                      monitor=CHECKPOINT_MONITOR)\n",
        "trainer = pl.Trainer(callbacks=[checkpoint_callback],\n",
        "                     gpus=GPUS, max_epochs=TRAIN_EPOCHES)\n",
        "print(\"Trainer intialization done in %f s\\n\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y1_nMkNAw9A"
      },
      "source": [
        "**Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2AJXEG_ztAw"
      },
      "outputs": [],
      "source": [
        "trainval_dm.setup(stage='fit')\n",
        "\n",
        "if prediction_model == 'CoverNet':\n",
        "    trainer.fit(model, trainval_dm)\n",
        "elif prediction_model == 'P2T':\n",
        "    model.train(trainval_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdtPl1sv4E8t"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwxNsBzxmlVG"
      },
      "source": [
        "Testing here is basically the metrics computation. Since the project requires the solution with trajectory horizons of both 3 seconds and 6 seconds, don't forget to restart runtime and reset the *TRAJECTORY_HORIZON* parameter to the desired value. We have conduct also experiments with a shortest agent history (1 second instead of 2), but only with Pretrain of the P2T trajectory generator class. So, for this moment, to compute correctly the metrics is necessary to have *AGENT_HISTORY* = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ed133b40b19941bb8dd5513d71b21e85",
            "2f656033d56041aa8978a46d8bb708a5",
            "3c31ca02039444c69eb80ebccc0aceb9",
            "dfbaacd239d74a9384ac57287256d41a",
            "fb5ae27d0a92495fa8588fbbc181c24a",
            "665955ba546b4410a6abd1850f2c57bc",
            "30f648d3e6c24a3fb99c19d9590f6294",
            "a9ac600e1d3a4de6943d40fdd41ddd4e",
            "c7057eb3bf8e4857b49cf7dacb237a42",
            "922e52d5f3334876acd3003bf35c6c41",
            "117186b02f5e4dedbe19d87d4f33a589",
            "f73a3f7cb2314a568782068f1c04151d",
            "a0b8bf4034f8497d9ca2641e84a36b71",
            "ff6087aaa0ff44b89938471d0fb0e957",
            "714f6a403d32439280cd5e6b48a5434f",
            "7301ee09215d48f38cb32881cd0e8377",
            "44d3596d0c5c4114b41ed7e503c9f1f2",
            "3d0e91cee37f4c90943d18036a885ec6",
            "9c741e0fca1245c5a08d76dec11bd3fd",
            "108df33aefeb49b8abc8ccf6b6106404",
            "8d7e0ca9b6384a8ebcf7ed368deb351a",
            "6ccae592d2da48629859a3c1b6bbeaa7"
          ]
        },
        "id": "mKTCwqfg4E8u",
        "outputId": "ebb5ce40-705e-4e59-83ae-b8248dcb7f8b"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader initialization\n",
        "print(\"Loading test dataloader ...\")\n",
        "trainval_dm.setup(stage='test')\n",
        "test_dataset = trainval_dm.test_dataset\n",
        "test_dataloader = trainval_dm.test_dataloader()\n",
        "test_generator = iter(test_dataloader)\n",
        "\n",
        "# Baseline testing\n",
        "if prediction_model == 'CoverNet':\n",
        "    # CoverNet initialization\n",
        "    print(\"\\nCoverNet checkpoint loading ...\")\n",
        "    checkpoint_name = 'epoch=19-step=80460.ckpt'\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, 'CoverNet', \n",
        "                                   HORIZON_FOLDER, BEST_FOLDER, checkpoint_name)\n",
        "    model = CoverNet.load_from_checkpoint(checkpoint_path=checkpoint_path, \n",
        "                                          map_location=None, hparams_file=None, strict=True, \n",
        "                                          K_size=K_SIZE, epsilon=EPSILON, traj_link=TRAJ_LINK, \n",
        "                                          traj_dir=TRAJ_DIR, device=device).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # CoverNet metrics computation\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    start = time.time()\n",
        "    reduced_traj = model.trajectories[:, :model.traj_samples].numpy()\n",
        "    print(\"\\nCoverNet metrics computation ...\")\n",
        "    print(\"1 - Producing predictions ...\")\n",
        "    for i, token in enumerate(tqdm(test_dataset.tokens)):\n",
        "        with torch.no_grad():\n",
        "            x_state, _, x_img, gt, _ = test_dataset[i]\n",
        "            x_state = x_state.to(device)\n",
        "            x_img = x_img.to(device)\n",
        "            x_state = torch.unsqueeze(torch.flatten(x_state, 0, 1), 0)\n",
        "            x_img = torch.unsqueeze(x_img, 0)\n",
        "            pred_logits = model((x_img, x_state))\n",
        "            pred_probs = F.softmax(pred_logits, dim=1)[0]\n",
        "            top_indices = pred_probs.argsort()[-MAX_PREDICTED_MODES:]\n",
        "            cutted_probs = pred_probs[top_indices].cpu().numpy()\n",
        "            cutted_traj = reduced_traj[top_indices.cpu()]\n",
        "        i_t, s_t = token.split(\"_\")\n",
        "        ground_truths.append(gt.numpy())\n",
        "        predictions.append(data_classes.Prediction(i_t, s_t, cutted_traj, cutted_probs))\n",
        "    print(\"2 - Computing metrics ...\")\n",
        "    convernet_metrics = compute_traj_metrics(predictions, ground_truths, helper)\n",
        "    print(\"Metric computation done in %f s\" % (time.time() - start))\n",
        "elif prediction_model == 'P2T':\n",
        "    # P2T initialization\n",
        "    print(\"\\nP2T testing initialization ...\")\n",
        "    model = P2T(mdp, device, train_dataset.centers, \n",
        "                from_checkpoint=['rm', 'tg'], checkpoint_method='best')\n",
        "    model.policy_samples = 1000\n",
        "    model.eval()\n",
        "\n",
        "    # P2T metrics computation\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    start = time.time()\n",
        "    print(\"P2T metrics computation ...\")\n",
        "    print(\"1 - Producing predictions ...\")\n",
        "    for i, token in enumerate(tqdm(test_dataset.tokens)):\n",
        "        with torch.no_grad():\n",
        "            data = test_dataset[i]\n",
        "            data = [d.unsqueeze(0) if isinstance(d, torch.Tensor) else d for d in data]\n",
        "            (x_img_static, gt, past, motion_feats, _, agents, _) = \\\n",
        "                model.traj_data_preparation(data, device)\n",
        "            model_input = (x_img_static, past, motion_feats, agents)\n",
        "            traj_clust, probs_clust = model(model_input)\n",
        "            traj_clust = traj_clust.squeeze(0).detach().cpu().numpy()\n",
        "            probs_clust = probs_clust.squeeze(0).detach().cpu().numpy()\n",
        "            traj_clust = traj_clust[probs_clust != 0]\n",
        "            probs_clust = probs_clust[probs_clust != 0]\n",
        "        i_t, s_t = token.split(\"_\")\n",
        "        ground_truths.append(gt.cpu().numpy())\n",
        "        predictions.append(data_classes.Prediction(i_t, s_t, traj_clust, probs_clust))\n",
        "    print(\"2 - Computing metrics ...\")\n",
        "    p2t_metrics = compute_traj_metrics(predictions, ground_truths, helper)\n",
        "    print(\"Metric computation done in %f s\" % (time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7R_TMe2fJMq"
      },
      "outputs": [],
      "source": [
        "print(\"2 - Computing metrics ...\")\n",
        "p2t_metrics = compute_traj_metrics(predictions, ground_truths, helper)\n",
        "print(\"Metric computation done in %f s\" % (time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2 - Computing metrics ...\")\n",
        "p2t_metrics = compute_traj_metrics(predictions, ground_truths, helper)\n",
        "print(\"Metric computation done in %f s\" % (time.time() - start))"
      ],
      "metadata": {
        "id": "G7R_TMe2fJMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ow8tPik4E8v"
      },
      "outputs": [],
      "source": [
        "## Obtained with trajectory horizon = 6 seconds\n",
        "convernet_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39b09rgy4E8w"
      },
      "outputs": [],
      "source": [
        "## Obtained with trajectory horizon = 3 seconds\n",
        "convernet_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3BS78CLztAy",
        "outputId": "193c2cdf-7db6-4a41-e0cf-287bbc89380c"
      },
      "outputs": [],
      "source": [
        "## Obtained with trajectory horizon = 6 seconds\n",
        "p2t_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGyk-xDJztAy"
      },
      "outputs": [],
      "source": [
        "## Obtained with trajectory horizon = 3 seconds\n",
        "p2t_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8pZmgo4E8w"
      },
      "source": [
        "## Code Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKmZ9Vs0ztAz",
        "outputId": "308c2c67-da0b-4196-aa17-3a373bc680f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************************************\n",
            "agent type = torch.float32      | surr agents type = torch.float32\n",
            "raster sta type = torch.float32 | raster dyn type = torch.float32\n",
            "gt type = torch.float32         | idx type = <class 'int'> | past type = torch.float32\n",
            "svf_e type = torch.float32      | plan_e type = torch.float32\n",
            "grid idcs type = torch.float32  | motion feats type = torch.float32\n",
            "****************************************************************************\n"
          ]
        }
      ],
      "source": [
        "print(\"****************************************************************************\")\n",
        "print(\"agent type =\", agent_state_vector.dtype, \"     | surr agents type =\", surr_agents.dtype)\n",
        "print(\"raster sta type =\", raster_img_static.dtype, \"| raster dyn type =\", raster_img_dynamic.dtype)\n",
        "print(\"gt type =\", gt_trajectory.dtype, \"        | idx type =\", type(idx), \"| past type =\", past.dtype)\n",
        "print(\"svf_e type =\", svf_e.dtype, \"     | plan_e type =\", plan_e.dtype)\n",
        "print(\"grid idcs type =\", grid_idcs.dtype, \" | motion feats type =\", motion_feats.dtype)\n",
        "print(\"****************************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMawwmh_ztAz"
      },
      "outputs": [],
      "source": [
        "agent_state_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4qPNSRNAoCA"
      },
      "outputs": [],
      "source": [
        "agent_state_vector.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKmZ9Vs0ztAz"
      },
      "outputs": [],
      "source": [
        "print(\"****************************************************************************\")\n",
        "print(\"agent type =\", agent_state_vector.dtype, \"     | surr agents type =\", surr_agents.dtype)\n",
        "print(\"raster sta type =\", raster_img_static.dtype, \"| raster dyn type =\", raster_img_dynamic.dtype)\n",
        "print(\"gt type =\", gt_trajectory.dtype, \"        | idx type =\", type(idx), \"| past type =\", past.dtype)\n",
        "print(\"svf_e type =\", svf_e.dtype, \"     | plan_e type =\", plan_e.dtype)\n",
        "print(\"grid idcs type =\", grid_idcs.dtype, \" | motion feats type =\", motion_feats.dtype)\n",
        "print(\"****************************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ho4Hc9xztA0"
      },
      "outputs": [],
      "source": [
        "agent_state_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bkaA3FxztA0"
      },
      "outputs": [],
      "source": [
        "raster_img_static.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy1unXJjztA0"
      },
      "outputs": [],
      "source": [
        "plt.imshow(raster_img_static.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuSjaFJNztA1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(raster_img_dynamic.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "surr_agents.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3YjsMBr3b5N",
        "outputId": "b0d29985-bb39-460b-978e-3f441fc7c8a5"
      },
      "execution_count": null,
      "metadata": {
        "id": "8kNYaD8hztA1"
      },
      "outputs": [],
      "source": [
        "gt_trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5dwF0WHztA1"
      },
      "outputs": [],
      "source": [
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-huxILDztA1"
      },
      "outputs": [],
      "source": [
        "past"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3YjsMBr3b5N"
      },
      "outputs": [],
      "source": [
        "surr_agents.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb-p5roQztA2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL1efwAmztA3"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt-YBnBEztA3"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEssO6EkztA3"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POIyRctrztA4"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[0:3].permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7IvG0iOztA4"
      },
      "outputs": [],
      "source": [
        "plt.imshow(surr_agents[1:4].permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhPr38-cztA4"
      },
      "outputs": [],
      "source": [
        "svf_e_mod = torch.cat([svf_e, torch.zeros(1, 25, 25)])\n",
        "plt.imshow(svf_e_mod.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhlBVvDbztA5"
      },
      "outputs": [],
      "source": [
        "plan_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRHC2oEKztA5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  0.],\n",
              "        [ 3.,  0.],\n",
              "        [ 5.,  0.],\n",
              "        [ 7.,  0.],\n",
              "        [ 9.,  0.],\n",
              "        [11.,  0.],\n",
              "        [13.,  0.],\n",
              "        [15.,  0.],\n",
              "        [17.,  0.],\n",
              "        [19.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "plan_e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5LUHy69ztA6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 12.],\n",
              "        [18., 12.],\n",
              "        [17., 12.],\n",
              "        [16., 12.],\n",
              "        [15., 12.],\n",
              "        [14., 12.],\n",
              "        [13., 12.],\n",
              "        [12., 12.],\n",
              "        [11., 12.],\n",
              "        [10., 12.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.],\n",
              "        [ 0.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "grid_idcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dYyA_rIztA6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(motion_feats.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTxBsrOMztA7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726],\n",
              "         [ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726],\n",
              "         [ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726],\n",
              "         ...,\n",
              "         [ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726],\n",
              "         [ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726],\n",
              "         [ 4.8726,  4.8726,  4.8726,  ...,  4.8726,  4.8726,  4.8726]],\n",
              "\n",
              "        [[-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800],\n",
              "         [-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800],\n",
              "         [-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800],\n",
              "         ...,\n",
              "         [-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800],\n",
              "         [-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800],\n",
              "         [-0.4800, -0.4400, -0.4000,  ...,  0.4000,  0.4400,  0.4800]],\n",
              "\n",
              "        [[ 0.7800,  0.7800,  0.7800,  ...,  0.7800,  0.7800,  0.7800],\n",
              "         [ 0.7400,  0.7400,  0.7400,  ...,  0.7400,  0.7400,  0.7400],\n",
              "         [ 0.7000,  0.7000,  0.7000,  ...,  0.7000,  0.7000,  0.7000],\n",
              "         ...,\n",
              "         [-0.1000, -0.1000, -0.1000,  ..., -0.1000, -0.1000, -0.1000],\n",
              "         [-0.1400, -0.1400, -0.1400,  ..., -0.1400, -0.1400, -0.1400],\n",
              "         [-0.1800, -0.1800, -0.1800,  ..., -0.1800, -0.1800, -0.1800]]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "motion_feats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx, past, surr_agents, svf_e, plan_e, grid_idcs, motion_feats = train_dataset[10]\n",
        "\n",
        "subfolder = f'batch_{idx//128}'\n",
        "add_data = np.load(os.path.join(\n",
        "    DATAROOT, PREPROCESSED_ADV_FOLDER, 'train',\n",
        "    subfolder, train_dataset.tokens[idx] + ADD_SUFFIX + ADDITIONAL_EXT),\n",
        "    allow_pickle=True).item()\n",
        "future_indefinite = add_data['future_indefinite']\n",
        "\n",
        "# TODO: check how to get automatically the agent position in the map                                        \n",
        "plot_agent_future(raster_img_dynamic, future_indefinite, agent_pos=(100,160), reference_frame='local')"
      ],
      "metadata": {
        "id": "1awlw0hmDDYP"
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "agent_state_vector, raster_img_static, raster_img_dynamic, gt_trajectory, idx, past, surr_agents, svf_e, plan_e, grid_idcs, motion_feats = train_dataset[10]\n",
        "\n",
        "subfolder = f'batch_{idx//128}'\n",
        "add_data = np.load(os.path.join(\n",
        "    DATAROOT, PREPROCESSED_ADV_FOLDER, 'train',\n",
        "    subfolder, train_dataset.tokens[idx] + ADD_SUFFIX + ADDITIONAL_EXT),\n",
        "    allow_pickle=True).item()\n",
        "future_indefinite = add_data['future_indefinite']\n",
        "\n",
        "# TODO: check how to get automatically the agent position in the map                                        \n",
        "plot_agent_future(raster_img_dynamic, future_indefinite, agent_pos=(100,160), reference_frame='local')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQWIhylL-_Po"
      },
      "source": [
        "**Network debugging - CoverNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVxTv8gCztA8"
      },
      "outputs": [],
      "source": [
        "# Covernet Training loop (MANUAL - DEBUG ONLY)\n",
        "if DEBUG_MODE:\n",
        "\n",
        "    # Dataset preparation\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=True)\n",
        "\n",
        "    # Training preparation\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=COVERNET_LR, momentum=COVERNET_MOMENTUM)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Plotting preparation\n",
        "    train_loss_arr = []\n",
        "    val_loss_arr = []\n",
        "    train_iterations = len(train_dataset) // BATCH_SIZE\n",
        "    val_iterations = len(val_dataset) // BATCH_SIZE\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(TRAIN_EPOCHES):\n",
        "        print(\"-------- Epoch %d --------\" % i)\n",
        "        model.train()\n",
        "\n",
        "        # Training\n",
        "        for j, data in enumerate(train_dataloader):\n",
        "            \n",
        "            # Data preparation\n",
        "            x_state, x_img_static, x_img_dynamic, gt, idx = data\n",
        "            x_state = x_state.to(device)\n",
        "            x_img_static = x_img_static.to(device)\n",
        "            x_img_dynamic = x_img_dynamic.to(device)\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            with torch.no_grad():\n",
        "                reduced_traj = model.trajectories[:, :SAMPLES_PER_SECOND*TRAJ_HORIZON]\n",
        "                y = get_positives(reduced_traj, gt)\n",
        "\n",
        "            # Inference\n",
        "            optimizer.zero_grad()\n",
        "            traj_logits = model((x_img_dynamic, x_state))\n",
        "            y = y.to(device)\n",
        "            loss = F.cross_entropy(traj_logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Logging\n",
        "            loss_val = loss.item()\n",
        "            train_loss_arr.append(loss_val)\n",
        "            print(\"[%d] %d - train loss = %f\" % (i, j, loss_val))\n",
        "\n",
        "        # Validation\n",
        "        model.train(mode=False)\n",
        "        for j, data in enumerate(val_dataloader):\n",
        "\n",
        "            # Data preparation\n",
        "            x_state, x_img_static, x_img_dynamic, gt, idx = data\n",
        "            x_state = x_state.to(device)\n",
        "            x_img_static = x_img_static.to(device)\n",
        "            x_img_dynamic = x_img_dynamic.to(device)\n",
        "            x_state = torch.flatten(x_state, 0, 1)\n",
        "            reduced_traj = model.trajectories[:, :SAMPLES_PER_SECOND*TRAJ_HORIZON]\n",
        "            y = get_positives(reduced_traj, gt)\n",
        "\n",
        "            # Inference\n",
        "            traj_logits = model((x_img_dynamic, x_state))\n",
        "            y = y.to(device)\n",
        "            loss = F.cross_entropy(traj_logits, y)\n",
        "\n",
        "            # Logging\n",
        "            loss_val = loss.item()\n",
        "            val_loss_arr.append(loss_val)\n",
        "            print(\"[%d] %d - val loss = %f\" % (i, j, loss_val))\n",
        "\n",
        "        # Plotting\n",
        "        if (i+1) % PLOT_PERIOD == 0:\n",
        "            plot_train_data(train_iterations, val_iterations, i+1, train_loss_arr, val_loss_arr)\n",
        "            a = input(\"Press Enter to continue...\")\n",
        "            plt.close('all')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "2c85f06a048b39aac865c8587deef3c29c1e1d5067cc6bdbf85545628eaaed39"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "068a6114b00e427f8393d85319962351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a6c8a923ff242e4b4d94650801a38f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108df33aefeb49b8abc8ccf6b6106404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "117186b02f5e4dedbe19d87d4f33a589": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27029c803ab547beaf634de3205120c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beb36ff225df4b2f954b20591826328b",
              "IPY_MODEL_57388def33e2422792567bb4340e1c73",
              "IPY_MODEL_c09acfdd256b4e26ab9da2522b499819"
            ],
            "layout": "IPY_MODEL_2f5674676aba46378d48a39e16979428"
          }
        },
        "2c143b3d29c6425f9741da7b5b3cde38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d265a46cede848d19064babef8f6f9f0",
              "IPY_MODEL_cfecfd4df3664336b5c3c6ea51aabd39",
              "IPY_MODEL_46bb4c94c71844d0b67b206e1b4aac54"
            ],
            "layout": "IPY_MODEL_8f79bd3668484bb6b2f7e205b4bc299d"
          }
        },
        "2f5674676aba46378d48a39e16979428": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f656033d56041aa8978a46d8bb708a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665955ba546b4410a6abd1850f2c57bc",
            "placeholder": "â",
            "style": "IPY_MODEL_30f648d3e6c24a3fb99c19d9590f6294",
            "value": "100%"
          }
        },
        "30f648d3e6c24a3fb99c19d9590f6294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373e78e969d843a08d08010d064162b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c31ca02039444c69eb80ebccc0aceb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ac600e1d3a4de6943d40fdd41ddd4e",
            "max": 61,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7057eb3bf8e4857b49cf7dacb237a42",
            "value": 61
          }
        },
        "3d0e91cee37f4c90943d18036a885ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8a5fc026fc4a88914776fb49ad189a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44d3596d0c5c4114b41ed7e503c9f1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46bb4c94c71844d0b67b206e1b4aac54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f89a5e1a2ccb45ceb8cecac19bdc38e5",
            "placeholder": "â",
            "style": "IPY_MODEL_f2c689524f83497a80d298d0c39cf097",
            "value": " 742/742 [00:00&lt;00:00, 23332.79it/s]"
          }
        },
        "4c0bfd39521a4d5ba82ebd9e88653578": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57388def33e2422792567bb4340e1c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a6c8a923ff242e4b4d94650801a38f1",
            "max": 61,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f8a5fc026fc4a88914776fb49ad189a",
            "value": 61
          }
        },
        "665955ba546b4410a6abd1850f2c57bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccae592d2da48629859a3c1b6bbeaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "714f6a403d32439280cd5e6b48a5434f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d7e0ca9b6384a8ebcf7ed368deb351a",
            "placeholder": "â",
            "style": "IPY_MODEL_6ccae592d2da48629859a3c1b6bbeaa7",
            "value": " 61/61 [00:00&lt;00:00, 236.57it/s]"
          }
        },
        "7301ee09215d48f38cb32881cd0e8377": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d7e0ca9b6384a8ebcf7ed368deb351a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f79bd3668484bb6b2f7e205b4bc299d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922e52d5f3334876acd3003bf35c6c41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c741e0fca1245c5a08d76dec11bd3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b8bf4034f8497d9ca2641e84a36b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d3596d0c5c4114b41ed7e503c9f1f2",
            "placeholder": "â",
            "style": "IPY_MODEL_3d0e91cee37f4c90943d18036a885ec6",
            "value": "100%"
          }
        },
        "a3563cebdfe149a3974fd669a51d2364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f4c00b5d6d46afa994bda5ef8281a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9ac600e1d3a4de6943d40fdd41ddd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99ab9d1c9974c22b1aa940dc6b225ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beb36ff225df4b2f954b20591826328b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f13d5142bf254781939e2b776371c5a6",
            "placeholder": "â",
            "style": "IPY_MODEL_a6f4c00b5d6d46afa994bda5ef8281a9",
            "value": "100%"
          }
        },
        "c09acfdd256b4e26ab9da2522b499819": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373e78e969d843a08d08010d064162b2",
            "placeholder": "â",
            "style": "IPY_MODEL_f331b35392a14c3bbdc518982544ac06",
            "value": " 61/61 [00:00&lt;00:00, 1959.93it/s]"
          }
        },
        "c7057eb3bf8e4857b49cf7dacb237a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfecfd4df3664336b5c3c6ea51aabd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3563cebdfe149a3974fd669a51d2364",
            "max": 742,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_068a6114b00e427f8393d85319962351",
            "value": 742
          }
        },
        "d265a46cede848d19064babef8f6f9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b99ab9d1c9974c22b1aa940dc6b225ef",
            "placeholder": "â",
            "style": "IPY_MODEL_4c0bfd39521a4d5ba82ebd9e88653578",
            "value": "100%"
          }
        },
        "dfbaacd239d74a9384ac57287256d41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922e52d5f3334876acd3003bf35c6c41",
            "placeholder": "â",
            "style": "IPY_MODEL_117186b02f5e4dedbe19d87d4f33a589",
            "value": " 61/61 [00:38&lt;00:00,  1.52it/s]"
          }
        },
        "ed133b40b19941bb8dd5513d71b21e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f656033d56041aa8978a46d8bb708a5",
              "IPY_MODEL_3c31ca02039444c69eb80ebccc0aceb9",
              "IPY_MODEL_dfbaacd239d74a9384ac57287256d41a"
            ],
            "layout": "IPY_MODEL_fb5ae27d0a92495fa8588fbbc181c24a"
          }
        },
        "f13d5142bf254781939e2b776371c5a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c689524f83497a80d298d0c39cf097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f331b35392a14c3bbdc518982544ac06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f73a3f7cb2314a568782068f1c04151d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0b8bf4034f8497d9ca2641e84a36b71",
              "IPY_MODEL_ff6087aaa0ff44b89938471d0fb0e957",
              "IPY_MODEL_714f6a403d32439280cd5e6b48a5434f"
            ],
            "layout": "IPY_MODEL_7301ee09215d48f38cb32881cd0e8377"
          }
        },
        "f89a5e1a2ccb45ceb8cecac19bdc38e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5ae27d0a92495fa8588fbbc181c24a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff6087aaa0ff44b89938471d0fb0e957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c741e0fca1245c5a08d76dec11bd3fd",
            "max": 61,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_108df33aefeb49b8abc8ccf6b6106404",
            "value": 61
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}